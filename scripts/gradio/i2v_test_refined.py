import os
import time
import sys
from omegaconf import OmegaConf
import torch
import numpy as np
from einops import repeat, rearrange
import torchvision.transforms as transforms
from pytorch_lightning import seed_everything
from huggingface_hub import hf_hub_download

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

from utils.utils import instantiate_from_config
from scripts.evaluation.funcs import load_model_checkpoint, save_videos, get_latent_z
# ç§»é™¤å¯¹å¤æ‚è°ƒåº¦å™¨çš„ä¾èµ–ï¼Œä½¿ç”¨æç®€ä¿®å¤æ–¹æ¡ˆ
# from dynamicrafter_scheduler import DynamiCrafterScheduler


def get_fixed_ddim_sampler(model, ddim_steps, ddim_eta, verbose=False):
    """
    è¿”å›ä¿®å¤åsigmaå€¼çš„DDIMSampler - æç®€ä¿®å¤æ–¹æ¡ˆ
    åªä¿®å¤sigmaè®¡ç®—ï¼Œä¿æŒå…¶ä»–é€»è¾‘å®Œå…¨ä¸å˜
    """
    from lvdm.models.samplers.ddim import DDIMSampler
    
    sampler = DDIMSampler(model)
    sampler.make_schedule(ddim_steps, ddim_discretize="uniform", ddim_eta=ddim_eta, verbose=verbose)
    
    # å…³é”®ä¿®å¤ï¼šç”¨DynamiCrafteråŸå§‹å‡½æ•°é‡æ–°è®¡ç®—sigmaå€¼
    from lvdm.models.utils_diffusion import make_ddim_sampling_parameters
    ddim_sigmas, ddim_alphas, ddim_alphas_prev = make_ddim_sampling_parameters(
        alphacums=model.alphas_cumprod.cpu(),
        ddim_timesteps=sampler.ddim_timesteps,
        eta=ddim_eta,
        verbose=False
    )
    
    # æ›¿æ¢æœ‰é—®é¢˜çš„sigmaå€¼ - ç»Ÿä¸€å¤„ç†ä¸ºtorch tensor
    if isinstance(ddim_sigmas, torch.Tensor):
        sampler.ddim_sigmas = ddim_sigmas.to(model.device)
    else:
        sampler.ddim_sigmas = torch.from_numpy(ddim_sigmas).to(model.device)
    
    if isinstance(ddim_alphas, torch.Tensor):
        sampler.ddim_alphas = ddim_alphas.to(model.device)
    else:
        sampler.ddim_alphas = torch.from_numpy(ddim_alphas).to(model.device)
    
    if isinstance(ddim_alphas_prev, torch.Tensor):
        sampler.ddim_alphas_prev = ddim_alphas_prev.to(model.device)
    else:
        sampler.ddim_alphas_prev = torch.from_numpy(ddim_alphas_prev).to(model.device)
    
    # è®¡ç®— ddim_sqrt_one_minus_alphas
    if isinstance(ddim_alphas, torch.Tensor):
        sampler.ddim_sqrt_one_minus_alphas = torch.sqrt(1. - ddim_alphas).to(model.device)
    else:
        sampler.ddim_sqrt_one_minus_alphas = torch.from_numpy(np.sqrt(1. - ddim_alphas)).to(model.device)
    
    if verbose:
        print("âœ… DDIMSampler fixed: sigma values replaced with numerically stable versions")
    
    return sampler


def image_guided_synthesis_fixed(model, prompts, videos, noise_shape, n_samples=1, ddim_steps=50, ddim_eta=1., 
                                unconditional_guidance_scale=1.0, fs=None, **kwargs):
    """
    ä½¿ç”¨æç®€ä¿®å¤æ–¹æ¡ˆçš„å›¾åƒå¼•å¯¼åˆæˆå‡½æ•°
    """
    # ä½¿ç”¨ä¿®å¤åçš„DDIMSampler
    ddim_sampler = get_fixed_ddim_sampler(model, ddim_steps, ddim_eta, verbose=False)
    
    batch_size = noise_shape[0]
    fs = torch.tensor([fs] * batch_size, dtype=torch.long, device=model.device)

    # å›¾åƒåµŒå…¥
    img = videos[:,:,0] #bchw
    img_emb = model.embedder(img) ## blc
    img_emb = model.image_proj_model(img_emb)

    # æ–‡æœ¬åµŒå…¥
    cond_emb = model.get_learned_conditioning(prompts)
    cond = {"c_crossattn": [torch.cat([cond_emb,img_emb], dim=1)]}
    
    # å›¾åƒæ¡ä»¶
    if model.model.conditioning_key == 'hybrid':
        z = get_latent_z(model, videos) # b c t h w
        img_cat_cond = z[:,:,:1,:,:]
        img_cat_cond = repeat(img_cat_cond, 'b c t h w -> b c (repeat t) h w', repeat=z.shape[2])
        cond["c_concat"] = [img_cat_cond]
    
    # æ— æ¡ä»¶å¼•å¯¼
    if unconditional_guidance_scale != 1.0:
        if model.uncond_type == "empty_seq":
            prompts_uc = batch_size * [""]
            uc_emb = model.get_learned_conditioning(prompts_uc)
        elif model.uncond_type == "zero_embed":
            uc_emb = torch.zeros_like(cond_emb)
        uc_img_emb = model.embedder(torch.zeros_like(img))
        uc_img_emb = model.image_proj_model(uc_img_emb)
        uc = {"c_crossattn": [torch.cat([uc_emb,uc_img_emb],dim=1)]}
        if model.model.conditioning_key == 'hybrid':
            uc["c_concat"] = [img_cat_cond]
    else:
        uc = None

    batch_variants = []
    for _ in range(n_samples):
        samples, _ = ddim_sampler.sample(S=ddim_steps,
                                        conditioning=cond,
                                        batch_size=batch_size,
                                        shape=noise_shape[1:],
                                        verbose=False,
                                        unconditional_guidance_scale=unconditional_guidance_scale,
                                        unconditional_conditioning=uc,
                                        eta=ddim_eta,
                                        fs=fs,
                                        **kwargs)

        # è§£ç åˆ°åƒç´ ç©ºé—´
        batch_images = model.decode_first_stage(samples)
        batch_variants.append(batch_images)
    
    # batch, <samples>, c, t, h, w
    batch_variants = torch.stack(batch_variants, dim=1)
    return batch_variants


# ç§»é™¤å¤æ‚çš„ batch_ddim_sampling_fixed_scheduler å‡½æ•°
# def batch_ddim_sampling_fixed_scheduler(model, cond, noise_shape, n_samples=1, ddim_steps=50, ddim_eta=1.0,
#                                        cfg_scale=1.0, temporal_cfg_scale=None, scheduler=None, **kwargs):
#     """
#     ä½¿ç”¨ä¿®å¤åçš„ DynamiCrafter è°ƒåº¦å™¨è¿›è¡Œæ‰¹é‡ DDIM é‡‡æ ·
#     """
#     # å¤æ‚çš„å®ç°è¢«ç§»é™¤ï¼Œä½¿ç”¨æç®€ä¿®å¤æ–¹æ¡ˆ


class Image2VideoFixedScheduler():
    """
    ä½¿ç”¨æç®€ä¿®å¤æ–¹æ¡ˆçš„ Image2Video ç±»
    """
    def __init__(self, result_dir='./tmp/', gpu_num=1, resolution='256_256') -> None:
        self.resolution = (int(resolution.split('_')[0]), int(resolution.split('_')[1]))  # hw
        self.download_model()
        
        self.result_dir = result_dir
        if not os.path.exists(self.result_dir):
            os.makedirs(self.result_dir)
            
        ckpt_path = f'checkpoints/dynamicrafter_{resolution.split("_")[1]}_v1/model.ckpt'
        config_file = f'configs/inference_{resolution.split("_")[1]}_v1.0.yaml'
        config = OmegaConf.load(config_file)
        model_config = config.pop("model", OmegaConf.create())
        model_config['params']['unet_config']['params']['use_checkpoint'] = False   
        
        model_list = []
        for gpu_id in range(gpu_num):
            model = instantiate_from_config(model_config)
            assert os.path.exists(ckpt_path), f"Error: checkpoint Not Found at {ckpt_path}!"
            model = load_model_checkpoint(model, ckpt_path)
            model.eval()
            model_list.append(model)
        
        self.model_list = model_list
        self.save_fps = 8

    def get_image(self, image, prompt, steps=50, cfg_scale=7.5, eta=1.0, fs=3, seed=123):
        """
        ä½¿ç”¨æç®€ä¿®å¤æ–¹æ¡ˆç”Ÿæˆè§†é¢‘
        
        Args:
            image: è¾“å…¥å›¾åƒ (numpy array)
            prompt: æ–‡æœ¬æç¤º
            steps: DDIM æ­¥æ•°
            cfg_scale: CFG æ¯”ä¾‹
            eta: DDIM eta å‚æ•°
            fs: å¸§æ•°
            seed: éšæœºç§å­
        """
        seed_everything(seed)
        transform = transforms.Compose([
            transforms.Resize(min(self.resolution)),
            transforms.CenterCrop(self.resolution),
        ])
        
        torch.cuda.empty_cache()
        print(f'ğŸš€ å¼€å§‹ç”Ÿæˆ: {prompt}')
        print(f'â° æ—¶é—´: {time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(time.time()))}')
        print(f'ğŸ”§ ä½¿ç”¨æç®€ä¿®å¤æ–¹æ¡ˆ')
        start = time.time()
        
        gpu_id = 0
        if steps > 60:
            steps = 60 
            
        model = self.model_list[gpu_id]
        model = model.cuda()
        
        batch_size = 1
        channels = model.model.diffusion_model.out_channels
        frames = model.temporal_length
        h, w = self.resolution[0] // 8, self.resolution[1] // 8
        noise_shape = [batch_size, channels, frames, h, w]

        # æ–‡æœ¬æ¡ä»¶
        with torch.no_grad(), torch.cuda.amp.autocast():
            # å›¾åƒæ¡ä»¶
            img_tensor = torch.from_numpy(image).permute(2, 0, 1).float().to(model.device)
            img_tensor = (img_tensor / 255. - 0.5) * 2

            image_tensor_resized = transform(img_tensor)  # 3,h,w
            videos = image_tensor_resized.unsqueeze(0).unsqueeze(2)  # bchw -> bc1hw
            
            print(f"ğŸ¬ å¼€å§‹é‡‡æ ·: {steps} æ­¥, CFG={cfg_scale}, eta={eta}")
            
            # ä½¿ç”¨æç®€ä¿®å¤æ–¹æ¡ˆè¿›è¡Œæ¨ç†
            batch_samples = image_guided_synthesis_fixed(
                model=model, 
                prompts=[prompt], 
                videos=videos, 
                noise_shape=noise_shape, 
                n_samples=1, 
                ddim_steps=steps, 
                ddim_eta=eta, 
                unconditional_guidance_scale=cfg_scale,
                fs=fs
            )
            
            # å‡†å¤‡æ–‡ä»¶å
            prompt_str = prompt.replace("/", "_slash_") if "/" in prompt else prompt
            prompt_str = prompt_str.replace(" ", "_") if " " in prompt else prompt_str
            prompt_str = prompt_str[:40]
            if len(prompt_str) == 0:
                prompt_str = 'empty_prompt'

        # ä¿å­˜è§†é¢‘
        save_videos(batch_samples, self.result_dir, filenames=[prompt_str], fps=self.save_fps)
        
        elapsed_time = time.time() - start
        print(f"âœ… ç”Ÿæˆå®Œæˆ!")
        print(f"ğŸ’¾ ä¿å­˜è·¯å¾„: {prompt_str}.mp4")
        print(f"â±ï¸  ç”¨æ—¶: {elapsed_time:.2f} ç§’")
        print(f"ğŸ¯ å¹³å‡æ¯æ­¥: {elapsed_time/steps:.2f} ç§’")
        
        model = model.cpu()
        return os.path.join(self.result_dir, f"{prompt_str}.mp4")
    
    def download_model(self):
        """ä¸‹è½½æ¨¡å‹æƒé‡"""
        REPO_ID = f'Doubiiu/DynamiCrafter_{self.resolution[1]}' if self.resolution[1] != 256 else 'Doubiiu/DynamiCrafter'
        filename_list = ['model.ckpt']
        model_dir = f'./checkpoints/dynamicrafter_{self.resolution[1]}_v1/'
        
        if not os.path.exists(model_dir):
            os.makedirs(model_dir)
            
        for filename in filename_list:
            local_file = os.path.join(model_dir, filename)
            if not os.path.exists(local_file):
                print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½æ¨¡å‹: {filename}")
                hf_hub_download(
                    repo_id=REPO_ID, 
                    filename=filename, 
                    local_dir=model_dir, 
                    local_dir_use_symlinks=False
                )
                print(f"âœ… ä¸‹è½½å®Œæˆ: {filename}")


def test_fixed_scheduler():
    """æµ‹è¯•æç®€ä¿®å¤æ–¹æ¡ˆ"""
    print("ğŸ§ª æµ‹è¯•æç®€ä¿®å¤æ–¹æ¡ˆ")
    print("=" * 50)
    
    # åˆ›å»º I2V å®ä¾‹
    i2v = Image2VideoFixedScheduler(result_dir='./results_minimal_fix/')
    
    # æµ‹è¯•å›¾åƒè·¯å¾„
    test_image_path = 'prompts/art.png'
    if not os.path.exists(test_image_path):
        print(f"âŒ æµ‹è¯•å›¾åƒä¸å­˜åœ¨: {test_image_path}")
        return
    
    # åŠ è½½æµ‹è¯•å›¾åƒ
    from PIL import Image
    import numpy as np
    
    img = Image.open(test_image_path).convert("RGB")
    img_array = np.array(img)
    
    # ç”Ÿæˆè§†é¢‘
    test_prompt = "a man fishing in a boat at sunset, peaceful water, golden hour lighting"
    
    print(f"ğŸ“¸ è¾“å…¥å›¾åƒ: {test_image_path}")
    print(f"ğŸ“ æç¤ºè¯: {test_prompt}")
    
    try:
        video_path = i2v.get_image(
            image=img_array,
            prompt=test_prompt,
            steps=20,  # å‡å°‘æ­¥æ•°ä»¥ä¾¿å¿«é€Ÿæµ‹è¯•
            cfg_scale=7.5,
            eta=0.0,   # ç¡®å®šæ€§é‡‡æ ·
            fs=3,
            seed=42
        )
        print(f"ğŸ‰ æµ‹è¯•æˆåŠŸ! è§†é¢‘ä¿å­˜åœ¨: {video_path}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    # å¯ä»¥é€‰æ‹©è¿è¡Œæµ‹è¯•æˆ–å•ç‹¬ä½¿ç”¨
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == 'test':
        test_fixed_scheduler()
    else:
        # ç®€å•ä½¿ç”¨ç¤ºä¾‹ - ä½¿ç”¨ 256 åˆ†è¾¨ç‡
        i2v = Image2VideoFixedScheduler(resolution='256_256')
        
        # ä½¿ç”¨ prompts/256/art.png å›¾ç‰‡
        if os.path.exists('prompts/256/art.png'):
            from PIL import Image
            import numpy as np
            
            img = Image.open('prompts/256/art.png').convert("RGB")
            img_array = np.array(img)
            
            video_path = i2v.get_image(
                image=img_array,
                prompt='man fishing in a boat at sunset',
                steps=30,
                cfg_scale=7.5,
                eta=0.0,
                seed=123
            )
            print(f'âœ… å®Œæˆ! è§†é¢‘è·¯å¾„: {video_path}')
        else:
            print("âŒ æµ‹è¯•å›¾åƒä¸å­˜åœ¨: prompts/256/art.png")
            print("ğŸ“ ç”¨æ³•ç¤ºä¾‹:")
            print("  python i2v_test_refined.py test  # è¿è¡Œæµ‹è¯•")
            print("  æˆ–è€…æŒ‰ç…§ä»£ç ä¸­çš„ç¤ºä¾‹ä½¿ç”¨ Image2VideoFixedScheduler ç±»")