import os
import time
import sys
from typing import Optional, Union, List, Any, Dict, Callable
from omegaconf import OmegaConf
import torch
import numpy as np
from einops import repeat, rearrange
import torchvision.transforms as transforms
from pytorch_lightning import seed_everything
from huggingface_hub import hf_hub_download
from PIL import Image

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

from utils.utils import instantiate_from_config
from scripts.evaluation.funcs import load_model_checkpoint, save_videos, get_latent_z


def get_fixed_ddim_sampler(model, ddim_steps, ddim_eta, verbose=False):
    """
    è¿”å›ä¿®å¤åsigmaå€¼çš„DDIMSampler - æç®€ä¿®å¤æ–¹æ¡ˆ
    åªä¿®å¤sigmaè®¡ç®—ï¼Œä¿æŒå…¶ä»–é€»è¾‘å®Œå…¨ä¸å˜
    """
    from lvdm.models.samplers.ddim import DDIMSampler
    
    sampler = DDIMSampler(model)
    sampler.make_schedule(ddim_steps, ddim_discretize="uniform", ddim_eta=ddim_eta, verbose=verbose)
    
    # å…³é”®ä¿®å¤ï¼šç”¨DynamiCrafteråŸå§‹å‡½æ•°é‡æ–°è®¡ç®—sigmaå€¼
    from lvdm.models.utils_diffusion import make_ddim_sampling_parameters
    ddim_sigmas, ddim_alphas, ddim_alphas_prev = make_ddim_sampling_parameters(
        alphacums=model.alphas_cumprod.cpu(),
        ddim_timesteps=sampler.ddim_timesteps,
        eta=ddim_eta,
        verbose=False
    )
    
    # æ›¿æ¢æœ‰é—®é¢˜çš„sigmaå€¼ - ç»Ÿä¸€å¤„ç†ä¸ºtorch tensor
    if isinstance(ddim_sigmas, torch.Tensor):
        sampler.ddim_sigmas = ddim_sigmas.to(model.device)
    else:
        sampler.ddim_sigmas = torch.from_numpy(ddim_sigmas).to(model.device)
    
    if isinstance(ddim_alphas, torch.Tensor):
        sampler.ddim_alphas = ddim_alphas.to(model.device)
    else:
        sampler.ddim_alphas = torch.from_numpy(ddim_alphas).to(model.device)
    
    if isinstance(ddim_alphas_prev, torch.Tensor):
        sampler.ddim_alphas_prev = ddim_alphas_prev.to(model.device)
    else:
        sampler.ddim_alphas_prev = torch.from_numpy(ddim_alphas_prev).to(model.device)
    
    # è®¡ç®— ddim_sqrt_one_minus_alphas
    if isinstance(ddim_alphas, torch.Tensor):
        sampler.ddim_sqrt_one_minus_alphas = torch.sqrt(1. - ddim_alphas).to(model.device)
    else:
        sampler.ddim_sqrt_one_minus_alphas = torch.from_numpy(np.sqrt(1. - ddim_alphas)).to(model.device)
    
    if verbose:
        print("âœ… DDIMSampler fixed: sigma values replaced with numerically stable versions")
    
    return sampler


class DynamiCrafterImg2VideoPipeline:
    """
    DynamiCrafter Image-to-Video Pipeline
    
    This pipeline implements a diffusers-like interface for DynamiCrafter image-to-video generation
    with the minimal fix for NaN issues.
    """
    
    def __init__(
        self, 
        resolution: str = '256_256',
        device: Optional[str] = None,
        dtype: Optional[torch.dtype] = None,
        **kwargs
    ):
        """
        Initialize the DynamiCrafter pipeline.
        
        Args:
            resolution: Model resolution (e.g., '256_256', '512_512', '1024_1024')
            device: Device to run the model on
            dtype: Data type for the model
        """
        self.resolution = tuple(map(int, resolution.split('_')))  # (height, width)
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.dtype = dtype or torch.float32
        
        # Initialize components
        self._load_components()
        self._setup_image_processor()
        
        # Pipeline state
        self._is_initialized = True
        print(f"âœ… DynamiCrafter Pipeline initialized: {self.resolution[0]}x{self.resolution[1]}")
        
    def _load_components(self):
        """Load model and scheduler components"""
        # Download model if needed
        self._download_model()
        
        # Load model - ä¿®å¤é…ç½®æ–‡ä»¶è·¯å¾„
        project_root = os.path.join(os.path.dirname(__file__), '..', '..')
        ckpt_path = os.path.join(project_root, f'checkpoints/dynamicrafter_{self.resolution[1]}_v1/model.ckpt')
        config_file = os.path.join(project_root, f'configs/inference_{self.resolution[1]}_v1.0.yaml')
        
        config = OmegaConf.load(config_file)
        model_config = config.pop("model", OmegaConf.create())
        model_config['params']['unet_config']['params']['use_checkpoint'] = False
        
        self.model = instantiate_from_config(model_config)
        assert os.path.exists(ckpt_path), f"Error: checkpoint Not Found at {ckpt_path}!"
        self.model = load_model_checkpoint(self.model, ckpt_path)
        self.model.eval()
        
        # ç¡®ä¿æ¨¡å‹ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡ - å¢å¼ºç‰ˆæœ¬
        self.model = self.model.to(self.device)
        
        # å¼ºåˆ¶ç¡®ä¿æ‰€æœ‰å­æ¨¡å—éƒ½ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡
        def move_to_device(module):
            """é€’å½’ç§»åŠ¨æ¨¡å—åˆ°ç›®æ ‡è®¾å¤‡"""
            for name, param in module.named_parameters():
                if param.device != torch.device(self.device):
                    param.data = param.data.to(self.device)
            
            for name, buffer in module.named_buffers():
                if buffer.device != torch.device(self.device):
                    buffer.data = buffer.data.to(self.device)
            
            for name, child in module.named_children():
                move_to_device(child)
        
        move_to_device(self.model)
        
        # ç‰¹åˆ«å¤„ç†æ–‡æœ¬ç¼–ç å™¨å’Œå›¾åƒç¼–ç å™¨
        if hasattr(self.model, 'cond_stage_model'):
            self.model.cond_stage_model = self.model.cond_stage_model.to(self.device)
            move_to_device(self.model.cond_stage_model)
        
        if hasattr(self.model, 'first_stage_model'):
            self.model.first_stage_model = self.model.first_stage_model.to(self.device)
            move_to_device(self.model.first_stage_model)
        
        if hasattr(self.model, 'embedder'):
            self.model.embedder = self.model.embedder.to(self.device)
            move_to_device(self.model.embedder)
        
        if hasattr(self.model, 'image_proj_model'):
            self.model.image_proj_model = self.model.image_proj_model.to(self.device)
            move_to_device(self.model.image_proj_model)
        
        print(f"ğŸ”„ Model moved to {self.device}")
        print(f"âœ… All model components verified on {self.device}")
        
        # éªŒè¯è®¾å¤‡
        device_check_passed = True
        for name, param in self.model.named_parameters():
            if param.device != torch.device(self.device):
                print(f"âš ï¸ Warning: {name} is still on {param.device}")
                device_check_passed = False
        
        if device_check_passed:
            print(f"âœ… Device check passed - all parameters on {self.device}")
        else:
            print(f"âš ï¸ Device check failed - some parameters not on {self.device}")
        
    def _setup_image_processor(self):
        """Setup image preprocessing transforms"""
        self.image_processor = transforms.Compose([
            transforms.Resize(min(self.resolution)),
            transforms.CenterCrop(self.resolution),
        ])
        
    def _download_model(self):
        """Download model weights if needed"""
        REPO_ID = f'Doubiiu/DynamiCrafter_{self.resolution[1]}' if self.resolution[1] != 256 else 'Doubiiu/DynamiCrafter'
        filename_list = ['model.ckpt']
        
        # ä¿®å¤æ¨¡å‹ä¸‹è½½è·¯å¾„
        project_root = os.path.join(os.path.dirname(__file__), '..', '..')
        model_dir = os.path.join(project_root, f'checkpoints/dynamicrafter_{self.resolution[1]}_v1/')
        
        if not os.path.exists(model_dir):
            os.makedirs(model_dir)
            
        for filename in filename_list:
            local_file = os.path.join(model_dir, filename)
            if not os.path.exists(local_file):
                print(f"ğŸ“¥ Downloading model: {filename}")
                hf_hub_download(
                    repo_id=REPO_ID, 
                    filename=filename, 
                    local_dir=model_dir, 
                    local_dir_use_symlinks=False
                )
                print(f"âœ… Download completed: {filename}")
    
    def enable_attention_slicing(self, slice_size: Optional[Union[str, int]] = "auto"):
        """Enable attention slicing for memory efficiency (placeholder for compatibility)"""
        print("ğŸ“ Note: Attention slicing not implemented yet")
        pass
    
    def disable_attention_slicing(self):
        """Disable attention slicing (placeholder for compatibility)"""
        print("ğŸ“ Note: Attention slicing not implemented yet")
        pass
    
    def enable_xformers_memory_efficient_attention(self):
        """Enable xformers memory efficient attention (placeholder for compatibility)"""
        print("ğŸ“ Note: xformers optimization not implemented yet")
        pass
    
    def to(self, device: Union[str, torch.device], dtype: Optional[torch.dtype] = None):
        """Move pipeline to device"""
        self.device = device
        if dtype is not None:
            self.dtype = dtype
        
        # ç¡®ä¿æ‰€æœ‰æ¨¡å‹ç»„ä»¶éƒ½ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡
        self.model = self.model.to(device)
        
        # å¼ºåˆ¶ç¡®ä¿æ‰€æœ‰å­æ¨¡å—éƒ½ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡
        def move_to_device(module):
            """é€’å½’ç§»åŠ¨æ¨¡å—åˆ°ç›®æ ‡è®¾å¤‡"""
            for name, param in module.named_parameters():
                if param.device != torch.device(device):
                    param.data = param.data.to(device)
            
            for name, buffer in module.named_buffers():
                if buffer.device != torch.device(device):
                    buffer.data = buffer.data.to(device)
            
            for name, child in module.named_children():
                move_to_device(child)
        
        move_to_device(self.model)
        
        # ç‰¹åˆ«å¤„ç†æ–‡æœ¬ç¼–ç å™¨å’Œå›¾åƒç¼–ç å™¨
        if hasattr(self.model, 'cond_stage_model'):
            self.model.cond_stage_model = self.model.cond_stage_model.to(device)
            move_to_device(self.model.cond_stage_model)
        
        if hasattr(self.model, 'first_stage_model'):
            self.model.first_stage_model = self.model.first_stage_model.to(device)
            move_to_device(self.model.first_stage_model)
        
        if hasattr(self.model, 'embedder'):
            self.model.embedder = self.model.embedder.to(device)
            move_to_device(self.model.embedder)
        
        if hasattr(self.model, 'image_proj_model'):
            self.model.image_proj_model = self.model.image_proj_model.to(device)
            move_to_device(self.model.image_proj_model)
        
        if dtype is not None:
            self.model = self.model.to(dtype)
        
        print(f"ğŸ”„ Pipeline moved to {device}")
        return self
    
    def _preprocess_image(self, image, height=None, width=None):
        """é¢„å¤„ç†è¾“å…¥å›¾åƒ"""
        if isinstance(image, Image.Image):
            image = np.array(image)
        
        if isinstance(image, np.ndarray):
            image = torch.from_numpy(image).permute(2, 0, 1).float()
        
        # æ ‡å‡†åŒ–åˆ° [-1, 1]
        if image.max() > 1.0:
            image = image / 255.0
        image = (image - 0.5) * 2
        
        # è°ƒæ•´å¤§å°
        if height is not None and width is not None:
            transform = transforms.Compose([
                transforms.Resize((height, width)),
            ])
        else:
            transform = self.image_processor
        
        return transform(image)
    
    def _encode_prompt(self, prompt, negative_prompt, device):
        """ç¼–ç æ–‡æœ¬æç¤º"""
        # ç¡®ä¿æ–‡æœ¬ç¼–ç å™¨åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        if hasattr(self.model, 'cond_stage_model'):
            self.model.cond_stage_model = self.model.cond_stage_model.to(device)
            # å¼ºåˆ¶è®¾ç½®è®¾å¤‡å±æ€§
            if hasattr(self.model.cond_stage_model, 'device'):
                self.model.cond_stage_model.device = device
            # é€’å½’ç¡®ä¿æ‰€æœ‰å­æ¨¡å—åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
            for name, module in self.model.cond_stage_model.named_modules():
                if hasattr(module, 'device'):
                    module.device = device
                for param in module.parameters():
                    if param.device != torch.device(device):
                        param.data = param.data.to(device)
        
        # æ­£å‘æç¤º
        text_embeddings = self.model.get_learned_conditioning(prompt)
        
        # è´Ÿå‘æç¤ºï¼ˆå¦‚æœæä¾›ï¼‰
        if negative_prompt is not None:
            uncond_embeddings = self.model.get_learned_conditioning(negative_prompt)
        else:
            if self.model.uncond_type == "empty_seq":
                uncond_embeddings = self.model.get_learned_conditioning([""] * len(prompt))
            elif self.model.uncond_type == "zero_embed":
                uncond_embeddings = torch.zeros_like(text_embeddings)
        
        return {"cond": text_embeddings, "uncond": uncond_embeddings}
    
    def _encode_image(self, image, num_frames):
        """ç¼–ç è¾“å…¥å›¾åƒ"""
        # æ·»åŠ æ‰¹æ¬¡ç»´åº¦
        if image.dim() == 3:
            image = image.unsqueeze(0)
        
        image = image.to(self.device)
        
        # å›¾åƒåµŒå…¥
        cond_images = self.model.embedder(image)
        img_emb = self.model.image_proj_model(cond_images)
        
        # æ½œåœ¨è¡¨ç¤º
        videos = image.unsqueeze(2)  # æ·»åŠ æ—¶é—´ç»´åº¦
        z = get_latent_z(self.model, videos)
        
        return img_emb, z
    
    def _prepare_conditioning(self, text_embeddings, image_embeddings, image_latents, 
                             frame_stride, guidance_scale, batch_size):
        """å‡†å¤‡æ¡ä»¶è¾“å…¥"""
        # ç»„åˆæ–‡æœ¬å’Œå›¾åƒåµŒå…¥
        cond_emb = torch.cat([text_embeddings["cond"], image_embeddings], dim=1)
        cond = {"c_crossattn": [cond_emb]}
        
        # å›¾åƒæ¡ä»¶ï¼ˆhybridæ¨¡å¼ï¼‰
        if self.model.model.conditioning_key == 'hybrid':
            img_cat_cond = image_latents[:,:,:1,:,:]
            img_cat_cond = repeat(img_cat_cond, 'b c t h w -> b c (repeat t) h w', 
                                 repeat=self.model.temporal_length)
            cond["c_concat"] = [img_cat_cond]
        
        # æ— æ¡ä»¶è¾“å…¥
        uc = None
        if guidance_scale != 1.0:
            # ä¿®å¤ï¼šåˆ›å»ºæ­£ç¡®å½¢çŠ¶çš„é›¶å›¾åƒï¼ˆåŸå§‹å›¾åƒç©ºé—´ï¼Œ3é€šé“ï¼‰
            zero_image = torch.zeros((batch_size, 3, self.resolution[0], self.resolution[1]), 
                                   device=self.model.device, dtype=self.model.dtype)
            uc_img_emb = self.model.embedder(zero_image)
            uc_img_emb = self.model.image_proj_model(uc_img_emb)
            
            uc_emb = torch.cat([text_embeddings["uncond"], uc_img_emb], dim=1)
            uc = {"c_crossattn": [uc_emb]}
            
            if self.model.model.conditioning_key == 'hybrid':
                uc["c_concat"] = [img_cat_cond]
        
        # å¸§æ­¥é•¿
        fs = torch.tensor([frame_stride] * batch_size, dtype=torch.long, device=self.model.device)
        
        return {"cond": cond, "uc": uc, "fs": fs}
    
    def _prepare_latents(self, noise_shape, device, generator, dtype):
        """å‡†å¤‡åˆå§‹ latents"""
        if generator is not None:
            latents = torch.randn(noise_shape, generator=generator, device=device, dtype=dtype)
        else:
            latents = torch.randn(noise_shape, device=device, dtype=dtype)
        return latents
    
    def _decode_latents(self, latents):
        """è§£ç æ½œåœ¨è¡¨ç¤ºåˆ°è§†é¢‘"""
        with torch.no_grad():
            videos = self.model.decode_first_stage(latents)
        return videos
    
    def _postprocess_video(self, videos, output_type):
        """åå¤„ç†è§†é¢‘è¾“å‡º"""
        if output_type == "numpy":
            videos = videos.cpu().float().numpy()
        elif output_type == "pil":
            # è½¬æ¢ä¸ºPILå›¾åƒåˆ—è¡¨ï¼ˆç®€åŒ–å®ç°ï¼‰
            videos = videos.cpu().float().numpy()
            print("ğŸ“ Note: PIL output conversion not fully implemented, returning numpy")
        # tensoræ ¼å¼ç›´æ¥è¿”å›
        
        return videos
    
    def __call__(
        self,
        image: Union[Image.Image, np.ndarray, torch.Tensor],
        prompt: Union[str, List[str]] = "",
        negative_prompt: Optional[Union[str, List[str]]] = None,
        num_inference_steps: int = 50,
        guidance_scale: float = 7.5,
        eta: float = 0.0,
        frame_stride: int = 3,
        num_frames: Optional[int] = None,
        height: Optional[int] = None,
        width: Optional[int] = None,
        num_videos_per_prompt: int = 1,
        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,
        latents: Optional[torch.FloatTensor] = None,
        output_type: str = "tensor",
        return_dict: bool = True,
        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,
        callback_steps: int = 1,
        cross_attention_kwargs: Optional[Dict[str, Any]] = None,
        **kwargs
    ) -> Union[torch.Tensor, Dict[str, Any]]:
        """
        Generate video from image and prompt using DynamiCrafter.
        
        Args:
            image: Input image for video generation
            prompt: Text prompt(s) to guide video generation
            negative_prompt: Negative prompt(s) 
            num_inference_steps: Number of denoising steps
            guidance_scale: Guidance scale for classifier-free guidance
            eta: DDIM eta parameter (0.0=deterministic, 1.0=stochastic)
            frame_stride: Frame stride control parameter
            num_frames: Number of frames to generate
            height: Height of generated video
            width: Width of generated video
            num_videos_per_prompt: Number of videos to generate per prompt
            generator: Random generator for reproducibility
            latents: Pre-generated noisy latents
            output_type: Output format ("tensor", "numpy", "pil")
            return_dict: Whether to return dict or tensor
            callback: Callback function
            callback_steps: Callback frequency
            cross_attention_kwargs: Cross-attention parameters
            
        Returns:
            Generated videos tensor or dict
        """
        
        # è¾“å…¥éªŒè¯å’Œæ ‡å‡†åŒ–
        if isinstance(prompt, str):
            batch_size = 1
            prompt = [prompt]
        else:
            batch_size = len(prompt)
        
        # å¤„ç† negative_prompt
        if negative_prompt is not None:
            if isinstance(negative_prompt, str):
                negative_prompt = [negative_prompt] * batch_size
            elif len(negative_prompt) != batch_size:
                raise ValueError(f"negative_prompt length ({len(negative_prompt)}) != batch_size ({batch_size})")
        
        # è®¾å¤‡ç®¡ç†
        device = self.device
        # ç§»é™¤é‡å¤çš„è®¾å¤‡ç§»åŠ¨ï¼Œå› ä¸ºæ¨¡å‹å·²ç»åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        # self.model = self.model.to(device)
        
        print(f"ğŸ¬ å¼€å§‹ç”Ÿæˆè§†é¢‘...")
        print(f"ğŸ“ æç¤ºè¯: {prompt}")
        print(f"ğŸ”§ å‚æ•°: steps={num_inference_steps}, guidance={guidance_scale}, eta={eta}")
        print(f"ğŸ’» è®¾å¤‡: {device}")
        
        # å›¾åƒé¢„å¤„ç†
        processed_image = self._preprocess_image(image, height, width)
        
        # æ¨¡å‹å‚æ•°
        num_frames = num_frames or self.model.temporal_length
        channels = self.model.model.diffusion_model.out_channels
        if height is None or width is None:
            height, width = self.resolution
        latent_height, latent_width = height // 8, width // 8
        
        # å™ªå£°å½¢çŠ¶
        noise_shape = (batch_size, channels, num_frames, latent_height, latent_width)
        
        with torch.no_grad(), torch.cuda.amp.autocast():
            # ç¼–ç æç¤ºè¯å’Œå›¾åƒ
            text_embeddings = self._encode_prompt(prompt, negative_prompt, device)
            image_embeddings, image_latents = self._encode_image(processed_image, num_frames)
            
            # å‡†å¤‡æ¡ä»¶è¾“å…¥
            conditioning = self._prepare_conditioning(
                text_embeddings=text_embeddings,
                image_embeddings=image_embeddings,
                image_latents=image_latents,
                frame_stride=frame_stride,
                guidance_scale=guidance_scale,
                batch_size=batch_size
            )
            
            # åˆå§‹åŒ–è°ƒåº¦å™¨
            scheduler = get_fixed_ddim_sampler(self.model, num_inference_steps, eta, verbose=False)
            
            # æ‰§è¡Œé‡‡æ ·
            print(f"ğŸ”„ å¼€å§‹ DDIM é‡‡æ ·...")
            samples, _ = scheduler.sample(
                S=num_inference_steps,
                conditioning=conditioning["cond"],
                batch_size=batch_size,
                shape=noise_shape[1:],
                verbose=False,
                unconditional_guidance_scale=guidance_scale,
                unconditional_conditioning=conditioning["uc"],
                eta=eta,
                fs=conditioning["fs"],
                **kwargs
            )
            
            # è§£ç  latents åˆ°è§†é¢‘
            print(f"ğŸï¸ è§£ç æ½œåœ¨è¡¨ç¤º...")
            videos = self._decode_latents(samples)
        
        # åå¤„ç†
        videos = self._postprocess_video(videos, output_type)
        
        print(f"âœ… è§†é¢‘ç”Ÿæˆå®Œæˆ! å½¢çŠ¶: {videos.shape}")
        
        # è¿”å›ç»“æœ
        if return_dict:
            return {"videos": videos}
        else:
            return videos
    
    def save_video(
        self, 
        video: torch.Tensor, 
        output_path: str, 
        fps: int = 8, 
        **kwargs
    ):
        """
        Save generated video to file
        
        Args:
            video: Generated video tensor
            output_path: Output file path
            fps: Frames per second
        """
        # Ensure output directory exists
        output_dir = os.path.dirname(output_path)
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
        
        # Get filename without extension
        filename = os.path.basename(output_path).split('.')[0]
        
        # ä¿®å¤ç»´åº¦é—®é¢˜ï¼šsave_videosæœŸæœ› (batch, samples, channels, time, height, width)
        # è€Œæˆ‘ä»¬çš„videoæ˜¯ (batch, channels, time, height, width)
        if video.dim() == 4:
            # å¦‚æœæ˜¯4ç»´ (channels, time, height, width)ï¼Œæ·»åŠ batchå’Œsamplesç»´åº¦
            video = video.unsqueeze(0).unsqueeze(0)  # -> (1, 1, channels, time, height, width)
        elif video.dim() == 5:
            # å¦‚æœæ˜¯5ç»´ (batch, channels, time, height, width)ï¼Œæ·»åŠ samplesç»´åº¦
            video = video.unsqueeze(1)  # -> (batch, 1, channels, time, height, width)
        
        # ç¡®ä¿è§†é¢‘tensoråœ¨CPUä¸Š
        if video.is_cuda:
            video = video.cpu()
        
        print(f"ğŸ“Š ä¿å­˜è§†é¢‘å½¢çŠ¶: {video.shape}")
        save_videos(video, output_dir or '.', filenames=[filename], fps=fps)
        
        print(f"ğŸ’¾ è§†é¢‘å·²ä¿å­˜: {output_path}")


def test_pipeline():
    """æµ‹è¯• DynamiCrafter Pipeline"""
    print("ğŸ§ª æµ‹è¯• DynamiCrafter Pipeline")
    print("=" * 60)
    
    try:
        # åˆå§‹åŒ– pipeline - å…ˆä½¿ç”¨é»˜è®¤è®¾å¤‡æµ‹è¯•åŸºæœ¬åŠŸèƒ½
        print("ğŸ”§ åˆå§‹åŒ– Pipeline (é»˜è®¤è®¾å¤‡)...")
        pipeline = DynamiCrafterImg2VideoPipeline(
            resolution='256_256'
            # device='cuda:3'  # æš‚æ—¶æ³¨é‡Šæ‰ï¼Œå…ˆæµ‹è¯•åŸºæœ¬åŠŸèƒ½
        )
        
        # æ£€æŸ¥æµ‹è¯•å›¾åƒ - æ›´æ–°ä¸ºå®é™…å­˜åœ¨çš„å›¾åƒ
        project_root = os.path.join(os.path.dirname(__file__), '..', '..')
        test_image_paths = [
            os.path.join(project_root, 'prompts/1024/pour_bear.png'),
            os.path.join(project_root, 'prompts/1024/robot01.png'),
            os.path.join(project_root, 'prompts/512_loop/24.png'),
            os.path.join(project_root, 'prompts/1024/astronaut04.png'),
        ]
        
        test_image_path = None
        for path in test_image_paths:
            if os.path.exists(path):
                test_image_path = path
                break
        
        if test_image_path is None:
            print("âŒ æµ‹è¯•å›¾åƒä¸å­˜åœ¨ï¼Œè¯·ç¡®ä¿æœ‰æµ‹è¯•å›¾åƒæ–‡ä»¶")
            print("ğŸ“‹ æŸ¥æ‰¾çš„è·¯å¾„:")
            for path in test_image_paths:
                print(f"  - {path} (å­˜åœ¨: {os.path.exists(path)})")
            return False
        
        # åŠ è½½æµ‹è¯•å›¾åƒ
        print(f"ğŸ“¸ åŠ è½½æµ‹è¯•å›¾åƒ: {test_image_path}")
        img = Image.open(test_image_path).convert("RGB")
        print(f"ğŸ“ å›¾åƒå°ºå¯¸: {img.size}")
        
        # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
        test_prompt = "a person walking in a beautiful garden with flowers blooming"
        
        print(f"ğŸ“ æµ‹è¯•æç¤ºè¯: {test_prompt}")
        print(f"ğŸ¬ å¼€å§‹ç”Ÿæˆ...")
        
        start_time = time.time()
        
        # ä½¿ç”¨ pipeline ç”Ÿæˆ
        result = pipeline(
            image=img,
            prompt=test_prompt,
            num_inference_steps=5,  # è¿›ä¸€æ­¥å‡å°‘æ­¥æ•°ä»¥ä¾¿å¿«é€Ÿæµ‹è¯•
            guidance_scale=7.5,
            eta=0.0,
            frame_stride=3,
            return_dict=True
        )
        
        elapsed_time = time.time() - start_time
        
        # æ£€æŸ¥ç»“æœ
        video = result["videos"]
        print(f"ğŸ‰ ç”ŸæˆæˆåŠŸ!")
        print(f"ğŸ“Š è§†é¢‘å½¢çŠ¶: {video.shape}")
        print(f"â±ï¸ ç”¨æ—¶: {elapsed_time:.2f} ç§’")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰NaN
        if torch.isnan(video).any():
            print("âŒ æ£€æµ‹åˆ° NaN å€¼!")
            return False
        else:
            print("âœ… æ—  NaN é—®é¢˜")
        
        # ä¿å­˜ç»“æœ
        output_dir = './results_pipeline_test/'
        os.makedirs(output_dir, exist_ok=True)
        output_path = os.path.join(output_dir, "test_video.mp4")
        
        pipeline.save_video(video, output_path)
        
        # éªŒè¯è¾“å‡º
        if os.path.exists(output_path):
            file_size = os.path.getsize(output_path)
            print(f"âœ… è§†é¢‘å·²ä¿å­˜: {output_path} ({file_size/1024:.1f} KB)")
        else:
            print("âŒ è§†é¢‘ä¿å­˜å¤±è´¥")
            return False
        
        print("ğŸ‰ Pipeline æµ‹è¯•æˆåŠŸ!")
        print("ğŸ“‹ æµ‹è¯•æ€»ç»“:")
        print("  âœ… æ¨¡å‹åŠ è½½æ­£å¸¸")
        print("  âœ… å›¾åƒé¢„å¤„ç†æ­£å¸¸")
        print("  âœ… æ–‡æœ¬ç¼–ç æ­£å¸¸")
        print("  âœ… DDIMé‡‡æ ·æ­£å¸¸")
        print("  âœ… è§†é¢‘è§£ç æ­£å¸¸")
        print("  âœ… è§†é¢‘ä¿å­˜æ­£å¸¸")
        print("  âœ… æ— NaNé—®é¢˜")
        return True
        
    except Exception as e:
        print(f"âŒ Pipeline æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == '__main__':
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == 'test':
        test_pipeline()
    else:
        print("ğŸ“ ç”¨æ³•:")
        print("  python dynamicrafter_pipeline.py test  # è¿è¡Œæµ‹è¯•")
        print("")
        print("ğŸ“– ç¤ºä¾‹ä»£ç :")
        print("```python")
        print("from dynamicrafter_pipeline import DynamiCrafterImg2VideoPipeline")
        print("from PIL import Image")
        print("")
        print("# åˆå§‹åŒ– pipeline")
        print("pipeline = DynamiCrafterImg2VideoPipeline(resolution='256_256')")
        print("")
        print("# åŠ è½½å›¾åƒ")
        print("image = Image.open('your_image.jpg')")
        print("")
        print("# ç”Ÿæˆè§†é¢‘")
        print("result = pipeline(")
        print("    image=image,")
        print("    prompt='your prompt here',")
        print("    num_inference_steps=50,")
        print("    guidance_scale=7.5")
        print(")")
        print("")
        print("# ä¿å­˜è§†é¢‘")
        print("pipeline.save_video(result['videos'], 'output.mp4')")
        print("```") 