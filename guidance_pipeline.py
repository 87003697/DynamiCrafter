# guidance_pipeline.py
"""
DynamiCrafter Guidance Pipeline - Simplified Version with Complete Debug

ä¸“æ³¨äºæ ¸å¿ƒçš„ SDS loss é€»è¾‘ï¼š
1. ä¿æŒ DynamiCrafter çš„æ‰€æœ‰æ ¸å¿ƒé€»è¾‘
2. åªæ›¿æ¢ scheduler.sample() è°ƒç”¨ä¸º optimization loop
3. åœ¨ optimization loop ä¸­ä½¿ç”¨ DynamiCrafter çš„ model.apply_model() æ–¹æ³•
4. å®Œæ•´çš„debugä¿å­˜åŠŸèƒ½ï¼ˆç±»ä¼¼åŸç‰ˆç»“æ„ï¼‰
"""

import os
import sys
import torch
import torch.nn.functional as F
import numpy as np
from PIL import Image
import torchvision.transforms as transforms
from typing import Optional, Union, List, Dict, Any
from omegaconf import OmegaConf
# æ·»åŠ debugç›¸å…³å¯¼å…¥
import matplotlib.pyplot as plt
import cv2
from datetime import datetime
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from utils.utils import instantiate_from_config
from scripts.evaluation.funcs import load_model_checkpoint, get_latent_z


class DynamiCrafterGuidancePipeline:
    """
    DynamiCrafter Guidance Pipeline - Simplified Version with Complete Debug
    ä¸“æ³¨äºæ ¸å¿ƒçš„ SDS loss é€»è¾‘ + å®Œæ•´debugåŠŸèƒ½
    """
    
    def __init__(self, resolution: str = '256_256', device: Optional[str] = None, debug_dir: Optional[str] = None):
        self.resolution = tuple(map(int, resolution.split('_')))
        
        # Debugè®¾ç½®
        self.debug_enabled = debug_dir is not None
        if self.debug_enabled:
            # ä½¿ç”¨ç±»ä¼¼åŸç‰ˆçš„ç›®å½•å‘½åï¼štimestamp_prompt_...ï¼ˆç¨åè®¾ç½®ï¼‰
            self.debug_base_dir = debug_dir
            self.debug_dir = None  # ç¨åæ ¹æ®promptåˆ›å»ºå…·ä½“ç›®å½•
            print(f"ğŸ› Debug mode enabled, base dir: {debug_dir}")
        else:
            self.debug_dir = None
        
        # Enhanced device selection
        if device is None:
            cuda_visible = os.environ.get('CUDA_VISIBLE_DEVICES', None)
            if cuda_visible is not None:
                device_num = cuda_visible.split(',')[0]
                self.device = f"cuda:{device_num}" if torch.cuda.is_available() else "cpu"
            else:
                if torch.cuda.is_available():
                    self.device = "cuda:3" if torch.cuda.device_count() > 3 else "cuda:0"
                else:
                    self.device = "cpu"
        else:
            self.device = device
        
        print(f"ğŸ”§ Device: {self.device}, Resolution: {self.resolution}")
        
        # Load model
        self._load_components()
        self._setup_image_processor()
        
        print(f"âœ… DynamiCrafter Guidance Pipeline initialized")
        
    def _load_components(self):
        """åŠ è½½ DynamiCrafter æ¨¡å‹"""
        # Download model if needed
        self._download_model()
        
        # Load model
        project_root = os.path.dirname(__file__)
        ckpt_path = os.path.join(project_root, f'checkpoints/dynamicrafter_{self.resolution[1]}_v1/model.ckpt')
        config_file = os.path.join(project_root, f'configs/inference_{self.resolution[1]}_v1.0.yaml')
        
        config = OmegaConf.load(config_file)
        model_config = config.pop("model", OmegaConf.create())
        model_config['params']['unet_config']['params']['use_checkpoint'] = False
        
        self.model = instantiate_from_config(model_config)
        assert os.path.exists(ckpt_path), f"Error: checkpoint Not Found at {ckpt_path}!"
        self.model = load_model_checkpoint(self.model, ckpt_path)
        self.model.eval()
        
        # è®¾ç½® perframe_ae
        if self.resolution[1] in [512, 1024]:
            self.model.perframe_ae = True
            print(f"âœ… Set perframe_ae=True for {self.resolution[1]} model")
        else:
            self.model.perframe_ae = False
            print(f"âœ… Set perframe_ae=False for {self.resolution[1]} model")
        
        # Move to device
        self.model = self.model.to(self.device)
        self._ensure_device_consistency()
        
        print(f"âœ… Model loaded: {self.resolution[1]}")
        
    def _ensure_device_consistency(self):
        """ç¡®ä¿è®¾å¤‡ä¸€è‡´æ€§"""
        def move_to_device(module):
            for name, param in module.named_parameters():
                if param.device != torch.device(self.device):
                    param.data = param.data.to(self.device)
            for name, buffer in module.named_buffers():
                if buffer.device != torch.device(self.device):
                    buffer.data = buffer.data.to(self.device)
            for name, child in module.named_children():
                move_to_device(child)
        
        move_to_device(self.model)
        
        # ç‰¹åˆ«å¤„ç†å„ä¸ªç¼–ç å™¨
        for component_name in ['cond_stage_model', 'first_stage_model', 'embedder', 'image_proj_model']:
            if hasattr(self.model, component_name):
                component = getattr(self.model, component_name)
                component = component.to(self.device)
                move_to_device(component)
                
    def _setup_image_processor(self):
        """è®¾ç½®å›¾åƒå¤„ç†å™¨"""
        self.image_processor = transforms.Compose([
            transforms.Resize(min(self.resolution)),
            transforms.CenterCrop(self.resolution),
        ])
        
    def _download_model(self):
        """ä¸‹è½½æ¨¡å‹"""
        from huggingface_hub import hf_hub_download
        
        REPO_ID = f'Doubiiu/DynamiCrafter_{self.resolution[1]}' if self.resolution[1] != 256 else 'Doubiiu/DynamiCrafter'
        filename_list = ['model.ckpt']
        
        project_root = os.path.dirname(__file__)
        model_dir = os.path.join(project_root, f'checkpoints/dynamicrafter_{self.resolution[1]}_v1/')
        
        if not os.path.exists(model_dir):
            os.makedirs(model_dir)
            
        for filename in filename_list:
            local_file = os.path.join(model_dir, filename)
            if not os.path.exists(local_file):
                print(f"ğŸ“¥ Downloading model: {filename}")
                hf_hub_download(
                    repo_id=REPO_ID, 
                    filename=filename, 
                    local_dir=model_dir, 
                    local_dir_use_symlinks=False
                )
                print(f"âœ… Download completed: {filename}")

    def _preprocess_image(self, image, height=None, width=None):
        """é¢„å¤„ç†å›¾åƒ"""
        if isinstance(image, Image.Image):
            image = np.array(image)
        
        if isinstance(image, np.ndarray):
            image = torch.from_numpy(image).permute(2, 0, 1).float()
        
        # æ ‡å‡†åŒ–åˆ° [-1, 1]
        if image.max() > 1.0:
            image = image / 255.0
        image = (image - 0.5) * 2
        
        # è°ƒæ•´å¤§å°
        if height is not None and width is not None:
            transform = transforms.Compose([
                transforms.Resize((height, width)),
            ])
        else:
            transform = self.image_processor
        
        return transform(image)

    def _encode_prompt(self, prompt, negative_prompt, device):
        """ç¼–ç æç¤ºè¯"""
        # ç¡®ä¿æ–‡æœ¬ç¼–ç å™¨åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        if hasattr(self.model, 'cond_stage_model'):
            self.model.cond_stage_model = self.model.cond_stage_model.to(device)
            # å¼ºåˆ¶è®¾ç½®è®¾å¤‡å±æ€§
            if hasattr(self.model.cond_stage_model, 'device'):
                self.model.cond_stage_model.device = device
            # é€’å½’ç¡®ä¿æ‰€æœ‰å­æ¨¡å—åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
            for name, module in self.model.cond_stage_model.named_modules():
                if hasattr(module, 'device'):
                    module.device = device
                for param in module.parameters():
                    if param.device != torch.device(device):
                        param.data = param.data.to(device)
        
        # æ­£å‘æç¤º
        text_embeddings = self.model.get_learned_conditioning(prompt)
        
        # è´Ÿå‘æç¤ºï¼ˆå¦‚æœæä¾›ï¼‰
        if negative_prompt is not None:
            uncond_embeddings = self.model.get_learned_conditioning(negative_prompt)
        else:
            if self.model.uncond_type == "empty_seq":
                uncond_embeddings = self.model.get_learned_conditioning([""] * len(prompt))
            elif self.model.uncond_type == "zero_embed":
                uncond_embeddings = torch.zeros_like(text_embeddings)
        
        return {"cond": text_embeddings, "uncond": uncond_embeddings}
    
    def _encode_image(self, image, num_frames):
        """ç¼–ç å›¾åƒ"""
        # æ·»åŠ æ‰¹æ¬¡ç»´åº¦
        if image.dim() == 3:
            image = image.unsqueeze(0)
        
        image = image.to(self.device)
        
        # å›¾åƒåµŒå…¥
        cond_images = self.model.embedder(image)
        img_emb = self.model.image_proj_model(cond_images)
        
        # æ½œåœ¨è¡¨ç¤º
        videos = image.unsqueeze(2)  # æ·»åŠ æ—¶é—´ç»´åº¦
        z = get_latent_z(self.model, videos)
        
        return img_emb, z
    
    def _prepare_conditioning(self, text_embeddings, image_embeddings, image_latents, 
                             frame_stride, guidance_scale, batch_size, num_frames=None):
        """å‡†å¤‡æ¡ä»¶è¾“å…¥"""
        # ç»„åˆæ–‡æœ¬å’Œå›¾åƒåµŒå…¥
        cond_emb = torch.cat([text_embeddings["cond"], image_embeddings], dim=1)
        cond = {"c_crossattn": [cond_emb]}
        
        # å›¾åƒæ¡ä»¶ï¼ˆhybridæ¨¡å¼ï¼‰
        if self.model.model.conditioning_key == 'hybrid':
            img_cat_cond = image_latents[:,:,:1,:,:]
            # ä½¿ç”¨å®é™…çš„ num_frames
            actual_num_frames = num_frames if num_frames is not None else self.model.temporal_length
            img_cat_cond = img_cat_cond.repeat(1, 1, actual_num_frames, 1, 1)
            cond["c_concat"] = [img_cat_cond]
        
        # æ— æ¡ä»¶è¾“å…¥
        uc = None
        if guidance_scale != 1.0:
            # åˆ›å»ºæ­£ç¡®å½¢çŠ¶çš„é›¶å›¾åƒ
            zero_image = torch.zeros((batch_size, 3, self.resolution[0], self.resolution[1]), 
                                   device=self.model.device, dtype=self.model.dtype)
            uc_img_emb = self.model.embedder(zero_image)
            uc_img_emb = self.model.image_proj_model(uc_img_emb)
            
            uc_emb = torch.cat([text_embeddings["uncond"], uc_img_emb], dim=1)
            uc = {"c_crossattn": [uc_emb]}
            
            if self.model.model.conditioning_key == 'hybrid':
                uc["c_concat"] = [img_cat_cond]
        
        # å¸§æ­¥é•¿
        fs = torch.tensor([frame_stride] * batch_size, dtype=torch.long, device=self.model.device)
        
        return {"cond": cond, "uc": uc, "fs": fs}

    def _decode_latents(self, latents):
        """è§£ç æ½œåœ¨è¡¨ç¤º"""
        with torch.no_grad():
            videos = self.model.decode_first_stage(latents)
        return videos

    def _sample_timestep(self, batch_size, min_step_ratio=0.02, max_step_ratio=0.98):
        """é‡‡æ ·æ—¶é—´æ­¥"""
        from lvdm.models.utils_diffusion import make_ddim_timesteps
        
        # æ ¹æ®æ¨¡å‹åˆ†è¾¨ç‡é€‰æ‹©åˆé€‚çš„ timestep_spacing
        if self.resolution[1] in [512, 1024]:
            timestep_spacing = 'uniform_trailing'
        else:
            timestep_spacing = 'uniform'
        
        ddim_timesteps = make_ddim_timesteps(
            ddim_discr_method=timestep_spacing,
            num_ddim_timesteps=50,  # ä½¿ç”¨50æ­¥ä½œä¸ºé»˜è®¤
            num_ddpm_timesteps=self.model.num_timesteps,
            verbose=False
        )
        
        # æ ¹æ®æ¯”ä¾‹é€‰æ‹©æ—¶é—´æ­¥èŒƒå›´
        min_idx = int(len(ddim_timesteps) * min_step_ratio)
        max_idx = int(len(ddim_timesteps) * max_step_ratio)
        max_idx = max(max_idx, min_idx + 1)
        
        # éšæœºé‡‡æ ·æ—¶é—´æ­¥ç´¢å¼•
        t_idx = torch.randint(min_idx, max_idx, (batch_size,), device="cpu")
        
        # è·å–å¯¹åº”çš„æ—¶é—´æ­¥å€¼
        t_values = ddim_timesteps[t_idx.cpu().numpy()]
        t = torch.from_numpy(t_values).long().to(self.device)
        
        return t

    def _add_noise(self, original_samples, noise, timesteps):
        """æ·»åŠ å™ªå£° - DDIM è°ƒåº¦"""
        # è·å– alpha_cumprod
        alphas_cumprod = self.model.alphas_cumprod.to(self.device)
        
        # ç¡®ä¿æ—¶é—´æ­¥åœ¨æ­£ç¡®èŒƒå›´å†…
        timesteps = timesteps.clamp(0, len(alphas_cumprod) - 1)
        
        # è·å–å¯¹åº”çš„ alpha_cumprod å€¼
        alpha_t = alphas_cumprod[timesteps]
        
        # ç¡®ä¿ç»´åº¦åŒ¹é…
        while len(alpha_t.shape) < len(original_samples.shape):
            alpha_t = alpha_t.unsqueeze(-1)
        
        # DDIM å™ªå£°æ·»åŠ 
        sqrt_alpha_t = torch.sqrt(alpha_t)
        sqrt_one_minus_alpha_t = torch.sqrt(1.0 - alpha_t)
        
        noisy_samples = sqrt_alpha_t * original_samples + sqrt_one_minus_alpha_t * noise
        return noisy_samples

    def _apply_guidance_rescale(self, noise_pred_cond, noise_pred_uncond, cfg_scale):
        """åº”ç”¨ guidance rescale"""
        # æ ‡å‡† CFG
        noise_pred = noise_pred_uncond + cfg_scale * (noise_pred_cond - noise_pred_uncond)
        
        # å¯¹äº 512 å’Œ 1024 æ¨¡å‹ï¼Œåº”ç”¨ guidance_rescale
        if self.resolution[1] in [512, 1024]:
            guidance_rescale = 0.7
            
            # å®ç° guidance rescale é€»è¾‘
            std_text = noise_pred_cond.std(dim=list(range(1, noise_pred_cond.ndim)), keepdim=True)
            std_cfg = noise_pred.std(dim=list(range(1, noise_pred.ndim)), keepdim=True)
            
            # rescale the results from guidance (prevent over-exposure)
            noise_pred_rescaled = noise_pred * (std_text / std_cfg)
            
            # mix with the original results from guidance by factor guidance_rescale
            noise_pred = guidance_rescale * noise_pred_rescaled + (1 - guidance_rescale) * noise_pred
        
        return noise_pred

    def _sds_loss(self, latents, conditioning, cfg_scale=7.5, min_step_ratio=0.02, max_step_ratio=0.98, weight_type="t"):
        """
        æ ¸å¿ƒ SDS loss è®¡ç®—
        
        SDS (Score Distillation Sampling) Loss åŸç†ï¼š
        1. éšæœºé‡‡æ ·æ—¶é—´æ­¥ t
        2. å‘ latents æ·»åŠ å™ªå£°å¾—åˆ° noisy_latents
        3. ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹é¢„æµ‹å™ªå£° noise_pred
        4. é€šè¿‡ DDIM reverse è®¡ç®—é¢„æµ‹çš„åŸå§‹æ ·æœ¬
        5. è®¡ç®— SDS æ¢¯åº¦ï¼šâˆ‡_latents = w(t) * (latents - pred_original_sample)
        6. æ„å»ºå·§å¦™çš„æŸå¤±å‡½æ•°ï¼Œä½¿å…¶æ¢¯åº¦ç­‰äº SDS æ¢¯åº¦
        """
        batch_size = latents.shape[0]
        
        # 1. é‡‡æ ·æ—¶é—´æ­¥
        t = self._sample_timestep(batch_size, min_step_ratio, max_step_ratio)
        
        # 2. æ·»åŠ å™ªå£°
        noise = torch.randn_like(latents)
        noisy_latents = self._add_noise(latents, noise, t)
        
        # 3. å‡†å¤‡æ¡ä»¶
        cond = conditioning["cond"]
        uc = conditioning["uc"]
        fs = conditioning["fs"]
        
        # 4. å‰å‘ä¼ æ’­ - ä½¿ç”¨ DynamiCrafter çš„ model.apply_model
        with torch.no_grad():
            # æ¡ä»¶é¢„æµ‹
            noise_pred_cond = self.model.apply_model(noisy_latents, t, cond, **{"fs": fs})
            
            # æ— æ¡ä»¶é¢„æµ‹
            if uc is not None and cfg_scale > 1.0:
                noise_pred_uncond = self.model.apply_model(noisy_latents, t, uc, **{"fs": fs})
                # åº”ç”¨ guidance_rescale (å¯¹ 512/1024 æ¨¡å‹å…³é”®)
                noise_pred = self._apply_guidance_rescale(noise_pred_cond, noise_pred_uncond, cfg_scale)
            else:
                noise_pred = noise_pred_cond
        
        # 5. è®¡ç®—é¢„æµ‹çš„åŸå§‹æ ·æœ¬ (DDIM reverse)
        alpha_t = self.model.alphas_cumprod[t].to(self.device)
        while len(alpha_t.shape) < len(latents.shape):
            alpha_t = alpha_t.unsqueeze(-1)
        
        sqrt_alpha_t = torch.sqrt(alpha_t)
        sqrt_one_minus_alpha_t = torch.sqrt(1.0 - alpha_t)
        
        # DDIM reverse formula: x_0 = (x_t - sqrt(1-Î±_t) * Îµ_Î¸) / sqrt(Î±_t)
        pred_original_sample = (noisy_latents - sqrt_one_minus_alpha_t * noise_pred) / sqrt_alpha_t
        
        # 6. è®¡ç®— SDS æ¢¯åº¦ (ä¸åŒæƒé‡ç­–ç•¥)
        if weight_type == "t":
            # åŸºäºæ—¶é—´æ­¥çš„æƒé‡ w(t) = 1 - Î±_t
            w = (1.0 - alpha_t).view(batch_size, 1, 1, 1, 1)
            grad = w * (latents - pred_original_sample.detach())
        elif weight_type == "ada":
            # è‡ªé€‚åº”æƒé‡ï¼šåŸºäºé¢„æµ‹è¯¯å·®çš„å¤§å°
            weighting_factor = torch.abs(latents - pred_original_sample.detach()).mean(dim=(1, 2, 3, 4), keepdim=True)
            weighting_factor = torch.clamp(weighting_factor, 1e-4)
            grad = (latents - pred_original_sample.detach()) / weighting_factor
        elif weight_type == "uniform":
            # å‡åŒ€æƒé‡
            grad = (latents - pred_original_sample.detach())
        else:
            raise ValueError(f"Unknown weight_type: {weight_type}")
        
        grad = grad.detach()
        grad = torch.nan_to_num(grad)
        
        # 7. æ„å»ºå·§å¦™çš„æŸå¤±å‡½æ•°
        # å…³é”®æŠ€å·§ï¼šæ„é€ æŸå¤±å‡½æ•° L = 0.5 * ||latents - target||^2
        # å…¶ä¸­ target = latents - gradï¼Œä½¿å¾— âˆ‡_latents L = latents - target = grad
        target = (latents - grad).detach()
        loss = 0.5 * F.mse_loss(latents, target, reduction="mean") / batch_size
        
        torch.cuda.empty_cache()
        
        return loss

    def _create_debug_structure(self, prompt: str, config_info: Dict):
        """åˆ›å»ºå®Œæ•´çš„debugç›®å½•ç»“æ„"""
        if not self.debug_enabled:
            return
        
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            # æ¸…ç†promptç”¨äºæ–‡ä»¶å
            safe_prompt = "".join(c if c.isalnum() or c in (' ', '-', '_') else '' for c in prompt)
            safe_prompt = '_'.join(safe_prompt.split())[:50]  # é™åˆ¶é•¿åº¦
            
            # åˆ›å»ºç±»ä¼¼åŸç‰ˆçš„ç›®å½•å
            loss_type = config_info.get('loss_type', 'sds')
            weight_type = config_info.get('weight_type', 't')
            lr = config_info.get('learning_rate', 0.05)
            steps = config_info.get('num_optimization_steps', 100)
            cfg = config_info.get('cfg_scale', 7.5)
            
            dir_name = f"{timestamp}_{safe_prompt}__{loss_type}_{weight_type}_lr{lr}_steps{steps}_cfg{cfg}"
            self.debug_dir = os.path.join(self.debug_base_dir, dir_name)
            
            # åˆ›å»ºå­ç›®å½•ç»“æ„
            self.debug_subdirs = {
                'inputs': os.path.join(self.debug_dir, 'inputs'),
                'outputs': os.path.join(self.debug_dir, 'outputs'),
                'process': os.path.join(self.debug_dir, 'process'),
                'debug': os.path.join(self.debug_dir, 'debug'),
                'params': os.path.join(self.debug_dir, 'params')
            }
            
            for subdir in self.debug_subdirs.values():
                os.makedirs(subdir, exist_ok=True)
            
            print(f"ğŸ› Debug structure created: {self.debug_dir}")
            
        except Exception as e:
            print(f"âš ï¸  Debug structure creation failed: {e}")

    def _save_input_files(self, original_image, processed_image, prompt: str):
        """ä¿å­˜è¾“å…¥æ–‡ä»¶åˆ°inputsç›®å½•"""
        if not self.debug_enabled or not hasattr(self, 'debug_subdirs'):
            return
        
        try:
            inputs_dir = self.debug_subdirs['inputs']
            
            # ä¿å­˜åŸå§‹å›¾åƒ
            if isinstance(original_image, Image.Image):
                original_image.save(os.path.join(inputs_dir, 'original_image.png'))
            elif isinstance(original_image, np.ndarray):
                if original_image.max() <= 1.0:
                    original_image = (original_image * 255).astype(np.uint8)
                Image.fromarray(original_image).save(os.path.join(inputs_dir, 'original_image.png'))
            
            # ä¿å­˜é¢„å¤„ç†å›¾åƒ
            if isinstance(processed_image, torch.Tensor):
                # è½¬æ¢ä¸ºPILæ ¼å¼ä¿å­˜
                if processed_image.dim() == 3:
                    # åæ ‡å‡†åŒ–: [-1, 1] -> [0, 1]
                    processed_np = ((processed_image + 1.0) / 2.0).clamp(0, 1)
                    processed_np = processed_np.permute(1, 2, 0).cpu().numpy()
                    processed_np = (processed_np * 255).astype(np.uint8)
                    Image.fromarray(processed_np).save(os.path.join(inputs_dir, 'processed_image.png'))
            
            # ä¿å­˜prompt
            with open(os.path.join(inputs_dir, 'prompt.txt'), 'w', encoding='utf-8') as f:
                f.write(prompt)
            
            print(f"âœ… Input files saved to {inputs_dir}")
            
        except Exception as e:
            print(f"âš ï¸  Input files save failed: {e}")

    def _save_parameters(self, config_info: Dict):
        """ä¿å­˜å‚æ•°é…ç½®åˆ°paramsç›®å½•"""
        if not self.debug_enabled or not hasattr(self, 'debug_subdirs'):
            return
        
        try:
            params_dir = self.debug_subdirs['params']
            
            # ä¿å­˜ä¸ºtxtæ ¼å¼ï¼ˆç±»ä¼¼åŸç‰ˆï¼‰
            txt_path = os.path.join(params_dir, 'parameters.txt')
            with open(txt_path, 'w', encoding='utf-8') as f:
                f.write("DynamiCrafter Guidance Pipeline Parameters\n")
                f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write("=" * 60 + "\n\n")
                
                for key, value in config_info.items():
                    f.write(f"{key}: {value}\n")
            
            # ä¿å­˜ä¸ºjsonæ ¼å¼
            json_path = os.path.join(params_dir, 'parameters.json')
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(config_info, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… Parameters saved to {params_dir}")
            
        except Exception as e:
            print(f"âš ï¸  Parameters save failed: {e}")

    def _save_debug_step(self, step: int, loss: float, latents: torch.Tensor, 
                        conditioning: Dict = None, save_interval: int = 100):
        """ä¿å­˜debugæ­¥éª¤ä¿¡æ¯ï¼ˆç±»ä¼¼åŸç‰ˆæ¯100æ­¥ä¿å­˜ï¼‰"""
        if not self.debug_enabled or not hasattr(self, 'debug_subdirs'):
            return
        
        # ä¿å­˜lossåˆ°åˆ—è¡¨ï¼ˆæ¯æ­¥éƒ½è®°å½•ï¼‰
        if not hasattr(self, 'debug_losses'):
            self.debug_losses = []
        self.debug_losses.append(loss)
        
        # æ¯save_intervalæ­¥ä¿å­˜è¯¦ç»†debugä¿¡æ¯
        if step % save_interval == 0:
            try:
                debug_dir = self.debug_subdirs['debug']
                
                with torch.no_grad():
                    # è§£ç å®Œæ•´è§†é¢‘
                    videos = self.model.decode_first_stage(latents)
                    videos = torch.clamp((videos + 1.0) / 2.0, 0, 1)
                    
                    batch_size, channels, num_frames, height, width = videos.shape
                    video_np = videos[0].permute(1, 2, 3, 0).cpu().numpy()  # [T, H, W, C]
                    
                    # ä¿å­˜ç¬¬ä¸€å¸§ï¼ˆç±»ä¼¼åŸç‰ˆçš„frame_00.pngï¼‰
                    first_frame = (video_np[0] * 255).astype(np.uint8)
                    frame_path = os.path.join(debug_dir, f"step_{step:06d}_frame_00.png")
                    Image.fromarray(first_frame).save(frame_path)
                    
                    # ä¿å­˜ä¸­é—´å¸§ï¼ˆç±»ä¼¼åŸç‰ˆçš„frame.pngï¼‰
                    mid_frame_idx = num_frames // 2
                    mid_frame = (video_np[mid_frame_idx] * 255).astype(np.uint8)
                    mid_frame_path = os.path.join(debug_dir, f"step_{step:06d}_frame.png")
                    Image.fromarray(mid_frame).save(mid_frame_path)
                    
                    # ä¿å­˜å®Œæ•´è§†é¢‘ï¼ˆç±»ä¼¼åŸç‰ˆçš„video.mp4ï¼‰
                    video_path = os.path.join(debug_dir, f"step_{step:06d}_video.mp4")
                    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
                    out = cv2.VideoWriter(video_path, fourcc, 8.0, (width, height))
                    
                    for frame_idx in range(num_frames):
                        frame = (video_np[frame_idx] * 255).astype(np.uint8)
                        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                        out.write(frame_bgr)
                    
                    out.release()
                    
                    print(f"ğŸ› Debug step {step} saved: frame_00, frame, video")
                    
            except Exception as e:
                print(f"âš ï¸  Debug step {step} save failed: {e}")

    def _save_process_video(self):
        """åˆ›å»ºä¼˜åŒ–è¿‡ç¨‹è§†é¢‘ï¼ˆç±»ä¼¼åŸç‰ˆçš„optimization_process.mp4ï¼‰"""
        if not self.debug_enabled or not hasattr(self, 'debug_subdirs'):
            return
        
        try:
            debug_dir = self.debug_subdirs['debug']
            process_dir = self.debug_subdirs['process']
            
            # æ”¶é›†æ‰€æœ‰ä¸­é—´å¸§
            frame_files = []
            for file in sorted(os.listdir(debug_dir)):
                if file.endswith('_frame.png'):  # ä½¿ç”¨ä¸­é—´å¸§
                    frame_files.append(os.path.join(debug_dir, file))
            
            if len(frame_files) < 2:
                print("âš ï¸  Not enough frames for process video")
                return
            
            # åˆ›å»ºä¼˜åŒ–è¿‡ç¨‹è§†é¢‘
            process_video_path = os.path.join(process_dir, 'optimization_process.mp4')
            
            # è¯»å–ç¬¬ä¸€å¸§ç¡®å®šå°ºå¯¸
            first_frame = cv2.imread(frame_files[0])
            height, width = first_frame.shape[:2]
            
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(process_video_path, fourcc, 2.0, (width, height))  # è¾ƒæ…¢çš„å¸§ç‡
            
            for frame_file in frame_files:
                frame = cv2.imread(frame_file)
                out.write(frame)
            
            out.release()
            print(f"âœ… Process video saved: {process_video_path}")
            
        except Exception as e:
            print(f"âš ï¸  Process video creation failed: {e}")

    def _save_final_outputs(self, videos: torch.Tensor):
        """ä¿å­˜æœ€ç»ˆè¾“å‡ºåˆ°outputsç›®å½•"""
        if not self.debug_enabled or not hasattr(self, 'debug_subdirs'):
            return
        
        try:
            outputs_dir = self.debug_subdirs['outputs']
            
            # ä¿å­˜æœ€ç»ˆè§†é¢‘ï¼ˆç±»ä¼¼åŸç‰ˆçš„final_video_000.mp4ï¼‰
            videos_np = videos.detach().cpu().numpy()
            videos_np = np.clip((videos_np + 1.0) / 2.0, 0, 1)
            
            batch_size, channels, num_frames, height, width = videos_np.shape
            
            for batch_idx in range(batch_size):
                video_np = videos_np[batch_idx].transpose(1, 2, 3, 0)  # [T, H, W, C]
                
                # ä¿å­˜ä¸ºMP4ï¼ˆä¸»è¦æ ¼å¼ï¼‰
                mp4_path = os.path.join(outputs_dir, f'final_video_{batch_idx:03d}.mp4')
                fourcc = cv2.VideoWriter_fourcc(*'mp4v')
                out = cv2.VideoWriter(mp4_path, fourcc, 8.0, (width, height))
                
                for frame_idx in range(num_frames):
                    frame = (video_np[frame_idx] * 255).astype(np.uint8)
                    frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                    out.write(frame_bgr)
                
                out.release()
                
                # é¢å¤–ä¿å­˜ä¸ºGIFï¼ˆä¾¿äºé¢„è§ˆï¼‰
                gif_path = os.path.join(outputs_dir, f'final_video_{batch_idx:03d}.gif')
                gif_frames = []
                for frame_idx in range(num_frames):
                    frame = (video_np[frame_idx] * 255).astype(np.uint8)
                    gif_frames.append(Image.fromarray(frame))
                
                gif_frames[0].save(
                    gif_path, 
                    save_all=True, 
                    append_images=gif_frames[1:], 
                    duration=125,  # 8fps
                    loop=0
                )
            
            print(f"âœ… Final outputs saved to {outputs_dir}")
            
        except Exception as e:
            print(f"âš ï¸  Final outputs save failed: {e}")

    def _save_loss_analysis(self):
        """ä¿å­˜lossåˆ†æåˆ°processç›®å½•"""
        if not self.debug_enabled or not hasattr(self, 'debug_subdirs') or not hasattr(self, 'debug_losses'):
            return
        
        try:
            process_dir = self.debug_subdirs['process']
            
            # ä¿å­˜lossæ•°æ®
            loss_data_path = os.path.join(process_dir, 'loss_data.txt')
            with open(loss_data_path, 'w') as f:
                f.write("# Step\tLoss\n")
                for i, loss in enumerate(self.debug_losses):
                    f.write(f"{i}\t{loss:.8f}\n")
            
            # åˆ›å»ºlossåˆ†æå›¾
            if len(self.debug_losses) > 1:
                plt.figure(figsize=(15, 10))
                
                # ä¸»å›¾ï¼šå®Œæ•´lossæ›²çº¿
                plt.subplot(2, 3, 1)
                plt.plot(self.debug_losses)
                plt.title('Complete SDS Loss Curve')
                plt.xlabel('Step')
                plt.ylabel('Loss')
                plt.yscale('log')
                plt.grid(True)
                
                # å­å›¾ï¼šæœ€å50æ­¥
                plt.subplot(2, 3, 2)
                recent_losses = self.debug_losses[-50:] if len(self.debug_losses) > 50 else self.debug_losses
                plt.plot(range(len(self.debug_losses)-len(recent_losses), len(self.debug_losses)), recent_losses)
                plt.title('Last 50 Steps')
                plt.xlabel('Step')
                plt.ylabel('Loss')
                plt.grid(True)
                
                # å­å›¾ï¼šlosså˜åŒ–ç‡
                loss_diff = np.diff(self.debug_losses)
                plt.subplot(2, 3, 3)
                plt.plot(loss_diff)
                plt.title('Loss Change Rate')
                plt.xlabel('Step')
                plt.ylabel('âˆ†Loss')
                plt.grid(True)
                
                # å­å›¾ï¼šlossåˆ†å¸ƒ
                plt.subplot(2, 3, 4)
                plt.hist(self.debug_losses, bins=50, alpha=0.7)
                plt.title('Loss Distribution')
                plt.xlabel('Loss Value')
                plt.ylabel('Frequency')
                plt.yscale('log')
                
                # å­å›¾ï¼šæ»‘åŠ¨å¹³å‡
                if len(self.debug_losses) > 10:
                    window_size = min(10, len(self.debug_losses) // 10)
                    moving_avg = np.convolve(self.debug_losses, np.ones(window_size)/window_size, mode='valid')
                    plt.subplot(2, 3, 5)
                    plt.plot(self.debug_losses, alpha=0.3, label='Original')
                    plt.plot(range(window_size-1, len(self.debug_losses)), moving_avg, label=f'Moving Avg ({window_size})')
                    plt.title('Loss with Moving Average')
                    plt.xlabel('Step')
                    plt.ylabel('Loss')
                    plt.yscale('log')
                    plt.legend()
                    plt.grid(True)
                
                # å­å›¾ï¼šlossç»Ÿè®¡
                plt.subplot(2, 3, 6)
                stats_text = f"""
                Total Steps: {len(self.debug_losses)}
                Final Loss: {self.debug_losses[-1]:.6e}
                Min Loss: {min(self.debug_losses):.6e}
                Max Loss: {max(self.debug_losses):.6e}
                Mean Loss: {np.mean(self.debug_losses):.6e}
                Std Loss: {np.std(self.debug_losses):.6e}
                """
                plt.text(0.1, 0.5, stats_text, transform=plt.gca().transAxes, 
                         fontsize=10, verticalalignment='center', fontfamily='monospace')
                plt.title('Loss Statistics')
                plt.axis('off')
                
                plt.tight_layout()
                plt.savefig(os.path.join(process_dir, 'loss_analysis.png'), dpi=150, bbox_inches='tight')
                plt.close()
            
            print(f"âœ… Loss analysis saved to {process_dir}")
            
        except Exception as e:
            print(f"âš ï¸  Loss analysis save failed: {e}")

    def _optimization_loop(self, noise_shape, conditioning, device, num_optimization_steps=100, 
                          learning_rate=0.05, cfg_scale=7.5, optimizer_type="Adam", debug_save_interval=100, **kwargs):
        """ä¼˜åŒ–å¾ªç¯ - æ›¿æ¢åŸæ¥çš„ scheduler.sample()"""
        
        # åˆå§‹åŒ– latents
        latents = torch.randn(noise_shape, device=device, dtype=torch.float32)
        latents = latents.detach().clone()
        latents.requires_grad_(True)
        
        # è®¾ç½®ä¼˜åŒ–å™¨
        if optimizer_type == "AdamW":
            optimizer = torch.optim.AdamW([latents], lr=learning_rate, betas=(0.9, 0.99), eps=1e-8)
        elif optimizer_type == "Adam":
            optimizer = torch.optim.Adam([latents], lr=learning_rate, betas=(0.9, 0.999), eps=1e-8)
        else:
            raise ValueError(f"Unknown optimizer_type: {optimizer_type}")
        
        print(f"[INFO] Starting SDS optimization: {num_optimization_steps} steps, lr={learning_rate}, cfg={cfg_scale}")
        
        # Debugä¿¡æ¯åˆå§‹åŒ–
        if self.debug_enabled:
            self.debug_losses = []
            print(f"ğŸ› Debug tracking enabled, save interval: {debug_save_interval}")
        
        # ä¼˜åŒ–å¾ªç¯
        for i in range(num_optimization_steps):
            optimizer.zero_grad()
            
            # è®¡ç®— SDS loss
            loss = self._sds_loss(latents, conditioning, cfg_scale=cfg_scale)
            
            loss.backward()
            optimizer.step()
            
            # Debugä¿å­˜ï¼ˆä½¿ç”¨ç±»ä¼¼åŸç‰ˆçš„é—´éš”ï¼‰
            self._save_debug_step(
                step=i, 
                loss=loss.item(), 
                latents=latents,
                conditioning=conditioning,
                save_interval=debug_save_interval
            )
            
            # è¿›åº¦æ—¥å¿—
            if i % max(1, num_optimization_steps // 10) == 0:
                print(f"[PROGRESS] Step {i}/{num_optimization_steps} - Loss: {loss.item():.6f}")
        
        print(f"[INFO] Optimization completed!")
        
        return latents.detach()

    def __call__(
        self,
        image: Union[Image.Image, np.ndarray, torch.Tensor],
        prompt: Union[str, List[str]] = "",
        negative_prompt: Optional[Union[str, List[str]]] = None,
        # === DynamiCrafter æ ‡å‡†å‚æ•° ===
        num_inference_steps: int = 50,
        guidance_scale: float = 7.5,
        eta: float = 1.0,  # ä¿®æ­£é»˜è®¤å€¼
        frame_stride: int = 24,  # ä¿®æ­£é»˜è®¤å€¼
        num_frames: Optional[int] = None,
        height: Optional[int] = None,
        width: Optional[int] = None,
        num_videos_per_prompt: int = 1,
        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,
        latents: Optional[torch.FloatTensor] = None,
        output_type: str = "tensor",
        return_dict: bool = True,
        # === Guidance å‚æ•° ===
        num_optimization_steps: int = 100,
        learning_rate: float = 0.05,
        loss_type: str = "sds",
        weight_type: str = "t",
        cfg_scale: Optional[float] = None,
        optimizer_type: str = "Adam",
        debug_save_interval: int = 100,  # æ–°å¢ï¼šdebugä¿å­˜é—´éš”
        **kwargs
    ) -> Union[torch.Tensor, Dict[str, Any]]:
        """
        Generate video using DynamiCrafter with guidance optimization.
        
        å¤§éƒ¨åˆ†é€»è¾‘æ¥è‡ª dynamicrafter_pipeline.pyï¼Œåªæ›¿æ¢ scheduler.sample() éƒ¨åˆ†ä¸º optimization loop
        """
        
        # è¾“å…¥éªŒè¯å’Œæ ‡å‡†åŒ–
        if isinstance(prompt, str):
            batch_size = 1
            prompt = [prompt]
        else:
            batch_size = len(prompt)
        
        # å¤„ç† negative_prompt
        if negative_prompt is not None:
            if isinstance(negative_prompt, str):
                negative_prompt = [negative_prompt] * batch_size
            elif len(negative_prompt) != batch_size:
                raise ValueError(f"negative_prompt length ({len(negative_prompt)}) != batch_size ({batch_size})")
        
        # è®¾å¤‡ç®¡ç†
        device = self.device
        
        # å¤„ç† cfg_scale å‚æ•°
        if cfg_scale is None:
            cfg_scale = guidance_scale
        
        print(f"ğŸ¬ DynamiCrafter Guidance - Prompt: {prompt[0]}")
        print(f"ğŸ”§ Parameters: steps={num_optimization_steps}, lr={learning_rate}, loss={loss_type}, cfg={cfg_scale}")
        print(f"ğŸ’» Device: {device}")
        
        # å›¾åƒé¢„å¤„ç†
        processed_image = self._preprocess_image(image, height, width)
        
        # æ¨¡å‹å‚æ•°
        num_frames = num_frames or self.model.temporal_length
        channels = self.model.model.diffusion_model.out_channels
        if height is None or width is None:
            height, width = self.resolution
        latent_height, latent_width = height // 8, width // 8
        
        # å™ªå£°å½¢çŠ¶
        noise_shape = (batch_size, channels, num_frames, latent_height, latent_width)
        print(f"ğŸ“Š Noise shape: {noise_shape}")
        
        # å‡†å¤‡é…ç½®ä¿¡æ¯ï¼ˆç”¨äºdebugï¼‰
        config_info = {
            'prompt': prompt[0] if isinstance(prompt, list) else prompt,
            'negative_prompt': negative_prompt[0] if negative_prompt else "None",
            'num_optimization_steps': num_optimization_steps,
            'learning_rate': learning_rate,
            'cfg_scale': cfg_scale,
            'optimizer_type': optimizer_type,
            'loss_type': loss_type,
            'weight_type': weight_type,
            'resolution': f"{height}x{width}",
            'num_frames': num_frames,
            'frame_stride': frame_stride,
            'device': device,
            'noise_shape': str(noise_shape),
            'debug_save_interval': debug_save_interval,
            'batch_size': batch_size,
            'channels': channels,
            'latent_height': latent_height,
            'latent_width': latent_width
        }
        
        # åˆ›å»ºdebugç»“æ„
        self._create_debug_structure(prompt[0], config_info)
        
        # ä¿å­˜è¾“å…¥æ–‡ä»¶
        self._save_input_files(image, processed_image, prompt[0])
        
        # ä¿å­˜å‚æ•°é…ç½®
        self._save_parameters(config_info)
        
        with torch.no_grad():
            # ç¼–ç æç¤ºè¯å’Œå›¾åƒ
            text_embeddings = self._encode_prompt(prompt, negative_prompt, device)
            image_embeddings, image_latents = self._encode_image(processed_image, num_frames)
            
            # å‡†å¤‡æ¡ä»¶è¾“å…¥
            conditioning = self._prepare_conditioning(
                text_embeddings=text_embeddings,
                image_embeddings=image_embeddings,
                image_latents=image_latents,
                frame_stride=frame_stride,
                guidance_scale=guidance_scale,
                batch_size=batch_size,
                num_frames=num_frames
            )
        
        # ===== å…³é”®æ›¿æ¢ç‚¹ï¼šç”¨ optimization loop æ›¿æ¢ scheduler.sample() =====
        samples = self._optimization_loop(
            noise_shape=noise_shape,
            conditioning=conditioning,
            device=device,
            num_optimization_steps=num_optimization_steps,
            learning_rate=learning_rate,
            cfg_scale=cfg_scale,
            optimizer_type=optimizer_type,
            debug_save_interval=debug_save_interval,
            generator=generator,
            **kwargs
        )
        
        # è§£ç  latents åˆ°è§†é¢‘
        print(f"ğŸï¸ Decoding latents...")
        videos = self._decode_latents(samples)
        
        print(f"âœ… Video generated! Shape: {videos.shape}")
        
        # ä¿å­˜æœ€ç»ˆç»“æœå’Œå®Œæ•´debugä¿¡æ¯
        if self.debug_enabled:
            self._save_final_outputs(videos)
            self._save_process_video()
            self._save_loss_analysis()
            print(f"ğŸ› Complete debug results saved to: {self.debug_dir}")
        
        # è¿”å›ç»“æœ
        if return_dict:
            return {"videos": videos}
        else:
            return videos


# æµ‹è¯•å‡½æ•°
def test_sds_logic_with_complete_debug():
    """æµ‹è¯• SDS é€»è¾‘ + å®Œæ•´DebugåŠŸèƒ½"""
    print("ğŸ§ª Testing SDS Logic with Complete Debug Structure")
    print("=" * 60)
    
    try:
        # å¯ç”¨debugæ¨¡å¼
        pipeline = DynamiCrafterGuidancePipeline(
            resolution='256_256',
            debug_dir='./results_dynamicrafter_guidance'  # ä½¿ç”¨ç±»ä¼¼åŸç‰ˆçš„ç›®å½•
        )
        
        # åˆ›å»ºæµ‹è¯•å›¾åƒ
        test_image = Image.fromarray(np.random.randint(0, 255, (256, 256, 3), dtype=np.uint8))
        
        # æµ‹è¯•ï¼ˆä½¿ç”¨è¾ƒå°‘æ­¥æ•°ä½†è¶³å¤Ÿçœ‹åˆ°debugç»“æ„ï¼‰
        result = pipeline(
            image=test_image,
            prompt="a person walking in the garden",
            num_optimization_steps=50,  # è¶³å¤Ÿç”Ÿæˆå¤šä¸ªdebugç‚¹
            learning_rate=0.05,
            cfg_scale=7.5,
            debug_save_interval=20  # æ¯20æ­¥ä¿å­˜ä¸€æ¬¡debugä¿¡æ¯
        )
        
        videos = result["videos"] if isinstance(result, dict) else result
        print(f"âœ… SDS test with complete debug completed! Result shape: {videos.shape}")
        
        if torch.isnan(videos).any():
            print("âŒ NaN detected!")
            return False
        else:
            print("âœ… No NaN issues")
            print(f"ğŸ› Complete debug files saved to: {pipeline.debug_dir}")
            return True
            
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == '__main__':
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == 'test':
        test_sds_logic_with_complete_debug()
    else:
        print("ğŸ“ DynamiCrafter Guidance Pipeline - Complete Debug Version")
        print("\nğŸ¯ Key Features:")
        print("  â€¢ ä¸“æ³¨äºæ ¸å¿ƒ SDS loss é€»è¾‘")
        print("  â€¢ ä¿æŒ DynamiCrafter çš„æ‰€æœ‰æ ¸å¿ƒé€»è¾‘")
        print("  â€¢ åªæ›¿æ¢ scheduler.sample() ä¸º optimization loop")
        print("  â€¢ å®Œæ•´debugä¿å­˜åŠŸèƒ½ï¼ˆç±»ä¼¼åŸç‰ˆç»“æ„ï¼‰")
        print("\nğŸ“ Debug Structure:")
        print("  â€¢ inputs/: åŸå§‹å›¾åƒã€é¢„å¤„ç†å›¾åƒã€prompt")
        print("  â€¢ outputs/: æœ€ç»ˆè§†é¢‘ï¼ˆMP4 + GIFï¼‰")
        print("  â€¢ process/: ä¼˜åŒ–è¿‡ç¨‹è§†é¢‘ã€lossåˆ†æ")
        print("  â€¢ debug/: ä¸­é—´æ­¥éª¤ï¼ˆå¸§ã€è§†é¢‘ï¼‰")
        print("  â€¢ params/: é…ç½®å‚æ•°ï¼ˆTXT + JSONï¼‰")
        print("\nğŸ“– Usage:")
        print("  python guidance_pipeline.py test  # æµ‹è¯•å®Œæ•´debugåŠŸèƒ½")
        print("\nğŸ“Š SDS Loss Components:")
        print("  1. _sample_timestep(): æ—¶é—´æ­¥é‡‡æ ·")
        print("  2. _add_noise(): DDIM å™ªå£°æ·»åŠ ")
        print("  3. model.apply_model(): å™ªå£°é¢„æµ‹")
        print("  4. _apply_guidance_rescale(): CFG + guidance rescale")
        print("  5. DDIM reverse: è®¡ç®—é¢„æµ‹çš„åŸå§‹æ ·æœ¬")
        print("  6. SDS æ¢¯åº¦è®¡ç®—: w(t) * (latents - pred_original)")
        print("  7. å·§å¦™çš„æŸå¤±æ„å»º: target = latents - grad")
