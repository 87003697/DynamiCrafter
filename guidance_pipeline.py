# dynamicrafter_guidance_pipeline.py

"""
DynamiCrafter Guidance Pipeline

æ­£ç¡®çš„æ”¹é€ æ€è·¯ï¼š
1. ä¿æŒ DynamiCrafter çš„æ‰€æœ‰æ ¸å¿ƒé€»è¾‘ (æ¥è‡ª dynamicrafter_pipeline.py)
2. åªæ›¿æ¢ scheduler.sample() è°ƒç”¨ä¸º optimization loop (å‚è€ƒ guidance_flux_pipeline.py)
3. åœ¨ optimization loop ä¸­ä½¿ç”¨ DynamiCrafter çš„ model.apply_model() æ–¹æ³•

è½¬æ¢å…³ç³»ï¼š
- pipeline_normal_to_flux.py (inference) â†’ guidance_flux_pipeline.py (guidance)
- dynamicrafter_pipeline.py (inference) â†’ guidance_pipeline.py (guidance)
"""

import os
import time
import sys
import json
import shutil
from datetime import datetime
from typing import Optional, Union, List, Any, Dict, Callable
from dataclasses import dataclass
import numpy as np
import torch
import torch.nn.functional as F
from einops import repeat, rearrange
import torchvision.transforms as transforms
from pytorch_lightning import seed_everything
from huggingface_hub import hf_hub_download
from PIL import Image
from omegaconf import OmegaConf
import imageio

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from utils.utils import instantiate_from_config
from scripts.evaluation.funcs import load_model_checkpoint, save_videos, get_latent_z
from lvdm.models.utils_diffusion import make_ddim_sampling_parameters


@dataclass
class GuidanceConfig:
    """Guidance optimization configuration"""
    num_optimization_steps: int = 1000
    learning_rate: float = 0.05
    loss_type: str = "sds"  # sds, csd, rfds
    weight_type: str = "auto"  # auto, t, ada, uniform
    cfg_scale: float = 7.5
    optimizer_type: str = "AdamW"
    
    # Dynamic step ratio parameters
    min_step_ratio_start: float = 0.02
    min_step_ratio_end: float = 0.02
    max_step_ratio_start: float = 0.98
    max_step_ratio_end: float = 0.98
    
    # Debug parameters
    save_debug_videos: bool = False
    debug_save_interval: int = 100
    debug_save_path: str = "debug_videos"


def create_output_structure(base_dir: str, prompt: str, loss_type: str, weight_type: str, 
                           learning_rate: float, num_optimization_steps: int, cfg_scale: float) -> Dict[str, str]:
    """Create organized output directory structure with descriptive naming."""
    
    # Create timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Create safe prompt name (remove special characters)
    safe_prompt = "".join(c for c in prompt if c.isalnum() or c in (' ', '-', '_')).rstrip()
    safe_prompt = safe_prompt.replace(' ', '_')[:50]  # Truncate long prompts
    
    # Determine effective weight type
    if weight_type == "auto":
        if loss_type == "sds":
            effective_weight_type = "t"
        elif loss_type == "csd":
            effective_weight_type = "ada"
        else:  # rfds
            effective_weight_type = "uniform"
    else:
        effective_weight_type = weight_type
    
    # Create main output directory
    output_name = f"{timestamp}_{safe_prompt}_{loss_type}_{effective_weight_type}_lr{learning_rate}_steps{num_optimization_steps}_cfg{cfg_scale}"
    main_dir = os.path.join(base_dir, output_name)
    
    # Create subdirectories
    dirs = {
        'main': main_dir,
        'inputs': os.path.join(main_dir, 'inputs'),
        'outputs': os.path.join(main_dir, 'outputs'),
        'debug': os.path.join(main_dir, 'debug'),
        'process': os.path.join(main_dir, 'process'),
        'params': os.path.join(main_dir, 'params')
    }
    
    # Create all directories
    for dir_path in dirs.values():
        os.makedirs(dir_path, exist_ok=True)
    
    return dirs


def save_parameters(params_dir: str, **kwargs):
    """Save all parameters to structured files."""
    
    # Save main parameters as JSON
    params_file = os.path.join(params_dir, 'parameters.json')
    with open(params_file, 'w') as f:
        json.dump(kwargs, f, indent=2, default=str)
    
    # Save readable text format
    txt_file = os.path.join(params_dir, 'parameters.txt')
    with open(txt_file, 'w') as f:
        f.write(f"DynamiCrafter Guidance Pipeline Parameters\n")
        f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("=" * 60 + "\n\n")
        
        for key, value in kwargs.items():
            f.write(f"{key}: {value}\n")
    
    print(f"Parameters saved to: {params_file}")


def save_input_conditions(inputs_dir: str, original_image: Image.Image, prompt: str, 
                         processed_image: torch.Tensor):
    """Save input conditions: original image, processed image, and prompt."""
    
    # Save original image
    original_image.save(os.path.join(inputs_dir, 'original_image.png'))
    
    # Save processed image
    if processed_image is not None:
        # Convert tensor to PIL Image
        if len(processed_image.shape) == 4:
            processed_image = processed_image[0]  # Remove batch dimension
        
        # Ensure proper format for saving
        if processed_image.shape[0] == 3:  # CHW format
            processed_image = processed_image.permute(1, 2, 0)  # HWC format
        
        processed_image = processed_image.cpu().numpy()
        
        # Denormalize from [-1, 1] to [0, 1]
        processed_image = (processed_image + 1.0) / 2.0
        processed_image = np.clip(processed_image, 0, 1)
        
        # Convert to uint8
        processed_image = (processed_image * 255).astype(np.uint8)
        
        processed_pil = Image.fromarray(processed_image)
        processed_pil.save(os.path.join(inputs_dir, 'processed_image.png'))
    
    # Save prompt
    with open(os.path.join(inputs_dir, 'prompt.txt'), 'w') as f:
        f.write(prompt)
    
    print(f"Input conditions saved to: {inputs_dir}")


def save_debug_frame(debug_dir: str, step: int, latents: torch.Tensor, 
                    model, device: str, frame_idx: int = 0):
    """Save debug frame from video latents."""
    
    try:
        with torch.no_grad():
            # Move to correct device
            if latents.device != torch.device(device):
                debug_latents = latents.to(device)
            else:
                debug_latents = latents.clone()
            
            # Decode latents to video
            decoded_video = model.decode_first_stage(debug_latents)
            
            # Extract specific frame
            if len(decoded_video.shape) == 5:  # (batch, channels, time, height, width)
                frame = decoded_video[0, :, frame_idx, :, :]  # (channels, height, width)
            else:
                frame = decoded_video[0]  # Use first frame if not 5D
            
            # Convert to numpy
            frame = frame.detach().cpu().numpy()
            
            # Convert CHW to HWC
            if frame.shape[0] == 3:
                frame = np.transpose(frame, (1, 2, 0))
            
            # Denormalize and convert to uint8
            frame = (frame + 1.0) / 2.0
            frame = np.clip(frame, 0, 1)
            frame = (frame * 255).astype(np.uint8)
            
            # Save frame
            frame_path = os.path.join(debug_dir, f'step_{step:06d}_frame_{frame_idx:02d}.png')
            Image.fromarray(frame).save(frame_path)
            
            # Clear memory
            del debug_latents, decoded_video, frame
            torch.cuda.empty_cache()
            
    except Exception as e:
        print(f"Warning: Could not save debug frame for step {step}: {e}")


def save_debug_video(debug_dir: str, step: int, latents: torch.Tensor, 
                    model, device: str):
    """Save debug video from latents."""
    
    try:
        with torch.no_grad():
            # Move to correct device
            if latents.device != torch.device(device):
                debug_latents = latents.to(device)
            else:
                debug_latents = latents.clone()
            
            # Decode latents to video
            decoded_video = model.decode_first_stage(debug_latents)
            
            # Convert to numpy
            if isinstance(decoded_video, torch.Tensor):
                video_np = decoded_video.detach().cpu().numpy()
            else:
                video_np = decoded_video
            
            # Process video format (batch, channels, time, height, width)
            if len(video_np.shape) == 5:
                # Take first batch
                video_np = video_np[0]  # (channels, time, height, width)
                
                # Convert to (time, height, width, channels)
                video_np = np.transpose(video_np, (1, 2, 3, 0))
                
                # Denormalize and convert to uint8
                video_np = (video_np + 1.0) / 2.0
                video_np = np.clip(video_np, 0, 1)
                video_np = (video_np * 255).astype(np.uint8)
                
                # Save video
                video_path = os.path.join(debug_dir, f'step_{step:06d}_video.mp4')
                imageio.mimsave(video_path, video_np, fps=8)
                
                # Also save first frame as PNG
                first_frame_path = os.path.join(debug_dir, f'step_{step:06d}_frame.png')
                Image.fromarray(video_np[0]).save(first_frame_path)
            
            # Clear memory
            del debug_latents, decoded_video, video_np
            torch.cuda.empty_cache()
            
    except Exception as e:
        print(f"Warning: Could not save debug video for step {step}: {e}")


def create_optimization_process_video(process_dir: str, debug_dir: str, 
                                    total_steps: int, debug_save_interval: int, fps: int = 2):
    """Create a video showing the optimization process."""
    
    try:
        # Find all debug frame images
        debug_frames = []
        for step in range(0, total_steps, debug_save_interval):
            frame_path = os.path.join(debug_dir, f'step_{step:06d}_frame.png')
            if os.path.exists(frame_path):
                debug_frames.append(frame_path)
        
        if not debug_frames:
            print("No debug frames found for process video creation")
            return
        
        # Create process video by combining frames
        process_frames = []
        
        for frame_path in debug_frames:
            try:
                frame = np.array(Image.open(frame_path))
                process_frames.append(frame)
            except Exception as e:
                print(f"Warning: Could not read frame from {frame_path}: {e}")
                continue
        
        if process_frames:
            # Save optimization process video
            process_video_path = os.path.join(process_dir, 'optimization_process.mp4')
            imageio.mimsave(process_video_path, process_frames, fps=fps)
            print(f"Optimization process video saved to: {process_video_path}")
        
    except Exception as e:
        print(f"Warning: Could not create optimization process video: {e}")


def get_fixed_ddim_sampler(model, ddim_steps, ddim_eta, verbose=False):
    """
    è¿”å›ä¿®å¤åsigmaå€¼çš„DDIMSampler - æ¥è‡ª dynamicrafter_pipeline.py
    """
    from lvdm.models.samplers.ddim import DDIMSampler
    
    sampler = DDIMSampler(model)
    sampler.make_schedule(ddim_steps, ddim_discretize="uniform", ddim_eta=ddim_eta, verbose=verbose)
    
    # å…³é”®ä¿®å¤ï¼šç”¨DynamiCrafteråŸå§‹å‡½æ•°é‡æ–°è®¡ç®—sigmaå€¼
    from lvdm.models.utils_diffusion import make_ddim_sampling_parameters
    ddim_sigmas, ddim_alphas, ddim_alphas_prev = make_ddim_sampling_parameters(
        alphacums=model.alphas_cumprod.cpu(),
        ddim_timesteps=sampler.ddim_timesteps,
        eta=ddim_eta,
        verbose=False
    )
    
    # æ›¿æ¢æœ‰é—®é¢˜çš„sigmaå€¼ - ç»Ÿä¸€å¤„ç†ä¸ºtorch tensor
    if isinstance(ddim_sigmas, torch.Tensor):
        sampler.ddim_sigmas = ddim_sigmas.to(model.device)
    else:
        sampler.ddim_sigmas = torch.from_numpy(ddim_sigmas).to(model.device)
    
    if isinstance(ddim_alphas, torch.Tensor):
        sampler.ddim_alphas = ddim_alphas.to(model.device)
    else:
        sampler.ddim_alphas = torch.from_numpy(ddim_alphas).to(model.device)
    
    if isinstance(ddim_alphas_prev, torch.Tensor):
        sampler.ddim_alphas_prev = ddim_alphas_prev.to(model.device)
    else:
        sampler.ddim_alphas_prev = torch.from_numpy(ddim_alphas_prev).to(model.device)
    
    # è®¡ç®— ddim_sqrt_one_minus_alphas
    if isinstance(ddim_alphas, torch.Tensor):
        sampler.ddim_sqrt_one_minus_alphas = torch.sqrt(1. - ddim_alphas).to(model.device)
    else:
        sampler.ddim_sqrt_one_minus_alphas = torch.from_numpy(np.sqrt(1. - ddim_alphas)).to(model.device)
    
    if verbose:
        print("âœ… DDIMSampler fixed: sigma values replaced with numerically stable versions")
    
    return sampler


class DynamiCrafterGuidancePipeline:
    """
    DynamiCrafter Guidance Pipeline - ä¸¥æ ¼æŒ‰ç…§è½¬æ¢é€»è¾‘æ”¹é€ 
    
    åŸºç¡€ç»“æ„å®Œå…¨æ¥è‡ª dynamicrafter_pipeline.py:DynamiCrafterImg2VideoPipeline
    åªæ›¿æ¢æ¨ç†éƒ¨åˆ†ä¸º guidance optimization
    """
    
    def __init__(
        self, 
        resolution: str = '256_256',
        device: Optional[str] = None,
        dtype: Optional[torch.dtype] = None,
        **kwargs
    ):
        """
        å®Œå…¨æ¥è‡ª dynamicrafter_pipeline.py:DynamiCrafterImg2VideoPipeline.__init__
        """
        self.resolution = tuple(map(int, resolution.split('_')))  # (height, width)
        
        # Enhanced device selection - support specific CUDA devices
        if device is None:
            # Check if CUDA_VISIBLE_DEVICES is set
            cuda_visible = os.environ.get('CUDA_VISIBLE_DEVICES', None)
            if cuda_visible is not None:
                # Use first visible device
                device_num = cuda_visible.split(',')[0]
                self.device = f"cuda:{device_num}" if torch.cuda.is_available() else "cpu"
            else:
                # Default to cuda:3 if available, otherwise first available CUDA device
                if torch.cuda.is_available():
                    if torch.cuda.device_count() > 3:
                        self.device = "cuda:3"
                    else:
                        self.device = "cuda:0"
                else:
                    self.device = "cpu"
        else:
            self.device = device
        
        self.dtype = dtype or torch.float32
        
        print(f"ğŸ”§ Using device: {self.device}")
        
        # æ–°å¢ï¼šguidanceé…ç½®
        self.guidance_config = GuidanceConfig()
        
        # Initialize components
        self._load_components()
        self._setup_image_processor()
        
        # Pipeline state
        self._is_initialized = True
        print(f"âœ… DynamiCrafter Guidance Pipeline initialized: {self.resolution[0]}x{self.resolution[1]}")
        
    def _load_components(self):
        """å®Œå…¨æ¥è‡ª dynamicrafter_pipeline.py:DynamiCrafterImg2VideoPipeline._load_components"""
        # Download model if needed
        self._download_model()
        
        # Load model
        project_root = os.path.dirname(__file__)
        ckpt_path = os.path.join(project_root, f'checkpoints/dynamicrafter_{self.resolution[1]}_v1/model.ckpt')
        config_file = os.path.join(project_root, f'configs/inference_{self.resolution[1]}_v1.0.yaml')
        
        config = OmegaConf.load(config_file)
        model_config = config.pop("model", OmegaConf.create())
        model_config['params']['unet_config']['params']['use_checkpoint'] = False
        
        self.model = instantiate_from_config(model_config)
        assert os.path.exists(ckpt_path), f"Error: checkpoint Not Found at {ckpt_path}!"
        self.model = load_model_checkpoint(self.model, ckpt_path)
        self.model.eval()
        
        # Move to device
        self.model = self.model.to(self.device)
        self._ensure_device_consistency()
        
        print(f"ğŸ”„ DynamiCrafter model loaded on {self.device}")
        
    def _ensure_device_consistency(self):
        """å®Œå…¨æ¥è‡ª dynamicrafter_pipeline.py"""
        def move_to_device(module):
            for name, param in module.named_parameters():
                if param.device != torch.device(self.device):
                    param.data = param.data.to(self.device)
            for name, buffer in module.named_buffers():
                if buffer.device != torch.device(self.device):
                    buffer.data = buffer.data.to(self.device)
            for name, child in module.named_children():
                move_to_device(child)
        
        move_to_device(self.model)
        
        # ç‰¹åˆ«å¤„ç†å„ä¸ªç¼–ç å™¨
        for component_name in ['cond_stage_model', 'first_stage_model', 'embedder', 'image_proj_model']:
            if hasattr(self.model, component_name):
                component = getattr(self.model, component_name)
                component = component.to(self.device)
                move_to_device(component)
                
    def _setup_image_processor(self):
        """å®Œå…¨æ¥è‡ª dynamicrafter_pipeline.py:DynamiCrafterImg2VideoPipeline._setup_image_processor"""
        self.image_processor = transforms.Compose([
            transforms.Resize(min(self.resolution)),
            transforms.CenterCrop(self.resolution),
        ])
        
    def _download_model(self):
        """å®Œå…¨æ¥è‡ª dynamicrafter_pipeline.py:DynamiCrafterImg2VideoPipeline._download_model"""
        REPO_ID = f'Doubiiu/DynamiCrafter_{self.resolution[1]}' if self.resolution[1] != 256 else 'Doubiiu/DynamiCrafter'
        filename_list = ['model.ckpt']
        
        project_root = os.path.dirname(__file__)
        model_dir = os.path.join(project_root, f'checkpoints/dynamicrafter_{self.resolution[1]}_v1/')
        
        if not os.path.exists(model_dir):
            os.makedirs(model_dir)
            
        for filename in filename_list:
            local_file = os.path.join(model_dir, filename)
            if not os.path.exists(local_file):
                print(f"ğŸ“¥ Downloading model: {filename}")
                hf_hub_download(
                    repo_id=REPO_ID, 
                    filename=filename, 
                    local_dir=model_dir, 
                    local_dir_use_symlinks=False
                )
                print(f"âœ… Download completed: {filename}")

    def _preprocess_image(self, image, height=None, width=None):
        """å®Œå…¨æ¥è‡ª dynamicrafter_pipeline.py:DynamiCrafterImg2VideoPipeline._preprocess_image"""
        if isinstance(image, Image.Image):
            image = np.array(image)
        
        if isinstance(image, np.ndarray):
            image = torch.from_numpy(image).permute(2, 0, 1).float()
        
        # æ ‡å‡†åŒ–åˆ° [-1, 1]
        if image.max() > 1.0:
            image = image / 255.0
        image = (image - 0.5) * 2
        
        # è°ƒæ•´å¤§å°
        if height is not None and width is not None:
            transform = transforms.Compose([
                transforms.Resize((height, width)),
            ])
        else:
            transform = self.image_processor
        
        return transform(image)

    def _encode_prompt(self, prompt, negative_prompt, device):
        """å®Œå…¨æ¥è‡ª dynamicrafter_pipeline.py:DynamiCrafterImg2VideoPipeline._encode_prompt"""
        # ç¡®ä¿æ–‡æœ¬ç¼–ç å™¨åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        if hasattr(self.model, 'cond_stage_model'):
            self.model.cond_stage_model = self.model.cond_stage_model.to(device)
            # å¼ºåˆ¶è®¾ç½®è®¾å¤‡å±æ€§
            if hasattr(self.model.cond_stage_model, 'device'):
                self.model.cond_stage_model.device = device
            # é€’å½’ç¡®ä¿æ‰€æœ‰å­æ¨¡å—åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
            for name, module in self.model.cond_stage_model.named_modules():
                if hasattr(module, 'device'):
                    module.device = device
                for param in module.parameters():
                    if param.device != torch.device(device):
                        param.data = param.data.to(device)
        
        # æ­£å‘æç¤º
        text_embeddings = self.model.get_learned_conditioning(prompt)
        
        # è´Ÿå‘æç¤ºï¼ˆå¦‚æœæä¾›ï¼‰
        if negative_prompt is not None:
            uncond_embeddings = self.model.get_learned_conditioning(negative_prompt)
        else:
            if self.model.uncond_type == "empty_seq":
                uncond_embeddings = self.model.get_learned_conditioning([""] * len(prompt))
            elif self.model.uncond_type == "zero_embed":
                uncond_embeddings = torch.zeros_like(text_embeddings)
        
        return {"cond": text_embeddings, "uncond": uncond_embeddings}
    
    def _encode_image(self, image, num_frames):
        """å®Œå…¨æ¥è‡ª dynamicrafter_pipeline.py:DynamiCrafterImg2VideoPipeline._encode_image"""
        # æ·»åŠ æ‰¹æ¬¡ç»´åº¦
        if image.dim() == 3:
            image = image.unsqueeze(0)
        
        image = image.to(self.device)
        
        # å›¾åƒåµŒå…¥
        cond_images = self.model.embedder(image)
        img_emb = self.model.image_proj_model(cond_images)
        
        # æ½œåœ¨è¡¨ç¤º
        videos = image.unsqueeze(2)  # æ·»åŠ æ—¶é—´ç»´åº¦
        z = get_latent_z(self.model, videos)
        
        return img_emb, z
    
    def _prepare_conditioning(self, text_embeddings, image_embeddings, image_latents, 
                             frame_stride, guidance_scale, batch_size):
        """å®Œå…¨æ¥è‡ª dynamicrafter_pipeline.py:DynamiCrafterImg2VideoPipeline._prepare_conditioning"""
        # ç»„åˆæ–‡æœ¬å’Œå›¾åƒåµŒå…¥
        cond_emb = torch.cat([text_embeddings["cond"], image_embeddings], dim=1)
        cond = {"c_crossattn": [cond_emb]}
        
        # å›¾åƒæ¡ä»¶ï¼ˆhybridæ¨¡å¼ï¼‰
        if self.model.model.conditioning_key == 'hybrid':
            img_cat_cond = image_latents[:,:,:1,:,:]
            img_cat_cond = repeat(img_cat_cond, 'b c t h w -> b c (repeat t) h w', 
                                 repeat=self.model.temporal_length)
            cond["c_concat"] = [img_cat_cond]
        
        # æ— æ¡ä»¶è¾“å…¥
        uc = None
        if guidance_scale != 1.0:
            # åˆ›å»ºæ­£ç¡®å½¢çŠ¶çš„é›¶å›¾åƒ
            zero_image = torch.zeros((batch_size, 3, self.resolution[0], self.resolution[1]), 
                                   device=self.model.device, dtype=self.model.dtype)
            uc_img_emb = self.model.embedder(zero_image)
            uc_img_emb = self.model.image_proj_model(uc_img_emb)
            
            uc_emb = torch.cat([text_embeddings["uncond"], uc_img_emb], dim=1)
            uc = {"c_crossattn": [uc_emb]}
            
            if self.model.model.conditioning_key == 'hybrid':
                uc["c_concat"] = [img_cat_cond]
        
        # å¸§æ­¥é•¿
        fs = torch.tensor([frame_stride] * batch_size, dtype=torch.long, device=self.model.device)
        
        return {"cond": cond, "uc": uc, "fs": fs}

    def _decode_latents(self, latents):
        """å®Œå…¨æ¥è‡ª dynamicrafter_pipeline.py:DynamiCrafterImg2VideoPipeline._decode_latents"""
        with torch.no_grad():
            videos = self.model.decode_first_stage(latents)
        return videos
    
    def _postprocess_video(self, videos, output_type):
        """å®Œå…¨æ¥è‡ª dynamicrafter_pipeline.py:DynamiCrafterImg2VideoPipeline._postprocess_video"""
        if output_type == "numpy":
            videos = videos.cpu().float().numpy()
        elif output_type == "pil":
            # è½¬æ¢ä¸ºPILå›¾åƒåˆ—è¡¨ï¼ˆç®€åŒ–å®ç°ï¼‰
            videos = videos.cpu().float().numpy()
            print("ğŸ“ Note: PIL output conversion not fully implemented, returning numpy")
        # tensoræ ¼å¼ç›´æ¥è¿”å›
        
        return videos

    def __call__(
        self,
        image: Union[Image.Image, np.ndarray, torch.Tensor],
        prompt: Union[str, List[str]] = "",
        negative_prompt: Optional[Union[str, List[str]]] = None,
        # === ä¿æŒ DynamiCrafter çš„æ ‡å‡†å‚æ•° ===
        num_inference_steps: int = 50,
        guidance_scale: float = 7.5,
        eta: float = 0.0,
        frame_stride: int = 3,
        num_frames: Optional[int] = None,
        height: Optional[int] = None,
        width: Optional[int] = None,
        num_videos_per_prompt: int = 1,
        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,
        latents: Optional[torch.FloatTensor] = None,
        output_type: str = "tensor",
        return_dict: bool = True,
        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,
        callback_steps: int = 1,
        cross_attention_kwargs: Optional[Dict[str, Any]] = None,
        # === æ–°å¢ï¼šGuidance å‚æ•° ===
        num_optimization_steps: int = 1000,
        learning_rate: float = 0.05,
        loss_type: str = "sds",
        weight_type: str = "auto",
        cfg_scale: Optional[float] = None,
        optimizer_type: str = "AdamW",
        min_step_ratio_start: Optional[float] = None,
        min_step_ratio_end: Optional[float] = None,
        max_step_ratio_start: Optional[float] = None,
        max_step_ratio_end: Optional[float] = None,
        # === æ–°å¢ï¼šEnhanced saving parameters ===
        save_results: bool = False,
        results_dir: str = "results_dynamicrafter_guidance",
        save_debug_images: bool = False,
        save_debug_videos: bool = False,
        save_process_video: bool = False,
        debug_save_interval: int = 100,
        debug_save_path: str = "debug_videos",
        **kwargs
    ) -> Union[torch.Tensor, Dict[str, Any]]:
        """
        Generate video using DynamiCrafter with guidance optimization.
        
        è¿™ä¸ªå‡½æ•°å¤§éƒ¨åˆ†æ¥è‡ª dynamicrafter_pipeline.py:DynamiCrafterImg2VideoPipeline.__call__
        åªæ›¿æ¢ scheduler.sample() éƒ¨åˆ†ä¸º optimization loop
        """
        
        # === å®Œå…¨æ¥è‡ª dynamicrafter_pipeline.py ===
        # è¾“å…¥éªŒè¯å’Œæ ‡å‡†åŒ–
        if isinstance(prompt, str):
            batch_size = 1
            prompt = [prompt]
        else:
            batch_size = len(prompt)
        
        # å¤„ç† negative_prompt
        if negative_prompt is not None:
            if isinstance(negative_prompt, str):
                negative_prompt = [negative_prompt] * batch_size
            elif len(negative_prompt) != batch_size:
                raise ValueError(f"negative_prompt length ({len(negative_prompt)}) != batch_size ({batch_size})")
        
        # è®¾å¤‡ç®¡ç†
        device = self.device
        
        # å¤„ç† cfg_scale å‚æ•°
        if cfg_scale is None:
            cfg_scale = guidance_scale
        
        # è®¾ç½®é»˜è®¤ guidance å‚æ•°
        if min_step_ratio_start is None:
            min_step_ratio_start = self.guidance_config.min_step_ratio_start
        if min_step_ratio_end is None:
            min_step_ratio_end = self.guidance_config.min_step_ratio_end
        if max_step_ratio_start is None:
            max_step_ratio_start = self.guidance_config.max_step_ratio_start
        if max_step_ratio_end is None:
            max_step_ratio_end = self.guidance_config.max_step_ratio_end
        
        # === æ–°å¢ï¼šEnhanced saving setup ===
        output_dirs = None
        if save_results:
            output_dirs = create_output_structure(
                results_dir, prompt[0], loss_type, weight_type, learning_rate, 
                num_optimization_steps, cfg_scale
            )
            print(f"ğŸ”§ Results will be saved to: {output_dirs['main']}")
        
        print(f"ğŸ¬ å¼€å§‹ DynamiCrafter Guidance ä¼˜åŒ–...")
        print(f"ğŸ“ æç¤ºè¯: {prompt}")
        print(f"ğŸ”§ Guidance å‚æ•°: steps={num_optimization_steps}, lr={learning_rate}, loss={loss_type}")
        print(f"ğŸ”§ DynamiCrafter å‚æ•°: guidance={guidance_scale}, eta={eta}, frame_stride={frame_stride}")
        print(f"ğŸ’» è®¾å¤‡: {device}")
        
        # å›¾åƒé¢„å¤„ç†
        original_image = image if isinstance(image, Image.Image) else Image.fromarray(np.array(image))
        processed_image = self._preprocess_image(image, height, width)
        
        # === æ–°å¢ï¼šSave input conditions ===
        if save_results:
            save_input_conditions(
                output_dirs['inputs'], original_image, prompt[0], processed_image
            )
        
        # æ¨¡å‹å‚æ•°
        num_frames = num_frames or self.model.temporal_length
        channels = self.model.model.diffusion_model.out_channels
        if height is None or width is None:
            height, width = self.resolution
        latent_height, latent_width = height // 8, width // 8
        
        # å™ªå£°å½¢çŠ¶
        noise_shape = (batch_size, channels, num_frames, latent_height, latent_width)
        
        with torch.no_grad():
            # ç¼–ç æç¤ºè¯å’Œå›¾åƒ
            text_embeddings = self._encode_prompt(prompt, negative_prompt, device)
            image_embeddings, image_latents = self._encode_image(processed_image, num_frames)
            
            # å‡†å¤‡æ¡ä»¶è¾“å…¥
            conditioning = self._prepare_conditioning(
                text_embeddings=text_embeddings,
                image_embeddings=image_embeddings,
                image_latents=image_latents,
                frame_stride=frame_stride,
                guidance_scale=guidance_scale,
                batch_size=batch_size
            )
        
        # === æ–°å¢ï¼šSave parameters ===
        if save_results:
            save_parameters(
                output_dirs['params'],
                prompt=prompt[0],
                negative_prompt=negative_prompt[0] if negative_prompt else None,
                height=height,
                width=width,
                num_frames=num_frames,
                num_optimization_steps=num_optimization_steps,
                learning_rate=learning_rate,
                loss_type=loss_type,
                weight_type=weight_type,
                cfg_scale=cfg_scale,
                optimizer_type=optimizer_type,
                frame_stride=frame_stride,
                min_step_ratio_start=min_step_ratio_start,
                min_step_ratio_end=min_step_ratio_end,
                max_step_ratio_start=max_step_ratio_start,
                max_step_ratio_end=max_step_ratio_end,
                save_debug_images=save_debug_images,
                save_debug_videos=save_debug_videos,
                save_process_video=save_process_video,
                debug_save_interval=debug_save_interval,
                device=str(device),
                batch_size=batch_size,
                channels=channels,
                latent_height=latent_height,
                latent_width=latent_width,
                noise_shape=noise_shape,
                **kwargs
            )
        
        # ===== å…³é”®æ›¿æ¢ç‚¹ï¼šç”¨ optimization loop æ›¿æ¢ scheduler.sample() =====
        # åŸä»£ç ï¼š
        # scheduler = get_fixed_ddim_sampler(self.model, num_inference_steps, eta, verbose=False)
        # samples, _ = scheduler.sample(...)
        
        # æ–°ä»£ç ï¼šoptimization loop
        samples = self._optimization_loop(
            noise_shape=noise_shape,
            conditioning=conditioning,
            device=device,
            num_optimization_steps=num_optimization_steps,
            learning_rate=learning_rate,
            loss_type=loss_type,
            weight_type=weight_type,
            cfg_scale=cfg_scale,
            optimizer_type=optimizer_type,
            min_step_ratio_start=min_step_ratio_start,
            min_step_ratio_end=min_step_ratio_end,
            max_step_ratio_start=max_step_ratio_start,
            max_step_ratio_end=max_step_ratio_end,
            # === Enhanced saving parameters ===
            output_dirs=output_dirs,
            save_results=save_results,
            save_debug_images=save_debug_images,
            save_debug_videos=save_debug_videos,
            save_process_video=save_process_video,
            debug_save_interval=debug_save_interval,
            debug_save_path=debug_save_path,
            generator=generator,
            **kwargs
        )
        
        # === å®Œå…¨æ¥è‡ª dynamicrafter_pipeline.py ===
        # è§£ç  latents åˆ°è§†é¢‘
        print(f"ğŸï¸ è§£ç æ½œåœ¨è¡¨ç¤º...")
        videos = self._decode_latents(samples)
        
        # === æ–°å¢ï¼šSave final results ===
        if save_results:
            # Save final video to outputs directory
            for i in range(videos.shape[0]):
                # Keep tensor format for save_videos function
                video_data = videos[i:i+1].unsqueeze(1)  # Add samples dimension: b,samples,c,t,h,w
                
                # Use DynamiCrafter's save_videos function
                save_videos(video_data, output_dirs['outputs'], 
                          filenames=[f'final_video_{i:03d}'], fps=8)
                
                output_path = os.path.join(output_dirs['outputs'], f'final_video_{i:03d}.mp4')
                print(f"Final video saved to: {output_path}")
            
            # Create optimization process video
            if save_process_video:
                create_optimization_process_video(
                    output_dirs['process'], output_dirs['debug'], 
                    num_optimization_steps, debug_save_interval, fps=2
                )
            
            print(f"âœ… All results saved to: {output_dirs['main']}")
        
        # åå¤„ç†
        videos = self._postprocess_video(videos, output_type)
        
        print(f"âœ… è§†é¢‘ç”Ÿæˆå®Œæˆ! å½¢çŠ¶: {videos.shape}")
        
        # è¿”å›ç»“æœ
        if return_dict:
            return {"videos": videos}
        else:
            return videos

    def _optimization_loop(
        self,
        noise_shape,
        conditioning,
        device,
        num_optimization_steps,
        learning_rate,
        loss_type,
        weight_type,
        cfg_scale,
        optimizer_type,
        min_step_ratio_start,
        min_step_ratio_end,
        max_step_ratio_start,
        max_step_ratio_end,
        # === Enhanced saving parameters ===
        output_dirs,
        save_results,
        save_debug_images,
        save_debug_videos,
        save_process_video,
        debug_save_interval,
        debug_save_path,
        generator=None,
        **kwargs
    ):
        """
        Optimization loop æ›¿æ¢åŸæ¥çš„ scheduler.sample()
        
        å‚è€ƒ guidance_flux_pipeline.py çš„ä¼˜åŒ–å¾ªç¯ï¼Œä½†é€‚ç”¨äº DynamiCrafter
        """
        
        # Create debug directory if needed
        if save_debug_videos and not save_results:
            os.makedirs(debug_save_path, exist_ok=True)
            print(f"[INFO] Debug videos will be saved to: {debug_save_path}")
        
        # åˆå§‹åŒ– latents
        if generator is not None:
            latents = torch.randn(noise_shape, generator=generator, device=device, dtype=torch.float32)
        else:
            latents = torch.randn(noise_shape, device=device, dtype=torch.float32)
        
        latents = latents.detach().clone()
        latents.requires_grad_(True)
        
        # è®¾ç½®ä¼˜åŒ–å™¨
        if optimizer_type == "AdamW":
            optimizer = torch.optim.AdamW([latents], lr=learning_rate, betas=(0.9, 0.99), eps=1e-8)
        elif optimizer_type == "Adam":
            optimizer = torch.optim.Adam([latents], lr=learning_rate, betas=(0.9, 0.999), eps=1e-8)
        else:
            raise ValueError(f"Unknown optimizer_type: {optimizer_type}")
        
        # ç¡®å®šæœ‰æ•ˆçš„æƒé‡ç±»å‹
        if weight_type == "auto":
            if loss_type == "sds":
                effective_weight_type = "t"
            elif loss_type == "csd":
                effective_weight_type = "ada"
            else:  # rfds
                effective_weight_type = "uniform"
        else:
            effective_weight_type = weight_type
        
        print(f"[INFO] Starting DynamiCrafter guidance optimization with {num_optimization_steps} steps")
        print(f"[INFO] Loss type: {loss_type.upper()}")
        print(f"[INFO] Weight type: {effective_weight_type}")
        print(f"[INFO] Learning rate: {learning_rate}")
        print(f"[INFO] CFG scale: {cfg_scale}")
        if save_results:
            print(f"[INFO] Enhanced saving enabled - interval: {debug_save_interval} steps")
        print("-" * 60)
        
        # ä¼˜åŒ–å¾ªç¯
        for i in range(num_optimization_steps):
            optimizer.zero_grad()
            
            # è®¡ç®—å½“å‰æ­¥éª¤æ¯”ä¾‹
            progress = i / (num_optimization_steps - 1) if num_optimization_steps > 1 else 0.0
            current_min_step_ratio = (
                min_step_ratio_start + 
                (min_step_ratio_end - min_step_ratio_start) * progress
            )
            current_max_step_ratio = (
                max_step_ratio_start + 
                (max_step_ratio_end - max_step_ratio_start) * progress
            )
            
            # è®¡ç®— guidance loss
            if loss_type == "sds":
                loss = self._sds_loss(
                    latents,
                    conditioning,
                    cfg_scale=cfg_scale,
                    min_step_ratio=current_min_step_ratio,
                    max_step_ratio=current_max_step_ratio,
                    weight_type=effective_weight_type
                )
            elif loss_type == "csd":
                loss = self._csd_loss(
                    latents,
                    conditioning,
                    cfg_scale=cfg_scale,
                    min_step_ratio=current_min_step_ratio,
                    max_step_ratio=current_max_step_ratio,
                    weight_type=effective_weight_type
                )
            elif loss_type == "rfds":
                loss = self._rfds_loss(
                    latents,
                    conditioning,
                    cfg_scale=cfg_scale,
                    min_step_ratio=current_min_step_ratio,
                    max_step_ratio=current_max_step_ratio,
                    weight_type=effective_weight_type
                )
            else:
                raise ValueError(f"Unknown loss_type: {loss_type}")
            
            loss.backward()
            optimizer.step()
            
            # === Enhanced debug saving ===
            if i % debug_save_interval == 0:
                if save_results:
                    # Save debug images
                    if save_debug_images:
                        save_debug_frame(
                            output_dirs['debug'], i, latents, 
                            self.model, str(self.device), frame_idx=0
                        )
                    
                    # Save debug videos
                    if save_debug_videos:
                        save_debug_video(
                            output_dirs['debug'], i, latents,
                            self.model, str(self.device)
                        )
                    
                    print(f"[DEBUG] Enhanced debug results saved for step {i}")
                    
                elif save_debug_videos:  # Compatibility with old interface
                    self._save_debug_video(i, latents, debug_save_path)
            
            # è¿›åº¦æ—¥å¿—
            if i % max(1, num_optimization_steps // 10) == 0:
                print(f"[PROGRESS] Step {i}/{num_optimization_steps} - Loss: {loss.item():.6f}")
        
        print(f"[INFO] Optimization completed!")
        
        return latents.detach()

    def _sample_timestep(self, batch_size, min_step_ratio=0.02, max_step_ratio=0.98):
        """
        é‡‡æ ·æ—¶é—´æ­¥ - é€‚ç”¨äº DynamiCrafter çš„ DDIM è°ƒåº¦
        """
        # ä½¿ç”¨ DynamiCrafter çš„ DDIM æ—¶é—´æ­¥
        from lvdm.models.utils_diffusion import make_ddim_timesteps
        
        ddim_timesteps = make_ddim_timesteps(
            ddim_discr_method='uniform', 
            num_ddim_timesteps=50,  # ä½¿ç”¨50æ­¥ä½œä¸ºé»˜è®¤
            num_ddpm_timesteps=self.model.num_timesteps,
            verbose=False
        )
        
        # æ ¹æ®æ¯”ä¾‹é€‰æ‹©æ—¶é—´æ­¥èŒƒå›´
        min_idx = int(len(ddim_timesteps) * min_step_ratio)
        max_idx = int(len(ddim_timesteps) * max_step_ratio)
        max_idx = max(max_idx, min_idx + 1)
        
        # éšæœºé‡‡æ ·æ—¶é—´æ­¥ç´¢å¼•
        t_idx = torch.randint(min_idx, max_idx, (batch_size,), device="cpu")
        
        # è·å–å¯¹åº”çš„æ—¶é—´æ­¥å€¼
        t_values = ddim_timesteps[t_idx.cpu().numpy()]
        t = torch.from_numpy(t_values).long().to(self.device)
        
        return t

    def _add_noise(self, original_samples, noise, timesteps):
        """
        æ·»åŠ å™ªå£° - é€‚ç”¨äº DynamiCrafter çš„ DDIM è°ƒåº¦
        """
        # è·å– alpha_cumprod
        alphas_cumprod = self.model.alphas_cumprod.to(self.device)
        
        # ç¡®ä¿æ—¶é—´æ­¥åœ¨æ­£ç¡®èŒƒå›´å†…
        timesteps = timesteps.clamp(0, len(alphas_cumprod) - 1)
        
        # è·å–å¯¹åº”çš„ alpha_cumprod å€¼
        alpha_t = alphas_cumprod[timesteps]
        
        # ç¡®ä¿ç»´åº¦åŒ¹é…
        while len(alpha_t.shape) < len(original_samples.shape):
            alpha_t = alpha_t.unsqueeze(-1)
        
        # DDIM å™ªå£°æ·»åŠ 
        sqrt_alpha_t = torch.sqrt(alpha_t)
        sqrt_one_minus_alpha_t = torch.sqrt(1.0 - alpha_t)
        
        noisy_samples = sqrt_alpha_t * original_samples + sqrt_one_minus_alpha_t * noise
        return noisy_samples

    def _sds_loss(self, latents, conditioning, cfg_scale=7.5, min_step_ratio=0.02, max_step_ratio=0.98, weight_type="t"):
        """
        SDS loss ä½¿ç”¨ DynamiCrafter çš„ model.apply_model
        """
        batch_size = latents.shape[0]
        
        # é‡‡æ ·æ—¶é—´æ­¥
        t = self._sample_timestep(batch_size, min_step_ratio, max_step_ratio)
        
        # æ·»åŠ å™ªå£°
        noise = torch.randn_like(latents)
        noisy_latents = self._add_noise(latents, noise, t)
        
        # å‡†å¤‡æ¡ä»¶
        cond = conditioning["cond"]
        uc = conditioning["uc"]
        fs = conditioning["fs"]
        
        # å‰å‘ä¼ æ’­ - ä½¿ç”¨ DynamiCrafter çš„ model.apply_model
        with torch.no_grad():
            # æ¡ä»¶é¢„æµ‹
            noise_pred_cond = self.model.apply_model(noisy_latents, t, cond, **{"fs": fs})
            
            # æ— æ¡ä»¶é¢„æµ‹
            if uc is not None and cfg_scale > 1.0:
                noise_pred_uncond = self.model.apply_model(noisy_latents, t, uc, **{"fs": fs})
                # åº”ç”¨ CFG
                noise_pred = noise_pred_uncond + cfg_scale * (noise_pred_cond - noise_pred_uncond)
            else:
                noise_pred = noise_pred_cond
        
        # è®¡ç®—é¢„æµ‹çš„åŸå§‹æ ·æœ¬
        alpha_t = self.model.alphas_cumprod[t].to(self.device)
        while len(alpha_t.shape) < len(latents.shape):
            alpha_t = alpha_t.unsqueeze(-1)
        
        sqrt_alpha_t = torch.sqrt(alpha_t)
        sqrt_one_minus_alpha_t = torch.sqrt(1.0 - alpha_t)
        
        pred_original_sample = (noisy_latents - sqrt_one_minus_alpha_t * noise_pred) / sqrt_alpha_t
        
        # è®¡ç®— SDS æ¢¯åº¦
        if weight_type == "t":
            w = (1.0 - alpha_t).view(batch_size, 1, 1, 1, 1)
            grad = w * (latents - pred_original_sample.detach())
        elif weight_type == "ada":
            weighting_factor = torch.abs(latents - pred_original_sample.detach()).mean(dim=(1, 2, 3, 4), keepdim=True)
            weighting_factor = torch.clamp(weighting_factor, 1e-4)
            grad = (latents - pred_original_sample.detach()) / weighting_factor
        elif weight_type == "uniform":
            grad = (latents - pred_original_sample.detach())
        else:
            raise ValueError(f"Unknown weight_type: {weight_type}")
        
        grad = grad.detach()
        grad = torch.nan_to_num(grad)
        
        # æ„å»ºæŸå¤±
        target = (latents - grad).detach()
        loss = 0.5 * F.mse_loss(latents, target, reduction="mean") / batch_size
        
        torch.cuda.empty_cache()
        
        return loss

    def _csd_loss(self, latents, conditioning, cfg_scale=7.5, min_step_ratio=0.02, max_step_ratio=0.98, weight_type="ada"):
        """
        CSD loss ä½¿ç”¨ DynamiCrafter çš„ model.apply_model
        
        æ­£ç¡®çš„ CSD (Classifier-Free Score Distillation) å®ç°ï¼š
        - fake = æ— æ¡ä»¶é¢„æµ‹çš„åŸå§‹æ ·æœ¬
        - real = å¼•å¯¼é¢„æµ‹ (CFG) çš„åŸå§‹æ ·æœ¬
        - é€šè¿‡æ— æ¡ä»¶å’Œå¼•å¯¼é¢„æµ‹çš„å·®å¼‚æ¥ä¼˜åŒ–
        """
        batch_size = latents.shape[0]
        
        # é‡‡æ ·æ—¶é—´æ­¥
        t = self._sample_timestep(batch_size, min_step_ratio, max_step_ratio)
        
        # æ·»åŠ å™ªå£°
        noise = torch.randn_like(latents)
        noisy_latents = self._add_noise(latents, noise, t)
        
        # å‡†å¤‡æ¡ä»¶
        cond = conditioning["cond"]
        uc = conditioning["uc"]
        fs = conditioning["fs"]
        
        # å‰å‘ä¼ æ’­ - åˆ†åˆ«è®¡ç®—æ— æ¡ä»¶å’Œæ¡ä»¶é¢„æµ‹
        with torch.no_grad():
            # æ¡ä»¶é¢„æµ‹
            noise_pred_cond = self.model.apply_model(noisy_latents, t, cond, **{"fs": fs})
            
            # æ— æ¡ä»¶é¢„æµ‹
            noise_pred_uncond = self.model.apply_model(noisy_latents, t, uc, **{"fs": fs})
            
            # å¼•å¯¼é¢„æµ‹ (CFG)
            noise_pred_guided = noise_pred_uncond + cfg_scale * (noise_pred_cond - noise_pred_uncond)
        
        # è®¡ç®—é¢„æµ‹çš„åŸå§‹æ ·æœ¬
        alpha_t = self.model.alphas_cumprod[t].to(self.device)
        while len(alpha_t.shape) < len(latents.shape):
            alpha_t = alpha_t.unsqueeze(-1)
        
        sqrt_alpha_t = torch.sqrt(alpha_t)
        sqrt_one_minus_alpha_t = torch.sqrt(1.0 - alpha_t)
        
        # CSD çš„æ ¸å¿ƒï¼šå¯¹æ¯”æ— æ¡ä»¶å’Œå¼•å¯¼é¢„æµ‹
        pred_fake_latents = (noisy_latents - sqrt_one_minus_alpha_t * noise_pred_uncond) / sqrt_alpha_t  # æ— æ¡ä»¶é¢„æµ‹
        pred_real_latents = (noisy_latents - sqrt_one_minus_alpha_t * noise_pred_guided) / sqrt_alpha_t   # å¼•å¯¼é¢„æµ‹
        
        # è®¡ç®— CSD æ¢¯åº¦
        if weight_type == "ada":
            weighting_factor = torch.abs(latents - pred_real_latents.detach()).mean(dim=(1, 2, 3, 4), keepdim=True)
            weighting_factor = torch.clamp(weighting_factor, 1e-4)
            grad = (pred_fake_latents - pred_real_latents) / weighting_factor
        elif weight_type == "t":
            w = (1.0 - alpha_t).view(batch_size, 1, 1, 1, 1)
            grad = w * (pred_fake_latents - pred_real_latents)
        elif weight_type == "uniform":
            grad = (pred_fake_latents - pred_real_latents)
        else:
            raise ValueError(f"Unknown weight_type: {weight_type}")
        
        grad = grad.detach()
        grad = torch.nan_to_num(grad)
        
        # æ„å»ºæŸå¤±
        target = (latents - grad).detach()
        loss = 0.5 * F.mse_loss(latents, target, reduction="mean") / batch_size
        
        torch.cuda.empty_cache()
        
        return loss

    def _rfds_loss(self, latents, conditioning, cfg_scale=7.5, min_step_ratio=0.02, max_step_ratio=0.98, weight_type="uniform"):
        """
        RFDS loss ä½¿ç”¨ DynamiCrafter çš„ model.apply_model
        """
        batch_size = latents.shape[0]
        
        # é‡‡æ ·æ—¶é—´æ­¥
        t = self._sample_timestep(batch_size, min_step_ratio, max_step_ratio)
        
        # æ·»åŠ å™ªå£°
        noise = torch.randn_like(latents)
        noisy_latents = self._add_noise(latents, noise, t)
        
        # å‡†å¤‡æ¡ä»¶
        cond = conditioning["cond"]
        uc = conditioning["uc"]
        fs = conditioning["fs"]
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            # æ¡ä»¶é¢„æµ‹
            noise_pred_cond = self.model.apply_model(noisy_latents, t, cond, **{"fs": fs})
            
            # æ— æ¡ä»¶é¢„æµ‹
            if uc is not None:
                noise_pred_uncond = self.model.apply_model(noisy_latents, t, uc, **{"fs": fs})
                # åº”ç”¨ CFG
                noise_pred = noise_pred_uncond + cfg_scale * (noise_pred_cond - noise_pred_uncond)
            else:
                noise_pred = noise_pred_cond
        
        # è®¡ç®—é¢„æµ‹çš„åŸå§‹æ ·æœ¬
        alpha_t = self.model.alphas_cumprod[t].to(self.device)
        while len(alpha_t.shape) < len(latents.shape):
            alpha_t = alpha_t.unsqueeze(-1)
        
        sqrt_alpha_t = torch.sqrt(alpha_t)
        sqrt_one_minus_alpha_t = torch.sqrt(1.0 - alpha_t)
        
        pred_original_sample = (noisy_latents - sqrt_one_minus_alpha_t * noise_pred) / sqrt_alpha_t
        
        # RFDS: ç›´æ¥ä¼˜åŒ–åŒ¹é…é¢„æµ‹çš„åŸå§‹æ ·æœ¬
        target = pred_original_sample.detach()
        
        if weight_type == "t":
            w = (1.0 - alpha_t).view(batch_size, 1, 1, 1, 1)
            loss = 0.5 * w * F.mse_loss(latents, target, reduction="mean")
        elif weight_type == "ada":
            prediction_error = latents - target
            weighting_factor = torch.abs(prediction_error).mean(dim=(1, 2, 3, 4), keepdim=True)
            weighting_factor = torch.clamp(weighting_factor, 1e-4)
            loss = 0.5 * F.mse_loss(latents, target, reduction="none")
            loss = (loss / weighting_factor).mean()
        elif weight_type == "uniform":
            loss = 0.5 * F.mse_loss(latents, target, reduction="mean")
        else:
            raise ValueError(f"Unknown weight_type: {weight_type}")
        
        loss = loss / batch_size
        
        torch.cuda.empty_cache()
        
        return loss

    def _save_debug_video(self, step, latents, debug_save_path):
        """ä¿å­˜ debug è§†é¢‘"""
        try:
            with torch.no_grad():
                # è§£ç  latents
                debug_videos = self.model.decode_first_stage(latents.detach())
                debug_videos = debug_videos.cpu().float().numpy()
                
                # ä¿å­˜è§†é¢‘
                filename = f"step_{step:06d}"
                save_videos(debug_videos.unsqueeze(1), debug_save_path, filenames=[filename], fps=8)
                
                print(f"[DEBUG] Saved debug video: {debug_save_path}/{filename}.mp4")
                
        except Exception as e:
            print(f"[DEBUG] Failed to save debug video: {e}")

    def save_video(self, video: torch.Tensor, output_path: str, fps: int = 8, **kwargs):
        """
        å®Œå…¨æ¥è‡ª dynamicrafter_pipeline.py:DynamiCrafterImg2VideoPipeline.save_video
        """
        # Ensure output directory exists
        output_dir = os.path.dirname(output_path)
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
        
        # Get filename without extension
        filename = os.path.basename(output_path).split('.')[0]
        
        # Fix dimensions for save_videos function
        if video.dim() == 4:
            video = video.unsqueeze(0).unsqueeze(0)
        elif video.dim() == 5:
            video = video.unsqueeze(1)
        
        # Ensure video tensor is on CPU
        if video.is_cuda:
            video = video.cpu()
        
        print(f"ğŸ“Š ä¿å­˜è§†é¢‘å½¢çŠ¶: {video.shape}")
        save_videos(video, output_dir or '.', filenames=[filename], fps=fps)
        
        print(f"ğŸ’¾ è§†é¢‘å·²ä¿å­˜: {output_path}")


def test_dynamicrafter_guidance_pipeline():
    """
    æµ‹è¯• DynamiCrafter Guidance Pipeline
    """
    print("ğŸ§ª æµ‹è¯• DynamiCrafter Guidance Pipeline")
    print("=" * 70)
    
    try:
        # åˆå§‹åŒ– pipeline
        print("ğŸ”§ åˆå§‹åŒ– DynamiCrafter Guidance Pipeline...")
        pipeline = DynamiCrafterGuidancePipeline(
            resolution='256_256'
        )
        
        # æŸ¥æ‰¾æµ‹è¯•å›¾åƒ
        project_root = os.path.dirname(__file__)
        test_image_paths = [
            os.path.join(project_root, 'prompts/1024/pour_bear.png'),
            os.path.join(project_root, 'prompts/512_loop/24.png'),
        ]
        
        test_image_path = None
        for path in test_image_paths:
            if os.path.exists(path):
                test_image_path = path
                break
        
        if test_image_path is None:
            print("âŒ æµ‹è¯•å›¾åƒä¸å­˜åœ¨")
            return False
        
        # åŠ è½½æµ‹è¯•å›¾åƒ
        print(f"ğŸ“¸ åŠ è½½æµ‹è¯•å›¾åƒ: {test_image_path}")
        img = Image.open(test_image_path).convert("RGB")
        
        # æµ‹è¯• Enhanced saving åŠŸèƒ½
        test_prompt = "a person walking in a beautiful garden with flowers blooming"
        
        print(f"ğŸ“ æµ‹è¯•æç¤ºè¯: {test_prompt}")
        print(f"ğŸ¬ å¼€å§‹ Enhanced SDS ä¼˜åŒ–ç”Ÿæˆ...")
        
        start_time = time.time()
        
        # ä½¿ç”¨ Enhanced guidance ç”Ÿæˆ
        result = pipeline(
            image=img,
            prompt=test_prompt,
            num_optimization_steps=20,  # çŸ­æµ‹è¯•
            learning_rate=0.05,
            loss_type="sds",
            cfg_scale=7.5,
            # Enhanced saving parameters
            save_results=True,
            save_debug_images=True,
            save_debug_videos=True,
            save_process_video=True,
            debug_save_interval=5,
            results_dir="./results_enhanced_test",
            return_dict=True
        )
        
        elapsed_time = time.time() - start_time
        
        # æ£€æŸ¥ç»“æœ
        video = result["videos"]
        print(f"ğŸ‰ Enhanced SDS ç”ŸæˆæˆåŠŸ!")
        print(f"ğŸ“Š è§†é¢‘å½¢çŠ¶: {video.shape}")
        print(f"â±ï¸ ç”¨æ—¶: {elapsed_time:.2f} ç§’")
        
        # æ£€æŸ¥ NaN
        if torch.isnan(video).any():
            print("âŒ æ£€æµ‹åˆ° NaN å€¼!")
            return False
        else:
            print("âœ… æ—  NaN é—®é¢˜")
        
        print("ğŸ‰ Enhanced DynamiCrafter Guidance Pipeline æµ‹è¯•æˆåŠŸ!")
        print("ğŸ“‹ æµ‹è¯•æ€»ç»“:")
        print("  âœ… æ¨¡å‹åŠ è½½æ­£å¸¸")
        print("  âœ… å›¾åƒé¢„å¤„ç†æ­£å¸¸")
        print("  âœ… æ–‡æœ¬ç¼–ç æ­£å¸¸")
        print("  âœ… Enhanced SDS ä¼˜åŒ–æ­£å¸¸")
        print("  âœ… è§†é¢‘è§£ç æ­£å¸¸")
        print("  âœ… Enhanced ä¿å­˜åŠŸèƒ½æ­£å¸¸")
        print("  âœ… æ— NaNé—®é¢˜")
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == '__main__':
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == 'test':
        test_dynamicrafter_guidance_pipeline()
    else:
        print("ğŸ“ DynamiCrafter Guidance Pipeline")
        print("ğŸ”¬ æ­£ç¡®çš„æ”¹é€ æ€è·¯ï¼š")
        print("  â€¢ ä¿æŒ DynamiCrafter çš„æ‰€æœ‰æ ¸å¿ƒé€»è¾‘")
        print("  â€¢ åªæ›¿æ¢ scheduler.sample() ä¸º optimization loop")
        print("  â€¢ åœ¨ optimization loop ä¸­ä½¿ç”¨ DynamiCrafter çš„ model.apply_model()")
        print("  â€¢ å¢å¼ºçš„å¯è§†åŒ–å’Œä¿å­˜åŠŸèƒ½")
        print("")
        print("ğŸ¯ è½¬æ¢å…³ç³»ï¼š")
        print("  â€¢ pipeline_normal_to_flux.py â†’ guidance_flux_pipeline.py")
        print("  â€¢ dynamicrafter_pipeline.py â†’ guidance_pipeline.py")
        print("")
        print("ğŸ¨ å¯è§†åŒ–åŠŸèƒ½ï¼š")
        print("  â€¢ ç»„ç»‡åŒ–çš„è¾“å‡ºç›®å½•ç»“æ„")
        print("  â€¢ è¾“å…¥æ¡ä»¶ä¿å­˜ (åŸå§‹å›¾åƒã€å¤„ç†åå›¾åƒã€æç¤ºè¯)")
        print("  â€¢ å‚æ•°ä¿å­˜ (å®Œæ•´çš„å‚æ•°è®°å½•)")
        print("  â€¢ ä¼˜åŒ–è¿‡ç¨‹è§†é¢‘ç”Ÿæˆ")
        print("  â€¢ å¢å¼ºçš„ debug ä¿å­˜åŠŸèƒ½")
        print("")
        print("ğŸ“– ä½¿ç”¨ç¤ºä¾‹:")
        print("```python")
        print("from guidance_pipeline import DynamiCrafterGuidancePipeline")
        print("from PIL import Image")
        print("")
        print("# åˆå§‹åŒ– pipeline")
        print("pipeline = DynamiCrafterGuidancePipeline(resolution='256_256')")
        print("")
        print("# åŠ è½½å›¾åƒ")
        print("image = Image.open('your_image.jpg')")
        print("")
        print("# Enhanced SDS ä¼˜åŒ–ç”Ÿæˆ")
        print("result = pipeline(")
        print("    image=image,")
        print("    prompt='person walking in garden',")
        print("    num_optimization_steps=1000,")
        print("    loss_type='sds',")
        print("    cfg_scale=7.5,")
        print("    # Enhanced saving")
        print("    save_results=True,")
        print("    save_debug_images=True,")
        print("    save_debug_videos=True,")
        print("    save_process_video=True,")
        print("    debug_save_interval=100")
        print(")")
        print("")
        print("# ä¿å­˜è§†é¢‘")
        print("pipeline.save_video(result['videos'], 'output.mp4')")
        print("```")
