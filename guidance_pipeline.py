# dynamicrafter_guidance_pipeline.py

"""
DynamiCrafter Guidance Pipeline

èåˆäº†ä»¥ä¸‹ä¸‰ä¸ªæºæ–‡ä»¶çš„æŠ€æœ¯ï¼š
1. scripts/gradio/dynamicrafter_pipeline.py - DynamiCrafter æ ¸å¿ƒå®ç°
2. _reference_codes/Stable3DGen/flux_kontext_pipeline.py - Flux Kontext æ ‡å‡†æµæ°´çº¿
3. _reference_codes/Stable3DGen/2D_experiments/guidance_flux_pipeline.py - Flux Guidance ä¼˜åŒ–æŠ€æœ¯

ä½œè€…ï¼šAssistant
åˆ›å»ºæ—¶é—´ï¼š2025å¹´
"""

import os
import time
import torch
import torch.nn.functional as F
import numpy as np
from typing import Optional, Union, List, Any, Dict, Callable
from dataclasses import dataclass
from omegaconf import OmegaConf
from einops import repeat, rearrange
import torchvision.transforms as transforms
from pytorch_lightning import seed_everything
from huggingface_hub import hf_hub_download
from PIL import Image

# DynamiCrafter imports - æ¥è‡ª scripts/gradio/dynamicrafter_pipeline.py
from utils.utils import instantiate_from_config
from scripts.evaluation.funcs import load_model_checkpoint, save_videos, get_latent_z
from lvdm.models.utils_diffusion import make_ddim_sampling_parameters


@dataclass
class DynamiCrafterGuidanceConfig:
    """
    DynamiCrafter Guidance ä¼˜åŒ–é…ç½®
    
    è®¾è®¡çµæ„Ÿæ¥æºï¼š
    - guidance_flux_pipeline.py:FluxGuidanceConfig (åŸºç¡€ç»“æ„)
    - dynamicrafter_pipeline.py (DynamiCrafterç‰¹å®šå‚æ•°)
    """
    # === æ¥è‡ª guidance_flux_pipeline.py:FluxGuidanceConfig ===
    num_optimization_steps: int = 1000
    learning_rate: float = 0.05
    loss_type: str = "sds"  # sds, csd, rfds
    weight_type: str = "auto"  # auto, t, ada, uniform
    cfg_scale: float = 7.5
    optimizer_type: str = "AdamW"  # AdamW, Adam
    
    # === æ–°å¢ï¼šDynamiCrafter ç‰¹å®šå‚æ•° ===
    frame_stride: int = 3
    temporal_guidance_scale: float = 1.0  # æ—¶é—´ç»´åº¦çš„é¢å¤–å¼•å¯¼
    
    # === æ¥è‡ª guidance_flux_pipeline.py:FluxGuidanceConfig ===
    min_step_ratio_start: float = 0.02
    min_step_ratio_end: float = 0.02
    max_step_ratio_start: float = 0.98
    max_step_ratio_end: float = 0.98
    
    weight_eps: float = 0.0
    
    # === ä¿®æ”¹ï¼šé’ˆå¯¹è§†é¢‘çš„ Debug å‚æ•° ===
    save_debug_videos: bool = False  # åŸæ¥æ˜¯ save_debug_images
    debug_save_interval: int = 100
    debug_save_path: str = "debug_videos"  # åŸæ¥æ˜¯ debug_images


class DynamiCrafterGuidancePipeline:
    """
    DynamiCrafter Guidance Pipeline for gradient-based video optimization.
    
    æ¶æ„ç»§æ‰¿å…³ç³»ï¼š
    - åŸºç¡€ç»“æ„ï¼šguidance_flux_pipeline.py:FluxGuidancePipeline
    - æ¨¡å‹åŠ è½½ï¼šdynamicrafter_pipeline.py:DynamiCrafterImg2VideoPipeline  
    - ä¼˜åŒ–å¾ªç¯ï¼šguidance_flux_pipeline.py:FluxGuidancePipeline._optimization_loop
    """
    
    def __init__(
        self, 
        resolution: str = '256_256',
        device: Optional[str] = None,
        dtype: Optional[torch.dtype] = None,
        **kwargs
    ):
        """
        åˆå§‹åŒ–å‡½æ•°
        
        å‚è€ƒæ¥æºï¼š
        - dynamicrafter_pipeline.py:DynamiCrafterImg2VideoPipeline.__init__ (ä¸»ä½“ç»“æ„)
        - guidance_flux_pipeline.py:FluxGuidancePipeline.__init__ (guidance_configéƒ¨åˆ†)
        """
        # === æ¥è‡ª dynamicrafter_pipeline.py:DynamiCrafterImg2VideoPipeline.__init__ ===
        self.resolution = tuple(map(int, resolution.split('_')))  # (height, width)
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.dtype = dtype or torch.float32
        
        # === æ¥è‡ª guidance_flux_pipeline.py:FluxGuidancePipeline.__init__ ===
        # Initialize guidance config
        self.guidance_config = DynamiCrafterGuidanceConfig()
        
        # === æ¥è‡ª dynamicrafter_pipeline.py:DynamiCrafterImg2VideoPipeline.__init__ ===
        # Load model components
        self._load_components()
        self._setup_image_processor()
        
        # === æ¥è‡ª guidance_flux_pipeline.py:FluxGuidancePipeline._optimize_memory ===
        self._optimize_memory()
        
        # === æ¥è‡ª dynamicrafter_pipeline.py:DynamiCrafterImg2VideoPipeline.__init__ ===
        # Pipeline state
        self._is_initialized = True
        print(f"âœ… DynamiCrafter Guidance Pipeline initialized: {self.resolution[0]}x{self.resolution[1]}")
        
    def _load_components(self):
        """
        åŠ è½½ DynamiCrafter æ¨¡å‹å’Œç»„ä»¶
        
        å®Œå…¨æ¥è‡ªï¼šdynamicrafter_pipeline.py:DynamiCrafterImg2VideoPipeline._load_components
        """
        # Download model if needed
        self._download_model()
        
        # Load model - ä¿®å¤é…ç½®æ–‡ä»¶è·¯å¾„
        project_root = os.path.dirname(__file__)
        ckpt_path = os.path.join(project_root, f'checkpoints/dynamicrafter_{self.resolution[1]}_v1/model.ckpt')
        config_file = os.path.join(project_root, f'configs/inference_{self.resolution[1]}_v1.0.yaml')
        
        config = OmegaConf.load(config_file)
        model_config = config.pop("model", OmegaConf.create())
        model_config['params']['unet_config']['params']['use_checkpoint'] = False
        
        self.model = instantiate_from_config(model_config)
        assert os.path.exists(ckpt_path), f"Error: checkpoint Not Found at {ckpt_path}!"
        self.model = load_model_checkpoint(self.model, ckpt_path)
        self.model.eval()
        
        # Move to device
        self.model = self.model.to(self.device)
        self._ensure_device_consistency()
        
        print(f"ğŸ”„ DynamiCrafter model loaded on {self.device}")
        
    def _ensure_device_consistency(self):
        """
        ç¡®ä¿æ‰€æœ‰æ¨¡å‹ç»„ä»¶åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        
        æ¥è‡ªï¼šdynamicrafter_pipeline.py:DynamiCrafterImg2VideoPipeline._load_components 
        (move_to_device å‡½æ•°éƒ¨åˆ†)
        """
        def move_to_device(module):
            for name, param in module.named_parameters():
                if param.device != torch.device(self.device):
                    param.data = param.data.to(self.device)
            for name, buffer in module.named_buffers():
                if buffer.device != torch.device(self.device):
                    buffer.data = buffer.data.to(self.device)
            for name, child in module.named_children():
                move_to_device(child)
        
        move_to_device(self.model)
        
        # ç‰¹åˆ«å¤„ç†å„ä¸ªç¼–ç å™¨
        for component_name in ['cond_stage_model', 'first_stage_model', 'embedder', 'image_proj_model']:
            if hasattr(self.model, component_name):
                component = getattr(self.model, component_name)
                component = component.to(self.device)
                move_to_device(component)
        
    def _optimize_memory(self):
        """
        ä¼˜åŒ–å†…å­˜ä½¿ç”¨
        
        å‚è€ƒæ¥æºï¼š
        - guidance_flux_pipeline.py:FluxGuidancePipeline._optimize_memory (æ ¸å¿ƒæ€è·¯)
        - é€‚é… DynamiCrafter çš„ç»„ä»¶ç»“æ„
        """
        # å°†æ–‡æœ¬ç¼–ç å™¨ç§»è‡³ CPU ä»¥èŠ‚çœæ˜¾å­˜
        if hasattr(self.model, 'cond_stage_model'):
            self.model.cond_stage_model.to("cpu")
        
        # ä¿æŒ VAE å’Œä¸»æ¨¡å‹åœ¨ GPU ä¸Š
        if hasattr(self.model, 'first_stage_model'):
            self.model.first_stage_model.to(self.device)
        
        torch.cuda.empty_cache()
        
    def _setup_image_processor(self):
        """
        è®¾ç½®å›¾åƒé¢„å¤„ç†
        
        æ¥è‡ªï¼šdynamicrafter_pipeline.py:DynamiCrafterImg2VideoPipeline._setup_image_processor
        """
        self.image_processor = transforms.Compose([
            transforms.Resize(min(self.resolution)),
            transforms.CenterCrop(self.resolution),
        ])
        
    def _download_model(self):
        """
        ä¸‹è½½æ¨¡å‹æƒé‡
        
        æ¥è‡ªï¼šdynamicrafter_pipeline.py:DynamiCrafterImg2VideoPipeline._download_model
        """
        REPO_ID = f'Doubiiu/DynamiCrafter_{self.resolution[1]}' if self.resolution[1] != 256 else 'Doubiiu/DynamiCrafter'
        filename_list = ['model.ckpt']
        
        # ä¿®å¤ï¼šguidance_pipeline.py ä½äºé¡¹ç›®æ ¹ç›®å½•ï¼Œä¸éœ€è¦å¾€ä¸Šä¸¤çº§
        project_root = os.path.dirname(__file__)
        model_dir = os.path.join(project_root, f'checkpoints/dynamicrafter_{self.resolution[1]}_v1/')
        
        if not os.path.exists(model_dir):
            os.makedirs(model_dir)
            
        for filename in filename_list:
            local_file = os.path.join(model_dir, filename)
            if not os.path.exists(local_file):
                print(f"ğŸ“¥ Downloading model: {filename}")
                hf_hub_download(
                    repo_id=REPO_ID, 
                    filename=filename, 
                    local_dir=model_dir, 
                    local_dir_use_symlinks=False
                )
                print(f"âœ… Download completed: {filename}")

    def _preprocess_image(self, image, height=None, width=None):
        """
        é¢„å¤„ç†è¾“å…¥å›¾åƒ
        
        æ¥è‡ªï¼šdynamicrafter_pipeline.py:DynamiCrafterImg2VideoPipeline._preprocess_image
        """
        if isinstance(image, Image.Image):
            image = np.array(image)
        
        if isinstance(image, np.ndarray):
            image = torch.from_numpy(image).permute(2, 0, 1).float()
        
        # æ ‡å‡†åŒ–åˆ° [-1, 1]
        if image.max() > 1.0:
            image = image / 255.0
        image = (image - 0.5) * 2
        
        # è°ƒæ•´å¤§å°
        if height is not None and width is not None:
            transform = transforms.Compose([
                transforms.Resize((height, width)),
            ])
        else:
            transform = self.image_processor
        
        return transform(image)

    def _encode_prompt(self, prompt, negative_prompt, device):
        """
        ç¼–ç æ–‡æœ¬æç¤º
        
        æ¥æºç»“åˆï¼š
        - dynamicrafter_pipeline.py:DynamiCrafterImg2VideoPipeline._encode_prompt (ä¸»ä½“é€»è¾‘)
        - guidance_flux_pipeline.py:FluxGuidancePipeline._get_t5_prompt_embeds (å†…å­˜ä¼˜åŒ–éƒ¨åˆ†)
        """
        # === æ¥è‡ª guidance_flux_pipeline.py çš„å†…å­˜ä¼˜åŒ–æŠ€æœ¯ ===
        # ä¸´æ—¶å°†æ–‡æœ¬ç¼–ç å™¨ç§»åˆ° GPU
        if hasattr(self.model, 'cond_stage_model'):
            self.model.cond_stage_model = self.model.cond_stage_model.to(device)
        
        # === æ¥è‡ª dynamicrafter_pipeline.py çš„æ ¸å¿ƒç¼–ç é€»è¾‘ ===
        with torch.no_grad():
            # æ­£å‘æç¤º
            text_embeddings = self.model.get_learned_conditioning(prompt)
            
            # è´Ÿå‘æç¤º
            if negative_prompt is not None:
                uncond_embeddings = self.model.get_learned_conditioning(negative_prompt)
            else:
                if self.model.uncond_type == "empty_seq":
                    uncond_embeddings = self.model.get_learned_conditioning([""] * len(prompt))
                elif self.model.uncond_type == "zero_embed":
                    uncond_embeddings = torch.zeros_like(text_embeddings)
        
        # === æ¥è‡ª guidance_flux_pipeline.py çš„å†…å­˜ä¼˜åŒ–æŠ€æœ¯ ===
        # å°†æ–‡æœ¬ç¼–ç å™¨ç§»å› CPU
        if hasattr(self.model, 'cond_stage_model'):
            self.model.cond_stage_model.to("cpu")
        
        torch.cuda.empty_cache()
        
        return {"cond": text_embeddings, "uncond": uncond_embeddings}
    
    def _encode_image(self, image, num_frames):
        """
        ç¼–ç è¾“å…¥å›¾åƒ
        
        æ¥è‡ªï¼šdynamicrafter_pipeline.py:DynamiCrafterImg2VideoPipeline._encode_image
        """
        # æ·»åŠ æ‰¹æ¬¡ç»´åº¦
        if image.dim() == 3:
            image = image.unsqueeze(0)
        
        image = image.to(self.device)
        
        with torch.no_grad():
            # å›¾åƒåµŒå…¥
            cond_images = self.model.embedder(image)
            img_emb = self.model.image_proj_model(cond_images)
            
            # æ½œåœ¨è¡¨ç¤º
            videos = image.unsqueeze(2)  # æ·»åŠ æ—¶é—´ç»´åº¦
            z = get_latent_z(self.model, videos)
        
        return img_emb, z
    
    def _prepare_conditioning(self, text_embeddings, image_embeddings, image_latents, 
                             frame_stride, guidance_scale, batch_size):
        """
        å‡†å¤‡æ¡ä»¶è¾“å…¥
        
        æ¥è‡ªï¼šdynamicrafter_pipeline.py:DynamiCrafterImg2VideoPipeline._prepare_conditioning
        """
        # ç»„åˆæ–‡æœ¬å’Œå›¾åƒåµŒå…¥
        cond_emb = torch.cat([text_embeddings["cond"], image_embeddings], dim=1)
        cond = {"c_crossattn": [cond_emb]}
        
        # å›¾åƒæ¡ä»¶ï¼ˆhybridæ¨¡å¼ï¼‰
        if self.model.model.conditioning_key == 'hybrid':
            img_cat_cond = image_latents[:,:,:1,:,:]
            img_cat_cond = repeat(img_cat_cond, 'b c t h w -> b c (repeat t) h w', 
                                 repeat=self.model.temporal_length)
            cond["c_concat"] = [img_cat_cond]
        
        # æ— æ¡ä»¶è¾“å…¥
        uc = None
        if guidance_scale != 1.0:
            # åˆ›å»ºæ­£ç¡®å½¢çŠ¶çš„é›¶å›¾åƒ
            zero_image = torch.zeros((batch_size, 3, self.resolution[0], self.resolution[1]), 
                                   device=self.model.device, dtype=self.model.dtype)
            uc_img_emb = self.model.embedder(zero_image)
            uc_img_emb = self.model.image_proj_model(uc_img_emb)
            
            uc_emb = torch.cat([text_embeddings["uncond"], uc_img_emb], dim=1)
            uc = {"c_crossattn": [uc_emb]}
            
            if self.model.model.conditioning_key == 'hybrid':
                uc["c_concat"] = [img_cat_cond]
        
        # å¸§æ­¥é•¿
        fs = torch.tensor([frame_stride] * batch_size, dtype=torch.long, device=self.model.device)
        
        return {"cond": cond, "uc": uc, "fs": fs}

    def sample_timestep_ddim(self, batch_size, min_step_ratio=0.02, max_step_ratio=0.98, ddim_steps=50):
        """
        ä¸º DynamiCrafter é‡‡æ · DDIM æ—¶é—´æ­¥
        
        å‚è€ƒæ¥æºï¼š
        - guidance_flux_pipeline.py:FluxGuidancePipeline.sample_timestep (é‡‡æ ·é€»è¾‘)
        - dynamicrafter_pipeline.py ä¸­çš„ DDIM ç›¸å…³ä»£ç  (æ—¶é—´æ­¥è®¡ç®—)
        """
        # åˆ›å»º DDIM æ—¶é—´æ­¥
        ddim_timesteps = np.linspace(0, self.model.num_timesteps - 1, ddim_steps).astype(np.int64)
        
        # æ ¹æ®æ¯”ä¾‹é€‰æ‹©æ—¶é—´æ­¥èŒƒå›´
        min_idx = int(len(ddim_timesteps) * min_step_ratio)
        max_idx = int(len(ddim_timesteps) * max_step_ratio)
        max_idx = max(max_idx, min_idx + 1)
        
        # éšæœºé‡‡æ ·æ—¶é—´æ­¥ç´¢å¼•
        t_idx = torch.randint(min_idx, max_idx, (batch_size,), device="cpu")
        
        # ä¿®å¤ï¼šç¡®ä¿æ­£ç¡®å¤„ç† numpy æ•°ç»„ç´¢å¼•
        t_values = ddim_timesteps[t_idx.cpu().numpy()]
        t = torch.from_numpy(t_values).to(self.device)
        
        return t
    
    def add_noise_ddim(self, original_samples, noise, timesteps):
        """
        ä¸º DynamiCrafter æ·»åŠ  DDIM é£æ ¼çš„å™ªå£°
        
        å‚è€ƒæ¥æºï¼š
        - guidance_flux_pipeline.py:FluxGuidancePipeline.add_noise_flow_matching (å™ªå£°æ·»åŠ æ€è·¯)
        - é€‚é…ä¸º DDIM çš„å™ªå£°è°ƒåº¦æ–¹å¼
        """
        # è·å– alpha_cumprod
        alphas_cumprod = self.model.alphas_cumprod.to(self.device)
        
        # ç¡®ä¿æ—¶é—´æ­¥åœ¨æ­£ç¡®èŒƒå›´å†…
        timesteps = timesteps.clamp(0, len(alphas_cumprod) - 1)
        
        # è·å–å¯¹åº”çš„ alpha_cumprod å€¼
        alpha_t = alphas_cumprod[timesteps]
        
        # ç¡®ä¿ç»´åº¦åŒ¹é…
        while len(alpha_t.shape) < len(original_samples.shape):
            alpha_t = alpha_t.unsqueeze(-1)
        
        # DDIM å™ªå£°æ·»åŠ : x_t = sqrt(alpha_t) * x_0 + sqrt(1 - alpha_t) * noise
        sqrt_alpha_t = torch.sqrt(alpha_t)
        sqrt_one_minus_alpha_t = torch.sqrt(1.0 - alpha_t)
        
        noisy_samples = sqrt_alpha_t * original_samples + sqrt_one_minus_alpha_t * noise
        return noisy_samples

    def __call__(
        self,
        image: Union[Image.Image, np.ndarray, torch.Tensor],
        prompt: Union[str, List[str]] = "",
        negative_prompt: Optional[Union[str, List[str]]] = None,
        height: Optional[int] = None,
        width: Optional[int] = None,
        num_frames: Optional[int] = None,
        # === æ¥è‡ª guidance_flux_pipeline.py çš„ Guidance å‚æ•° ===
        num_optimization_steps: int = 1000,
        learning_rate: float = 0.05,
        loss_type: str = "sds",
        weight_type: str = "auto",
        cfg_scale: float = 7.5,
        optimizer_type: str = "AdamW",
        # === DynamiCrafter ç‰¹æœ‰å‚æ•° ===
        frame_stride: int = 3,
        temporal_guidance_scale: float = 1.0,
        # === æ¥è‡ª guidance_flux_pipeline.py çš„åŠ¨æ€å‚æ•° ===
        min_step_ratio_start: Optional[float] = None,
        min_step_ratio_end: Optional[float] = None,
        max_step_ratio_start: Optional[float] = None,
        max_step_ratio_end: Optional[float] = None,
        # === æ¥è‡ª guidance_flux_pipeline.py çš„ Debug å‚æ•°ï¼ˆä¿®æ”¹ä¸ºè§†é¢‘ï¼‰ ===
        save_debug_videos: bool = False,  # åŸæ¥æ˜¯ save_debug_images
        debug_save_interval: int = 100,
        debug_save_path: str = "debug_videos",  # åŸæ¥æ˜¯ debug_images
        # === æ¥è‡ª dynamicrafter_pipeline.py çš„æ ‡å‡†å‚æ•° ===
        generator: Optional[torch.Generator] = None,
        output_type: str = "tensor",
        return_dict: bool = True,
        **kwargs
    ):
        """
        ä¸»è°ƒç”¨å‡½æ•°
        
        å‚è€ƒæ¥æºï¼š
        - guidance_flux_pipeline.py:FluxGuidancePipeline.__call__ (æ•´ä½“ç»“æ„å’Œguidanceå‚æ•°)
        - dynamicrafter_pipeline.py:DynamiCrafterImg2VideoPipeline.__call__ (è§†é¢‘ç”Ÿæˆé€»è¾‘)
        - èåˆä¸¤è€…çš„ä¼˜åŠ¿ï¼Œä¸ºè§†é¢‘ç”Ÿæˆæä¾› guidance ä¼˜åŒ–
        """
        
        # === æ¥è‡ª guidance_flux_pipeline.py:FluxGuidancePipeline.__call__ ===
        # Set default values from config
        if min_step_ratio_start is None:
            min_step_ratio_start = self.guidance_config.min_step_ratio_start
        if min_step_ratio_end is None:
            min_step_ratio_end = self.guidance_config.min_step_ratio_end
        if max_step_ratio_start is None:
            max_step_ratio_start = self.guidance_config.max_step_ratio_start
        if max_step_ratio_end is None:
            max_step_ratio_end = self.guidance_config.max_step_ratio_end
        
        # === æ¥è‡ª dynamicrafter_pipeline.py:DynamiCrafterImg2VideoPipeline.__call__ ===
        # Input validation and standardization
        if isinstance(prompt, str):
            batch_size = 1
            prompt = [prompt]
        else:
            batch_size = len(prompt)
        
        # Handle negative_prompt
        if negative_prompt is not None:
            if isinstance(negative_prompt, str):
                negative_prompt = [negative_prompt] * batch_size
            elif len(negative_prompt) != batch_size:
                raise ValueError(f"negative_prompt length ({len(negative_prompt)}) != batch_size ({batch_size})")
        
        device = self.device
        
        print(f"ğŸ¬ å¼€å§‹ DynamiCrafter Guidance ä¼˜åŒ–...")
        print(f"ğŸ“ æç¤ºè¯: {prompt}")
        print(f"ğŸ”§ å‚æ•°: steps={num_optimization_steps}, lr={learning_rate}, loss={loss_type}")
        print(f"ğŸ’» è®¾å¤‡: {device}")
        
        # === æ¥è‡ª dynamicrafter_pipeline.py:DynamiCrafterImg2VideoPipeline.__call__ ===
        # Image preprocessing
        processed_image = self._preprocess_image(image, height, width)
        
        # Model parameters
        num_frames = num_frames or self.model.temporal_length
        channels = self.model.model.diffusion_model.out_channels
        if height is None or width is None:
            height, width = self.resolution
        latent_height, latent_width = height // 8, width // 8
        
        # Noise shape for video latents (includes time dimension)
        noise_shape = (batch_size, channels, num_frames, latent_height, latent_width)
        
        with torch.no_grad():
            # Encode prompts and image
            text_embeddings = self._encode_prompt(prompt, negative_prompt, device)
            image_embeddings, image_latents = self._encode_image(processed_image, num_frames)
            
            # Prepare conditioning
            conditioning = self._prepare_conditioning(
                text_embeddings=text_embeddings,
                image_embeddings=image_embeddings,
                image_latents=image_latents,
                frame_stride=frame_stride,
                guidance_scale=cfg_scale,
                batch_size=batch_size
            )
        
        # === æ¥è‡ª dynamicrafter_pipeline.py çš„åˆå§‹åŒ–é€»è¾‘ ===
        # Initialize video latents for optimization
        if generator is not None:
            video_latents = torch.randn(noise_shape, generator=generator, device=device, dtype=torch.float32)
        else:
            video_latents = torch.randn(noise_shape, device=device, dtype=torch.float32)
        
        # === æ¥è‡ª guidance_flux_pipeline.py:FluxGuidancePipeline.__call__ ===
        # Start optimization loop
        return self._optimization_loop(
            video_latents=video_latents,
            conditioning=conditioning,
            height=height,
            width=width,
            num_frames=num_frames,
            num_optimization_steps=num_optimization_steps,
            learning_rate=learning_rate,
            loss_type=loss_type,
            weight_type=weight_type,
            cfg_scale=cfg_scale,
            optimizer_type=optimizer_type,
            temporal_guidance_scale=temporal_guidance_scale,
            min_step_ratio_start=min_step_ratio_start,
            min_step_ratio_end=min_step_ratio_end,
            max_step_ratio_start=max_step_ratio_start,
            max_step_ratio_end=max_step_ratio_end,
            output_type=output_type,
            return_dict=return_dict,
            save_debug_videos=save_debug_videos,
            debug_save_interval=debug_save_interval,
            debug_save_path=debug_save_path,
        )

    def _optimization_loop(
        self,
        video_latents,
        conditioning,
        height,
        width,
        num_frames,
        num_optimization_steps,
        learning_rate,
        loss_type,
        weight_type,
        cfg_scale,
        optimizer_type,
        temporal_guidance_scale,
        min_step_ratio_start,
        min_step_ratio_end,
        max_step_ratio_start,
        max_step_ratio_end,
        output_type,
        return_dict,
        save_debug_videos,
        debug_save_interval,
        debug_save_path,
    ):
        """
        DynamiCrafter guidance optimization loop.
        
        ä¸»è¦å‚è€ƒï¼šguidance_flux_pipeline.py:FluxGuidancePipeline._optimization_loop
        ä¿®æ”¹é€‚é…ï¼šDynamiCrafter çš„è§†é¢‘è§£ç å’Œä¿å­˜æ–¹å¼
        """
        
        # === æ¥è‡ª guidance_flux_pipeline.py:FluxGuidancePipeline._optimization_loop ===
        # Create debug directory if needed
        if save_debug_videos:
            os.makedirs(debug_save_path, exist_ok=True)
            print(f"[INFO] Debug videos will be saved to: {debug_save_path}")
        
        # Set up latents for optimization
        video_latents = video_latents.to(torch.float32).detach().clone()
        video_latents.requires_grad_(True)
        
        # Set up optimizer
        if optimizer_type == "AdamW":
            optimizer = torch.optim.AdamW([video_latents], lr=learning_rate, betas=(0.9, 0.99), eps=1e-8)
        elif optimizer_type == "Adam":
            optimizer = torch.optim.Adam([video_latents], lr=learning_rate, betas=(0.9, 0.999), eps=1e-8)
        else:
            raise ValueError(f"Unknown optimizer_type: {optimizer_type}")
        
        # Determine effective weight type
        if weight_type == "auto":
            if loss_type == "sds":
                effective_weight_type = "t"
            elif loss_type == "csd":
                effective_weight_type = "ada"
            else:  # rfds
                effective_weight_type = "uniform"
        else:
            effective_weight_type = weight_type
        
        print(f"[INFO] Starting DynamiCrafter guidance optimization with {num_optimization_steps} steps")
        print(f"[INFO] Loss type: {loss_type.upper()}")
        print(f"[INFO] Weight type: {effective_weight_type}")
        print(f"[INFO] Learning rate: {learning_rate}")
        print(f"[INFO] CFG scale: {cfg_scale}")
        print(f"[INFO] Temporal guidance scale: {temporal_guidance_scale}")
        print("-" * 60)
        
        # === ä¿®æ”¹ï¼šé€‚é… DynamiCrafter çš„è§†é¢‘ä¿å­˜ ===
        # Helper function to save debug videos
        def save_debug_video(step, current_latents):
            with torch.no_grad():
                try:
                    # === æ¥è‡ª guidance_flux_pipeline.py çš„å†…å­˜ä¼˜åŒ–æ€è·¯ ===
                    # Move non-essential components to CPU
                    if hasattr(self.model, 'cond_stage_model'):
                        self.model.cond_stage_model.to("cpu")
                    
                    torch.cuda.empty_cache()
                    
                    # === æ¥è‡ª dynamicrafter_pipeline.py çš„è§†é¢‘è§£ç é€»è¾‘ ===
                    # Decode latents to video
                    debug_videos = self.model.decode_first_stage(current_latents.detach())
                    
                    # Post-process and save
                    debug_videos = debug_videos.cpu().float().numpy()
                    
                    # === æ¥è‡ª dynamicrafter_pipeline.py çš„è§†é¢‘ä¿å­˜å‡½æ•° ===
                    # Save using DynamiCrafter's save_videos function
                    filename = f"step_{step:06d}"
                    save_videos(debug_videos.unsqueeze(1), debug_save_path, filenames=[filename], fps=8)
                    
                    print(f"[DEBUG] Saved debug video: {debug_save_path}/{filename}.mp4")
                    
                    torch.cuda.empty_cache()
                    
                except Exception as e:
                    print(f"[DEBUG] Failed to save debug video: {e}")
        
        # === æ¥è‡ª guidance_flux_pipeline.py:FluxGuidancePipeline._optimization_loop ===
        # Optimization loop
        for i in range(num_optimization_steps):
            optimizer.zero_grad()
            
            # Calculate current step ratios
            progress = i / (num_optimization_steps - 1) if num_optimization_steps > 1 else 0.0
            current_min_step_ratio = (
                min_step_ratio_start + 
                (min_step_ratio_end - min_step_ratio_start) * progress
            )
            current_max_step_ratio = (
                max_step_ratio_start + 
                (max_step_ratio_end - max_step_ratio_start) * progress
            )
            
            # === æ–°å¢ï¼šé€‚é… DynamiCrafter çš„æŸå¤±å‡½æ•° ===
            # Compute guidance loss
            if loss_type == "sds":
                loss = self._sds_loss_video(
                    video_latents,
                    conditioning,
                    cfg_scale=cfg_scale,
                    temporal_guidance_scale=temporal_guidance_scale,
                    min_step_ratio=current_min_step_ratio,
                    max_step_ratio=current_max_step_ratio,
                    weight_type=effective_weight_type,
                )
            elif loss_type == "csd":
                loss = self._csd_loss_video(
                    video_latents,
                    conditioning,
                    cfg_scale=cfg_scale,
                    temporal_guidance_scale=temporal_guidance_scale,
                    min_step_ratio=current_min_step_ratio,
                    max_step_ratio=current_max_step_ratio,
                    weight_type=effective_weight_type,
                )
            elif loss_type == "rfds":
                loss = self._rfds_loss_video(
                    video_latents,
                    conditioning,
                    cfg_scale=cfg_scale,
                    temporal_guidance_scale=temporal_guidance_scale,
                    min_step_ratio=current_min_step_ratio,
                    max_step_ratio=current_max_step_ratio,
                    weight_type=effective_weight_type,
                )
            else:
                raise ValueError(f"Unknown loss_type: {loss_type}")
            
            loss.backward()
            optimizer.step()
            
            # Save debug videos if enabled
            if save_debug_videos and (i + 1) % debug_save_interval == 0:
                save_debug_video(i + 1, video_latents)
            
            # Print progress
            print_interval = max(1, min(100, num_optimization_steps // 10))
            if (i + 1) % print_interval == 0 or i == 0:
                print(f"[PROGRESS] Step {i+1:4d}/{num_optimization_steps} | Loss: {loss.item():.6f} | Step ratio: [{current_min_step_ratio:.3f}, {current_max_step_ratio:.3f}]")
        
        # Save final debug video if enabled
        if save_debug_videos:
            save_debug_video(num_optimization_steps, video_latents)
        
        print(f"[INFO] DynamiCrafter guidance optimization completed! Final loss: {loss.item():.6f}")
        print("-" * 60)
        
        # === æ¥è‡ª guidance_flux_pipeline.py + dynamicrafter_pipeline.py ===
        # Final processing
        if output_type == "latent":
            video = video_latents
        else:
            with torch.no_grad():
                # === æ¥è‡ª guidance_flux_pipeline.py çš„å†…å­˜ä¼˜åŒ– ===
                # Move non-essential components to CPU for memory
                if hasattr(self.model, 'cond_stage_model'):
                    self.model.cond_stage_model.to("cpu")
                
                torch.cuda.empty_cache()
                
                # === æ¥è‡ª dynamicrafter_pipeline.py çš„è§†é¢‘è§£ç  ===
                # Decode latents to video
                video = self.model.decode_first_stage(video_latents)
                
                if output_type == "numpy":
                    video = video.cpu().float().numpy()
                elif output_type == "pil":
                    video = video.cpu().float().numpy()
                    print("ğŸ“ Note: PIL output conversion for videos not fully implemented, returning numpy")
        
        print(f"âœ… Video generation completed! Shape: {video.shape}")
        
        # === æ¥è‡ª dynamicrafter_pipeline.py çš„è¿”å›æ ¼å¼ ===
        if return_dict:
            return {"videos": video}
        else:
            return video

    def _sds_loss_video(self, video_latents, conditioning, cfg_scale=7.5, temporal_guidance_scale=1.0, 
                       min_step_ratio=0.02, max_step_ratio=0.98, weight_type="t"):
        """
        SDS loss for video latents using DynamiCrafter's 3D UNet.
        
        å‚è€ƒæ¥æºï¼š
        - guidance_flux_pipeline.py:FluxGuidancePipeline._sds_loss (SDS æŸå¤±è®¡ç®—é€»è¾‘)
        - é€‚é… DynamiCrafter çš„ 3D UNet å’Œ DDIM è°ƒåº¦
        """
        batch_size = video_latents.shape[0]
        
        # === é€‚é… DynamiCrafterï¼šä½¿ç”¨ DDIM æ—¶é—´æ­¥é‡‡æ · ===
        # Sample timesteps using DDIM
        t = self.sample_timestep_ddim(batch_size, min_step_ratio, max_step_ratio)
        
        # Add noise to video latents
        noise = torch.randn_like(video_latents)
        noisy_latents = self.add_noise_ddim(video_latents, noise, t)
        
        # === æ¥è‡ª dynamicrafter_pipeline.py çš„æ¡ä»¶å‡†å¤‡ ===
        # Prepare conditioning
        cond = conditioning["cond"]
        uc = conditioning["uc"]
        fs = conditioning["fs"]
        
        # === é€‚é… DynamiCrafterï¼šä½¿ç”¨ 3D UNet è¿›è¡Œå‰å‘ä¼ æ’­ ===
        # Forward pass with CFG
        with torch.no_grad():
            # ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„ apply_model æ–¹æ³•è€Œä¸æ˜¯ç›´æ¥è°ƒç”¨ diffusion_model
            # Conditional prediction
            noise_pred_cond = self.model.apply_model(
                noisy_latents, t, cond, **{"fs": fs}
            )
            
            # Unconditional prediction
            if uc is not None:
                noise_pred_uncond = self.model.apply_model(
                    noisy_latents, t, uc, **{"fs": fs}
                )
                
                # Apply CFG
                noise_pred = noise_pred_uncond + cfg_scale * (noise_pred_cond - noise_pred_uncond)
            else:
                noise_pred = noise_pred_cond
            
            # === æ–°å¢ï¼šåº”ç”¨æ—¶é—´å¼•å¯¼ ===
            # Apply temporal guidance if specified
            if temporal_guidance_scale != 1.0:
                noise_pred = noise_pred * temporal_guidance_scale
        
        # === æ¥è‡ª guidance_flux_pipeline.pyï¼Œé€‚é… DDIM ===
        # Calculate predicted original sample
        alpha_t = self.model.alphas_cumprod[t]
        while len(alpha_t.shape) < len(video_latents.shape):
            alpha_t = alpha_t.unsqueeze(-1)
        
        sqrt_alpha_t = torch.sqrt(alpha_t)
        sqrt_one_minus_alpha_t = torch.sqrt(1.0 - alpha_t)
        
        pred_original_sample = (noisy_latents - sqrt_one_minus_alpha_t * noise_pred) / sqrt_alpha_t
        
        # === æ¥è‡ª guidance_flux_pipeline.py:FluxGuidancePipeline._sds_loss ===
        # Calculate SDS gradient with different weighting strategies
        if weight_type == "t":
            # Time-dependent weighting
            w = (1.0 - alpha_t).view(batch_size, 1, 1, 1, 1)  # é€‚é…5Då¼ é‡ (video)
            grad = w * (video_latents - pred_original_sample.detach())
        elif weight_type == "ada":
            # Adaptive weighting
            weighting_factor = torch.abs(video_latents - pred_original_sample.detach()).mean(dim=(1, 2, 3, 4), keepdim=True)
            weighting_factor = torch.clamp(weighting_factor, 1e-4)
            grad = (video_latents - pred_original_sample.detach()) / weighting_factor
        elif weight_type == "uniform":
            # Uniform weighting
            grad = (video_latents - pred_original_sample.detach())
        else:
            raise ValueError(f"Unknown weight_type: {weight_type}")
        
        grad = grad.detach()
        grad = torch.nan_to_num(grad)
        
        # Construct loss using reparameterization trick
        target = (video_latents - grad).detach()
        loss = 0.5 * F.mse_loss(video_latents, target, reduction="mean") / batch_size
        
        torch.cuda.empty_cache()
        
        return loss

    def _csd_loss_video(self, video_latents, conditioning, cfg_scale=7.5, temporal_guidance_scale=1.0,
                       min_step_ratio=0.02, max_step_ratio=0.98, weight_type="ada"):
        """
        CSD loss for video latents using DynamiCrafter's 3D UNet.
        
        å‚è€ƒæ¥æºï¼š
        - guidance_flux_pipeline.py:FluxGuidancePipeline._csd_loss (CSD æŸå¤±è®¡ç®—é€»è¾‘)
        - é€‚é… DynamiCrafter çš„ 3D UNet å’Œæ—¶é—´ç»´åº¦
        """
        batch_size = video_latents.shape[0]
        
        # === é€‚é… DynamiCrafterï¼šä½¿ç”¨ DDIM æ—¶é—´æ­¥ ===
        # Sample timesteps
        t = self.sample_timestep_ddim(batch_size, min_step_ratio, max_step_ratio)
        
        # Add noise
        noise = torch.randn_like(video_latents)
        noisy_latents = self.add_noise_ddim(video_latents, noise, t)
        
        # === æ¥è‡ª dynamicrafter_pipeline.py çš„æ¡ä»¶å‡†å¤‡ ===
        # Prepare conditioning
        cond = conditioning["cond"]
        fs = conditioning["fs"]
        
        # === æ¥è‡ª guidance_flux_pipeline.pyï¼Œé€‚é… DynamiCrafter çš„ 3D UNet ===
        # Forward pass with low and high guidance
        with torch.no_grad():
            # ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„ apply_model æ–¹æ³•
            # Low guidance (fake distribution)
            noise_pred_fake = self.model.apply_model(
                noisy_latents, t, cond, **{"fs": fs}
            )
            
            # High guidance (real distribution) - simulate with higher noise
            noise_enhanced = noise * (1.0 + cfg_scale * 0.1)  # Slight enhancement
            noisy_latents_enhanced = self.add_noise_ddim(video_latents, noise_enhanced, t)
            
            noise_pred_real = self.model.apply_model(
                noisy_latents_enhanced, t, cond, **{"fs": fs}
            )
        
        # === æ¥è‡ª guidance_flux_pipeline.pyï¼Œé€‚é… DDIM ===
        # Calculate predicted original samples
        alpha_t = self.model.alphas_cumprod[t]
        while len(alpha_t.shape) < len(video_latents.shape):
            alpha_t = alpha_t.unsqueeze(-1)
        
        sqrt_alpha_t = torch.sqrt(alpha_t)
        sqrt_one_minus_alpha_t = torch.sqrt(1.0 - alpha_t)
        
        pred_fake_latents = (noisy_latents - sqrt_one_minus_alpha_t * noise_pred_fake) / sqrt_alpha_t
        pred_real_latents = (noisy_latents_enhanced - sqrt_one_minus_alpha_t * noise_pred_real) / sqrt_alpha_t
        
        # === æ¥è‡ª guidance_flux_pipeline.py:FluxGuidancePipeline._csd_loss ===
        # Calculate CSD gradient
        if weight_type == "ada":
            # Adaptive weighting
            weighting_factor = torch.abs(video_latents - pred_real_latents.detach()).mean(dim=(1, 2, 3, 4), keepdim=True)
            weighting_factor = torch.clamp(weighting_factor, 1e-4)
            grad = (pred_fake_latents - pred_real_latents) / weighting_factor
        elif weight_type == "t":
            # Time-dependent weighting
            w = (1.0 - alpha_t).view(batch_size, 1, 1, 1, 1)  # é€‚é…5Då¼ é‡
            grad = w * (pred_fake_latents - pred_real_latents)
        elif weight_type == "uniform":
            # Uniform weighting
            grad = (pred_fake_latents - pred_real_latents)
        else:
            raise ValueError(f"Unknown weight_type: {weight_type}")
        
        grad = grad.detach()
        grad = torch.nan_to_num(grad)
        
        # Construct loss
        target = (video_latents - grad).detach()
        loss = 0.5 * F.mse_loss(video_latents, target, reduction="mean") / batch_size
        
        torch.cuda.empty_cache()
        
        return loss

    def _rfds_loss_video(self, video_latents, conditioning, cfg_scale=7.5, temporal_guidance_scale=1.0,
                        min_step_ratio=0.02, max_step_ratio=0.98, weight_type="uniform"):
        """
        RFDS loss for video latents using DynamiCrafter's 3D UNet.
        
        å‚è€ƒæ¥æºï¼š
        - guidance_flux_pipeline.py:FluxGuidancePipeline._rfds_loss (RFDS æŸå¤±è®¡ç®—é€»è¾‘)
        - é€‚é… DynamiCrafter çš„ 3D UNet å’Œè§†é¢‘ç”Ÿæˆ
        """
        batch_size = video_latents.shape[0]
        
        # === é€‚é… DynamiCrafterï¼šä½¿ç”¨ DDIM æ—¶é—´æ­¥ ===
        # Sample timesteps
        t = self.sample_timestep_ddim(batch_size, min_step_ratio, max_step_ratio)
        
        # Add noise
        noise = torch.randn_like(video_latents)
        noisy_latents = self.add_noise_ddim(video_latents, noise, t)
        
        # === æ¥è‡ª dynamicrafter_pipeline.py çš„æ¡ä»¶å‡†å¤‡ ===
        # Prepare conditioning
        cond = conditioning["cond"]
        uc = conditioning["uc"]
        fs = conditioning["fs"]
        
        # === é€‚é… DynamiCrafterï¼šä½¿ç”¨ 3D UNet è¿›è¡Œå‰å‘ä¼ æ’­ ===
        # Forward pass with CFG
        with torch.no_grad():
            # ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„ apply_model æ–¹æ³•
            # Conditional prediction
            noise_pred_cond = self.model.apply_model(
                noisy_latents, t, cond, **{"fs": fs}
            )
            
            # Unconditional prediction
            if uc is not None:
                noise_pred_uncond = self.model.apply_model(
                    noisy_latents, t, uc, **{"fs": fs}
                )
                
                # Apply CFG
                noise_pred = noise_pred_uncond + cfg_scale * (noise_pred_cond - noise_pred_uncond)
            else:
                noise_pred = noise_pred_cond
        
        # === æ¥è‡ª guidance_flux_pipeline.pyï¼Œé€‚é… DDIM ===
        # Calculate predicted original sample
        alpha_t = self.model.alphas_cumprod[t]
        while len(alpha_t.shape) < len(video_latents.shape):
            alpha_t = alpha_t.unsqueeze(-1)
        
        sqrt_alpha_t = torch.sqrt(alpha_t)
        sqrt_one_minus_alpha_t = torch.sqrt(1.0 - alpha_t)
        
        pred_original_sample = (noisy_latents - sqrt_one_minus_alpha_t * noise_pred) / sqrt_alpha_t
        
        # === æ¥è‡ª guidance_flux_pipeline.py:FluxGuidancePipeline._rfds_loss ===
        # RFDS: Direct optimization to match predicted original sample
        target = pred_original_sample.detach()
        
        if weight_type == "t":
            # Time-dependent weighting
            w = (1.0 - alpha_t).view(batch_size, 1, 1, 1, 1)  # é€‚é…5Då¼ é‡
            loss = 0.5 * w * F.mse_loss(video_latents, target, reduction="mean")
        elif weight_type == "ada":
            # Adaptive weighting
            prediction_error = video_latents - target
            weighting_factor = torch.abs(prediction_error).mean(dim=(1, 2, 3, 4), keepdim=True)
            weighting_factor = torch.clamp(weighting_factor, 1e-4)
            loss = 0.5 * F.mse_loss(video_latents, target, reduction="none")
            loss = (loss / weighting_factor).mean()
        elif weight_type == "uniform":
            # Uniform weighting
            loss = 0.5 * F.mse_loss(video_latents, target, reduction="mean")
        else:
            raise ValueError(f"Unknown weight_type: {weight_type}")
        
        loss = loss / batch_size
        
        torch.cuda.empty_cache()
        
        return loss

    def save_video(
        self, 
        video: torch.Tensor, 
        output_path: str, 
        fps: int = 8, 
        **kwargs
    ):
        """
        Save generated video to file using DynamiCrafter's save function.
        
        æ¥è‡ªï¼šdynamicrafter_pipeline.py:DynamiCrafterImg2VideoPipeline.save_video
        """
        # Ensure output directory exists
        output_dir = os.path.dirname(output_path)
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
        
        # Get filename without extension
        filename = os.path.basename(output_path).split('.')[0]
        
        # Fix dimensions for save_videos function
        if video.dim() == 4:
            video = video.unsqueeze(0).unsqueeze(0)
        elif video.dim() == 5:
            video = video.unsqueeze(1)
        
        # Ensure video tensor is on CPU
        if video.is_cuda:
            video = video.cpu()
        
        print(f"ğŸ“Š ä¿å­˜è§†é¢‘å½¢çŠ¶: {video.shape}")
        save_videos(video, output_dir or '.', filenames=[filename], fps=fps)
        
        print(f"ğŸ’¾ è§†é¢‘å·²ä¿å­˜: {output_path}")


def test_dynamicrafter_guidance_pipeline():
    """
    æµ‹è¯• DynamiCrafter Guidance Pipeline
    
    å‚è€ƒæ¥æºï¼š
    - dynamicrafter_pipeline.py:test_pipeline (æµ‹è¯•ç»“æ„)
    - é€‚é… guidance ç›¸å…³çš„æµ‹è¯•å‚æ•°
    """
    print("ğŸ§ª æµ‹è¯• DynamiCrafter Guidance Pipeline")
    print("=" * 70)
    
    try:
        # === æ¥è‡ª dynamicrafter_pipeline.py:test_pipeline ===
        # Initialize pipeline
        print("ğŸ”§ åˆå§‹åŒ– DynamiCrafter Guidance Pipeline...")
        pipeline = DynamiCrafterGuidancePipeline(
            resolution='256_256'
        )
        
        # Find test image
        project_root = os.path.dirname(__file__)
        test_image_paths = [
            os.path.join(project_root, 'prompts/1024/pour_bear.png'),
            os.path.join(project_root, 'prompts/512_loop/24.png'),
        ]
        
        test_image_path = None
        for path in test_image_paths:
            if os.path.exists(path):
                test_image_path = path
                break
        
        if test_image_path is None:
            print("âŒ æµ‹è¯•å›¾åƒä¸å­˜åœ¨")
            return False
        
        # Load test image
        print(f"ğŸ“¸ åŠ è½½æµ‹è¯•å›¾åƒ: {test_image_path}")
        img = Image.open(test_image_path).convert("RGB")
        
        # === é€‚é… guidance æµ‹è¯• ===
        # Test basic functionality
        test_prompt = "a person walking in a beautiful garden with flowers blooming"
        
        print(f"ğŸ“ æµ‹è¯•æç¤ºè¯: {test_prompt}")
        print(f"ğŸ¬ å¼€å§‹ SDS ä¼˜åŒ–ç”Ÿæˆ...")
        
        start_time = time.time()
        
        # Generate with SDS
        result = pipeline(
            image=img,
            prompt=test_prompt,
            num_optimization_steps=50,  # Short test
            learning_rate=0.05,
            loss_type="sds",
            cfg_scale=7.5,
            save_debug_videos=True,
            debug_save_interval=25,
            debug_save_path="./debug_dynamicrafter_guidance",
            return_dict=True
        )
        
        elapsed_time = time.time() - start_time
        
        # === æ¥è‡ª dynamicrafter_pipeline.py:test_pipeline ===
        # Check results
        video = result["videos"]
        print(f"ğŸ‰ SDS ç”ŸæˆæˆåŠŸ!")
        print(f"ğŸ“Š è§†é¢‘å½¢çŠ¶: {video.shape}")
        print(f"â±ï¸ ç”¨æ—¶: {elapsed_time:.2f} ç§’")
        
        # Check for NaN
        if torch.isnan(video).any():
            print("âŒ æ£€æµ‹åˆ° NaN å€¼!")
            return False
        else:
            print("âœ… æ—  NaN é—®é¢˜")
        
        # Save result
        output_dir = './results_dynamicrafter_guidance/'
        os.makedirs(output_dir, exist_ok=True)
        output_path = os.path.join(output_dir, "test_sds_video.mp4")
        
        pipeline.save_video(video, output_path)
        
        print("ğŸ‰ DynamiCrafter Guidance Pipeline æµ‹è¯•æˆåŠŸ!")
        print("ğŸ“‹ æµ‹è¯•æ€»ç»“:")
        print("  âœ… æ¨¡å‹åŠ è½½æ­£å¸¸")
        print("  âœ… å›¾åƒé¢„å¤„ç†æ­£å¸¸")
        print("  âœ… æ–‡æœ¬ç¼–ç æ­£å¸¸")
        print("  âœ… SDS ä¼˜åŒ–æ­£å¸¸")
        print("  âœ… è§†é¢‘è§£ç æ­£å¸¸")
        print("  âœ… è§†é¢‘ä¿å­˜æ­£å¸¸")
        print("  âœ… æ— NaNé—®é¢˜")
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == '__main__':
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == 'test':
        test_dynamicrafter_guidance_pipeline()
    else:
        print("ğŸ“ DynamiCrafter Guidance Pipeline")
        print("ğŸ”¬ èåˆæŠ€æœ¯æ¥æºï¼š")
        print("  â€¢ DynamiCrafter: scripts/gradio/dynamicrafter_pipeline.py")
        print("  â€¢ Flux Kontext: _reference_codes/Stable3DGen/flux_kontext_pipeline.py") 
        print("  â€¢ Flux Guidance: _reference_codes/Stable3DGen/2D_experiments/guidance_flux_pipeline.py")
        print("")
        print("ğŸ¯ æ ¸å¿ƒåˆ›æ–°ï¼šå°† Flux Guidance çš„å…ˆè¿›ä¼˜åŒ–æŠ€æœ¯åº”ç”¨äº DynamiCrafter çš„è§†é¢‘ç”Ÿæˆ")
        print("")
        print("ğŸ“– ä½¿ç”¨ç¤ºä¾‹:")
        print("```python")
        print("from dynamicrafter_guidance_pipeline import DynamiCrafterGuidancePipeline")
        print("from PIL import Image")
        print("")
        print("# åˆå§‹åŒ– pipeline")
        print("pipeline = DynamiCrafterGuidancePipeline(resolution='256_256')")
        print("")
        print("# åŠ è½½å›¾åƒ")
        print("image = Image.open('your_image.jpg')")
        print("")
        print("# SDS ä¼˜åŒ–ç”Ÿæˆ")
        print("result = pipeline(")
        print("    image=image,")
        print("    prompt='person walking in garden',")
        print("    num_optimization_steps=1000,")
        print("    loss_type='sds',           # æˆ– 'csd', 'rfds'")
        print("    cfg_scale=7.5")
        print(")")
        print("")
        print("# ä¿å­˜è§†é¢‘")
        print("pipeline.save_video(result['videos'], 'output.mp4')")
        print("```")
